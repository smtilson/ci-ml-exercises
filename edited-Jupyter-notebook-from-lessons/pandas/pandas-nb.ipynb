{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 13 - Long x Wide format\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Differentiate a dataset in a long or wide format\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Long x Wide format\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Your data can be in the **long** or the **wide** format\n",
    "\n",
    "* In a **long** format, every row represents an observation belonging to a particular category.\n",
    "* In a **wide** format, categorical data is grouped. You can frame it as a summary of long data. It is typically easier to read and interpret compared to the long format. \n",
    "    \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The **need** to consider long or wide formats is because a given function (imported from the libraries used in data science) may be constructed considering one format or another. It will be your job to pre-process the data to properly combine your data and the functions imported from the libraries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's see the examples below so we can understand the use cases\n",
    "\n",
    "* Consider the following dataset: invented **records for Average Temperature for four days in 3 different cities**.\n",
    "  * You can create using the techniques you have learned so far. The example below is in the **long format**, where the dataset has a column for possible variable types (City, Day, Temperature).\n",
    "  * Each cell value is a representation of the column it is related to.\n",
    "    * For example, when you see 1 under the 'Day' column, you know you are referring to: day. \n",
    "    * When you see 21 under the 'Temperature' column, you know it is a temperature\n",
    "  * The more data you add (imagine if you keep adding days for a giving city), the more the DataFrame will grow vertically, in a \"long\" way\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed=101)\n",
    "\n",
    "df_long = pd.DataFrame(data= {'City':['Dublin','Dublin','Dublin','Dublin',\n",
    "                                      'Cork','Cork','Cork','Cork','Galway',\n",
    "                                      'Galway','Galway','Galway'],\n",
    "                              'Day':[1,2,3,4]*3,\n",
    "                              'AvgTemperature': np.random.randint(10,30,12)})\n",
    "\n",
    "df_long\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The same dataset could be represented in a different format\n",
    "  * The example below is in the **wide format**. \n",
    "  * Each cell value is the response for the set of variables. In this case, it is a temperature value, for a given day, in a given city\n",
    "    * When you see 25 under the 'Day1' column, you know it is a temperature.\n",
    "\n",
    "  * The more data you add (imagine if you keep adding days for a given city), the more the DataFrame will grow horizontally, in a \"wide\" way.\n",
    "\n",
    "df_wide = pd.DataFrame(data= {'City': ['Cork','Dublin','Galway'],\n",
    "                              'Day1':[25,21,14],\n",
    "                              'Day2':[19,27,18],\n",
    "                              'Day3':[23,16,10],\n",
    "                              'Day4':[18,21,24]}\n",
    "                  )\n",
    "df_wide\n",
    "\n",
    "Naturally, you will not hard-code the transformation from one format to another. \n",
    "  * To transform Long to Wide, use **pd.pivot_table()**  . The documentation is found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html). \n",
    "  * To transform Wide to Long, use **pd.melt()**. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)\n",
    "\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pivot Table\n",
    "\n",
    "Let's learn the use cases\n",
    "  * Consider your long DataFrame\n",
    "\n",
    "df_long\n",
    "\n",
    "You can transform it to a wide format using `pd.pivot_table(`)\n",
    "  * index are the variables you want to remain untouched. In this case, it is one variable: '`City`'\n",
    "  * columns are the variables you want to spread. In this case, it is: '`Day`'\n",
    "  * values are the numerical values you want to aggregate/process\n",
    "\n",
    "pd.pivot_table(data=df_long,\n",
    "               index=['City'],\n",
    "               columns=['Day'],\n",
    "               values='AvgTemperature')\n",
    "\n",
    "You will notice that the output is a DataFrame with multi-index: `Day` and `City.` You can transform it to a more conventional format by applying the methods `reset_index()` and `rename_axis(),` using additional processing to format the DataFrame nicely.\n",
    "  * `.rename_axis()` is used to set the name of the axis for the index.\n",
    "  * `mapper` sets the name, in this case `None`\n",
    "  * `axis=1` indicates you want to rename the index\n",
    "\n",
    "The documentation is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename_axis.html)\n",
    "\n",
    "pd.pivot_table(data=df_long,\n",
    "               index='City',\n",
    "               columns='Day',\n",
    "               values='AvgTemperature').reset_index().rename_axis(mapper=None, axis=1)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the following fictitious data showing the daily sales of various models of cars over four weekly periods.\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed=144)\n",
    "df_long_practice = pd.DataFrame(data= {'Model':['Hyundai','Hyundai','Hyundai','Hyundai',\n",
    "                                      'Ford','Ford','Ford','Ford','Mazda',\n",
    "                                      'Mazda','Mazda','Mazda','Fiat', 'Fiat','Fiat','Fiat'],\n",
    "                              'Week':['Week 1', 'Week 2', 'Week 3', 'Week 4']*4,\n",
    "                              'DailySales': np.random.randint(1,10,16)})\n",
    "df_long_practice\n",
    "\n",
    "Using the pivot_table function\n",
    "* set index to `Model`, set columns to `Week` and set values to `DailySales`\n",
    "* Then reset the index \n",
    "* Then use rename_axis, setting mapper to None and axis to 1 \n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Melt\n",
    "\n",
    "You can transform from wide to long. Let's recap df_wide\n",
    "\n",
    "df_wide\n",
    "\n",
    "Use` pd.melt()`\n",
    "  * `frame` is the DataFrame\n",
    "  * `var_name` is the categorical column name you will create\n",
    "  * `value_name` is the numerical column name you will create\n",
    "  * `value_vars` are the columns you will \"shrink\" or \"melt\"\n",
    "  * `id_vars` are columns to use as identifier variables or variables that will remain untouched.\n",
    "\n",
    "pd.melt(frame=df_wide,\n",
    "        var_name='Day',\n",
    "        value_name='AverageTemperature',\n",
    "        value_vars=['Day1','Day2','Day3','Day4'],\n",
    "        id_vars=['City'])\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the following fictitious data showing the daily sales of various models of cars over four weekly periods and is displayed in the wide format.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_wide_practice = pd.DataFrame(data= {'Model': ['Hyundai','Ford','Mazda', 'Fiat'],\n",
    "                              'Week 1':[8,9,7,7],\n",
    "                              'Week 2':[4,3,1,6],\n",
    "                              'Week 3':[2,6,2,2],\n",
    "                              'Week 4':[9,7,3,3]}\n",
    "                  )\n",
    "df_wide_practice\n",
    "\n",
    "\n",
    "Using **melt** set the frame to `df_wide_practice`\n",
    "* Use var_name to create a column named `Week`\n",
    "* Use value_name to create a column named `DailySales`\n",
    "* Use value_vars to melt the columns from week 1 through to week 4\n",
    "* Use id_vars on the `Model` column, as this should remain untouched.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pivot Table to generate statistics\n",
    "\n",
    "Another use case for **pd.pivot_table()** is to generate a table of statistics that helps summarise the data of a larger table\n",
    "\n",
    "\n",
    "Consider the following dataset. \n",
    "  * It shows records for diamond prices and their characteristics, like colour, cut, clarity and depth\n",
    "\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('diamonds')\n",
    "df = df.head(200)\n",
    "print(df.shape)\n",
    "df.head(10)\n",
    "\n",
    "You are interested in summarising and checking the mean price per cut and colour.\n",
    "* Note we have multiple rows of the same cut and colour The example below queries data where: cut == 'Ideal' and colour == 'E'.\n",
    "\n",
    "  * Therefore we need to use an aggregation function when using a pivot table. In this case, we will use: 'mean'\n",
    "\n",
    "df.query(\"cut == 'Ideal' and color == 'E'\")\n",
    "\n",
    "These are the arguments for `pd.pivot_table()`\n",
    "  * `values` are 'price' and `aggfunc` is 'mean'\n",
    "  * `index` is 'cut and `columns` are 'colour'\n",
    "\n",
    "\n",
    "\n",
    "In the end, you can check the price level for each cut and colour\n",
    "\n",
    "pd.pivot_table(data=df,\n",
    "               index='cut',\n",
    "               columns='color',\n",
    "               values='price',\n",
    "               aggfunc='mean')\n",
    "\n",
    "Imagine now that you are interested in summarising and checking the max price per cut and colour per clarity.\n",
    "  * Your index will be a list: ['cut','color']. \n",
    "  * Your columns will be: 'clarity'\n",
    "  * You will notice few cells have NaN, indicating that particular combination doesn't exist\n",
    "   * You can use the parameter ``fill_value`` and set it to a suitable value to replace missing values with; a good value, in this case, would be 0\n",
    "  * You will also notice that this transformation, as it is, is not super insightful since we create a chunk of numbers. However, this processing step prepares the data to be parsed to a function where you could potentially visualise the data and extract useful insights. We will do that in future lessons\n",
    "\n",
    "pd.pivot_table(data=df,\n",
    "               index=['cut','color'],\n",
    "               columns='clarity',\n",
    "               values='price',\n",
    "               fill_value=0,\n",
    "               aggfunc='max')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips, based on the day of the week, day time, total bill, gender, if it is a smoker table or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "print(f\"DataFrame shape: {df_practice.shape}\")\n",
    "df_practice.head(50)\n",
    "\n",
    "You have to choose your days and times to work; you want to check which days you should work and which shift to maximise your potential tips.\n",
    "\n",
    "To retrieve this data, you need to use a pivot_table and set data to df_practice\n",
    "* You will need to set index as ``day`` and ``time``.\n",
    "* To see which tables give the most tips, you will need to set columns to ``size``\n",
    "* And as you are interested in tips, you will need to set values to ``tip``\n",
    "* For the aggfunc you will use ``sum``\n",
    "* And so as not to have NaN showing set ``fill_value`` to 0\n",
    "\n",
    "When you run this, you should have a good idea of what days and shifts to work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **WARNING** <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">\n",
    "\n",
    "You might have noticed up to this point; there is a lot covered in Pandas, multiple functions/methods with multiple use cases.\n",
    "  * It is virtually **IMPOSSIBLE** to remember all functions and all arguments. Using library documentation is fundamental to developing a project.\n",
    "  * Naturally, over time you will get familiar with and used to the most common use cases, but in general, a data professional spends **a lot of time** searching the Internet for the documentation and uses cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 14 - Group By\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Group our data based on attributes and aggregate functions\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Group By\n",
    "\n",
    "The groupby method allows you to **group rows of data together and call aggregate functions.**\n",
    "* It can be used to group large amounts of data and compute operations on them. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)\n",
    "\n",
    "\n",
    "Consider the following data. It is a fictional revenue, quantity and margin record for your team considering a set of products for a given month\n",
    "  * First, you are interested in checking performance at the product level\n",
    "\n",
    "data = {'Product':['Bread','Bread','Milk','Milk','Milk','Butter','Butter','Butter'],\n",
    "        'Person':['Anna','Anna','Brian','John','John','Carl','Sarah','Anna'],\n",
    "        'Sales':[200,120,340,124,243,350,500,240],\n",
    "        'Quantity':[3,5,3,8,2,7,5,4],\n",
    "        'Margin':[100,20,280,50,100,67,300,200]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "You can use `.groupby()` to group your data. The argument is 'by' and receives a list using the columns you want to group. In this case, it is one column: 'Product'\n",
    "* This **method itself doesnâ€™t really do any operations** to produce a useful result **until** you aggregate it.\n",
    "* For teaching purposes, let's first group and assign to a variable. You will notice the output is an object\n",
    "\n",
    "by_group = df.groupby(by=['Product'])\n",
    "by_group\n",
    "\n",
    "You can iterate across the object's elements to check how it was grouped\n",
    "\n",
    "for product, frame in by_group:\n",
    "  print(f\"Entries for '{product}' \")\n",
    "  print(\"------------------------\")\n",
    "  print(f\"{frame} \\n\\n\")\n",
    "\n",
    "You should apply a function to `.groupby()` to reveal more interesting information\n",
    "  * Imagine if you want to know the average levels of sales, quantity and margin per product. You can chain with the method `.mean()`\n",
    "  * We added the method `.round()`, so the numerical decimals values can be rounded to an appropriate value. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html)\n",
    "\n",
    "df.groupby(by=['Product']).mean().round(2)\n",
    "\n",
    "Get the minimum values with `.min()`\n",
    "  * Let's make sense of it, but first, let's recap df\n",
    "\n",
    "df\n",
    "\n",
    "Now we groupby 'Product' and aggregate by `.min()`\n",
    "  * The values indicated in the answer are minimum levels for each variable separately.\n",
    "  * For example, it might be tempting to conclude that for Milk, Brian made the lowest sales of 124 with a margin of 50. This is not the case\n",
    "    * The person who made the least amount of sales, particularly for Milk, was John (2 sales) \n",
    "    * For the milk group, Brian is the minimum Person because B is 66 in ASCII and J (John) is 74\n",
    "    * The lowest sales were 124; when we look at the data, we see John made it \n",
    "    * The lowest margin was 50; when looking at the data, it was made by John\n",
    "    * The lowest quantity is 2; when looking at the data, it was made by John\n",
    "\n",
    "df.groupby(by= ['Product']).min()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the dataset, which consists of people, the County they are from, the programming language they use and how many years of experience they have using it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = {'first_name':['Cliona','Emily','Sean','John','Adam','Jane','Sarah','Carl'],\n",
    "        'county':['Dublin','Dublin','Kildare','Dublin','Dublin','Kildare','Kildare','Kilkenny'],\n",
    "        'language':['python','javascript','javascript','python','python','python','javascript','python'],\n",
    "        'age':[23,45,31,26,56,45,43,27],\n",
    "        'years_experience':[5,2,7,5,3,2,4,6]}\n",
    "\n",
    "df_practice = pd.DataFrame(data)\n",
    "df_practice\n",
    "\n",
    "Feel free to try out your ideas to group the data by or use the following.\n",
    "\n",
    "You are looking to find out the mean years_experience in a language by County, so you will need to group by two columns and then use the mean method. You should also round to two decimal places.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> You can use the `.describe()` method in your grouped data\n",
    "  * You can see, for each product, the summary statistics of all other variables\n",
    "\n",
    "df.groupby(by=['Product']).describe().round(2)\n",
    "\n",
    "If you prefer, you can transpose a DataFrame, adding `.T`.\n",
    "* You may use it if that helps you see the result better. If not, don't use it\n",
    "* `.T` can be applied to a DataFrame. Since the result of `.describe(`) is a DataFrame, it works\n",
    "\n",
    "df.groupby(by= ['Product']).describe().round(2).T\n",
    "\n",
    "Naturally, you can add the argument `include='all'` for `.describe()` to consider categorical variables in your analysis\n",
    "* You will notice it added the information for Person, whereas before, you only had Sales, Quantity and Margin\n",
    "\n",
    "df.groupby(by=['Product']).describe(include='all').round(2)\n",
    "\n",
    "* You can group by more than one variable, in this, case by Person and Product.\n",
    "  * You will notice the summary statistics broken down by each Person and each Product that a given person sold\n",
    "\n",
    "df.groupby(by=['Person','Product']).describe().round(1)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "df_practice = sns.load_dataset('tips')\n",
    "print(df_practice.shape)\n",
    "df_practice.head(10)\n",
    "\n",
    "\n",
    "Feel free to try out your ideas to group the data or use the following suggestion.\n",
    "\n",
    "\n",
    "You are looking to find the summary statistics broken down by day and time.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Getting back to aggregation alternative methods. You may be interested in aggregating more than one function\n",
    "  * You can groupby and chain `.agg()` to aggregate multiple functions. The documentation is found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html). The arguments are the functions you are interested in, in this case, '`mean`', '`min`', and '`max`' parsed in a list.\n",
    "\n",
    "(df\n",
    " .groupby(by=['Product'])\n",
    " .agg(['mean','min','max'])\n",
    " .round(1)\n",
    " )\n",
    "\n",
    "Naturally, you can group by more than one variable, in this case: Person and Product, in this order\n",
    "* For this exercise, we are interested only in Sales and Margin levels. After you groupby, you can subset the variables you are interested in, using the brackets approach, in this case: 'Sales' and 'Margin'\n",
    "\n",
    "(df\n",
    " .groupby(by=['Person','Product'])\n",
    " ['Sales','Margin']\n",
    " .agg(['mean','min','max'])\n",
    " .round(2)\n",
    ")\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "df_practice = sns.load_dataset('tips')\n",
    "print(df_practice.shape)\n",
    "df_practice.head(10)\n",
    "\n",
    "\n",
    "Feel free to try out your ideas or use the following suggestion.\n",
    "\n",
    "You are interested in finding out the mean and max values of ``total_bill`` and ``tip`` rounded to two places, grouped by ``day`` and ``time.``\n",
    "* so you would have total_bill and tip as the variables you **subset**, using the brackets approach,\n",
    " * Your output should look like below\n",
    "\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You can apply custom functions when grouping and aggregating\n",
    "  * Let's forecast sales for next month based on the sum of Sales and multiply by 1.5. This logic is fictitious and serves to explain the concept\n",
    "  * The function `sales_forecast_next_month` is parsed into the list at `.agg()`\n",
    "\n",
    "def sales_forecast_next_month(x):\n",
    "  return x.sum() * 1.5\n",
    "\n",
    "(df\n",
    " .groupby(by=['Product'])\n",
    " ['Sales']\n",
    " .agg(['sum','mean',sales_forecast_next_month])\n",
    " .round(2)\n",
    " )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You noticed that the results up to now generated a DataFrame that is multi-indexed. In this exercise, we present another way to groupby, aggregate and create columns with multiple use cases in a DataFrame format that is more in line with the DataFrame format we have been working on\n",
    "  * First let's recap df\n",
    "\n",
    "df\n",
    "\n",
    "The structure is the following:\n",
    "  * You `.groupby()` as usual\n",
    "  * At `.agg()`, you will parse the column name and a tuple. The tuple contains a variable you will aggregate and the aggregation function.\n",
    "    * In the example, we create a column called `Total_of_Sales` (note it is not a string) using the Sales variable and the aggregation function 'sum'\n",
    "    * We keep this pattern and create columns based on specific variables and aggregation functions\n",
    "  * After it, you add `.reset_index()` to remove MultIndex\n",
    "\n",
    "You are interested in generating a report per product, showing its performance in terms of margin, sales, sales average, total quantity, the team and a forecast broken down per product.\n",
    "\n",
    "df_report = (df\n",
    "             .groupby(by=['Product'])\n",
    "             .agg(Total_of_Margin=('Margin','sum'),\n",
    "                  Total_of_Sales=('Sales','sum'),\n",
    "                  Avg_of_Sales=('Sales','mean'),\n",
    "                  Quantity=('Quantity','sum'),\n",
    "                  Number_of_People_Selling=('Person','nunique'),\n",
    "                  Team=('Person','unique'),\n",
    "                  SalesForecastForNextMonth=('Sales',sales_forecast_next_month)\n",
    "                  )\n",
    "             .reset_index()\n",
    "             \n",
    "             )\n",
    "\n",
    "df_report\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, day time, total bill, gender, if it is a smoker table or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "df_practice = sns.load_dataset('tips')\n",
    "print(df_practice.shape)\n",
    "df_practice.head(10)\n",
    "\n",
    "\n",
    "Feel free to try out your ideas or use the following suggestion.\n",
    "\n",
    "You wish to get the total sales per day and a count of the number of tables served per day and display this data in separate columns.\n",
    "\n",
    "* You should group by ``day``\n",
    "* Create a column named ``Total_of_Sales,`` using the total_bill variable and the aggregation function 'sum'\n",
    "* Create a column named ``Tables_Served,`` using the time variable and the aggregation function 'count'\n",
    "* And you should reset the index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Say now you are interested in adding one level and evaluating the performance: per `person` and  `product`\n",
    "* Just parse in a list of these variables at `.groupby()`. \n",
    "* The mechanics for `.agg()` remains the same\n",
    "\n",
    "(df\n",
    " .groupby(by=['Person','Product'])\n",
    " .agg(Total_of_Sales=('Sales','sum'),\n",
    "      Quantity=('Quantity','sum'),\n",
    "      Total_of_Margin=('Margin','sum'))\n",
    " .reset_index()\n",
    "  )\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 15 - Concatenate, Merge and Join\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Manage DataFrame columns\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Concatenate, Merge and Join\n",
    "\n",
    "You can merge, join, and concatenate your DataFrames to better analyse and derive insights from the data.\n",
    "\n",
    "\n",
    "* There are three main ways of combining DataFrames together: **Concatenating, Merging, and Joining**\n",
    "  * `pd.concat()` for combining DataFrames across rows or columns\n",
    "  *` pd.merge()` for combining data on common columns or indices\n",
    "  * `.join()` for combining data on a key column or an index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Concatenate\n",
    "\n",
    "Your datasets are just stitched together along either the row axis or column axis.\n",
    "  * Consider 3 DataFrames; their content is constant, so you can better understand how the data was concatenated\n",
    "  * They have the same columns. df2 and df3 have the same indices\n",
    "\n",
    "df1 = pd.DataFrame(data= 1,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[0,1,2,3])\n",
    "\n",
    "df2 = pd.DataFrame(data= 2 ,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[4,5,6,7])\n",
    "\n",
    "df3 = pd.DataFrame(data= 3,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[4,5,6,7])\n",
    "\n",
    "\n",
    "df1\n",
    "\n",
    "df2\n",
    "\n",
    "df3\n",
    "\n",
    "You can concatenate by rows; ideally, when DataFrames share the same columns, have different indices parse in a list of DataFrames and use `pd.concat()`.\n",
    "  * `axis=0`, indicating you are concatenating by row. Like, connecting \"vertically\"\n",
    "  * Even though df2 and df3 have the same indices, the `pd.concat()` operation worked. Note that all three DataFrames were \"glued\" one to another.\n",
    "* The documentation is found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)\n",
    "\n",
    "pd.concat([df1,df2,df3],axis=0) \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **Note**: An alternative way to concatenate is `.append()`, where you concatenate/append one DataFrame to another. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)\n",
    " * You can either append a single DataFrame or, passing in a list of DataFrames, append multiple.\n",
    "\n",
    "print('Appending a single DataFrame')\n",
    "print(df1.append(df2))\n",
    "print()\n",
    "print('Passing in a list of DataFrames')\n",
    "print(df1.append([df2, df3]))\n",
    "\n",
    "\n",
    "You can concatenate by columns when you set axis=1.\n",
    "  * This is useful when DataFrames share the same indices and have different columns\n",
    "  * Note that since df2 and df3 have the same indices, less NaN was generated for indices 4,5,6,7.\n",
    "\n",
    "df1 = pd.DataFrame(data= 1,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[0,1,2,3])\n",
    "\n",
    "df4 = pd.DataFrame(data= 2 ,\n",
    "                   columns=['E','F','G','H'],\n",
    "                   index=[4,5,6,7])\n",
    "\n",
    "df5 = pd.DataFrame(data= 3,\n",
    "                   columns=['I','J','K','L'],\n",
    "                   index=[4,5,6,7])\n",
    "\n",
    "pd.concat([df1,df2,df3], axis=1) \n",
    "# pd.concat([df1,df4,df5], axis=1)  # uncomment this line and try this option too\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the following four DataFrames, three of which share the same indices but have different columns, and two that share the same columns but have different indices.\n",
    "\n",
    "pf1 = pd.DataFrame(data= 1,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[0,1,2,3])\n",
    "\n",
    "pf2 = pd.DataFrame(data= 2 ,\n",
    "                   columns=['E','F','G','H'],\n",
    "                   index=[0,1,2,3])\n",
    "\n",
    "pf3 = pd.DataFrame(data= 3,\n",
    "                   columns=['I','J','K','L'],\n",
    "                   index=[0,1,2,3])\n",
    "\n",
    "pf4 = pd.DataFrame(data= 4,\n",
    "                   columns=['A','B','C','D'],\n",
    "                   index=[4,5,6,7])\n",
    "\n",
    "Feel free to try out your ideas or use the following suggestions.\n",
    "\n",
    "1. Choosing concat, which three DataFrames would you use to display a DataFrame with no NaN showing (the use of the correct value of the axis will be necessary for this)\n",
    "2. Which DataFrame will you append to pf1 to get a DataFrame with no NaN showing?\n",
    "\n",
    "# write your code here\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Merge\n",
    "\n",
    "You can merge DataFrames using `pd.merge()`. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html). This logic is similar to merging SQL tables.\n",
    "* It is most useful when you want to combine rows that share data based on a common variable/key\n",
    "\n",
    "\n",
    "Consider 2 DataFrames\n",
    "\n",
    "df_1 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "   \n",
    "df_2 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K4'],\n",
    "                          'C': ['C0', 'C1', 'C2', 'C4'],\n",
    "                          'D': ['D0', 'D1', 'D2', 'D4']})    \n",
    "\n",
    "Note at the `key` column:  `df_1` has K3, and `df_2` has K4.\n",
    "* Both have K0, K1 and K2\n",
    "\n",
    "df_1\n",
    "\n",
    "df_2\n",
    "\n",
    "You provide 2 DataFrames you would like to merge.\n",
    "  * The first is referenced as left. The second, right.\n",
    "  * 'how' is the type of merge to be performed - how: 'left','right', 'outer', 'inner'. In case you are familiar with SQL, this is equivalent to when combining SQL tables\n",
    "    * It will be your job in the workplace to decide which type of merge should be performed\n",
    "  * 'on' is the column to merge on\n",
    "\n",
    "\n",
    "\n",
    "The example below shows: `how='left'`. Note that the result doesn't consider the 'key' column value, which df_1 doesn't have, like K4\n",
    "\n",
    "\n",
    "pd.merge(df_1, df_2, how='left', on='key')\n",
    "\n",
    "The example below shows: `how='right'`. Note that the result doesn't consider the 'key' column value, which df_2 doesn't have, like K3\n",
    "\n",
    "\n",
    "pd.merge(df_1, df_2, how='right', on='key')\n",
    "\n",
    "The example below shows:` how='inner'`. Note that the result doesn't consider the 'key' column values, which the left has and right doesn't, like K3 and K4\n",
    "* It tends to have less missing data. In this case, no missing values\n",
    "\n",
    "\n",
    "pd.merge(df_1,df_2,how='inner',on='key')\n",
    "\n",
    "The example below shows: `how='outer'`. Note that the result considers all 'key' column values.\n",
    "* Typically the result will produce more missing values\n",
    "\n",
    "pd.merge(df_1,df_2,how='outer',on='key')\n",
    "\n",
    "There could be cases where the DataFrames columns you are merging will not have a shared column name. Imagine the following DataFrame.\n",
    "\n",
    "df_3 = pd.DataFrame({'Column_key': ['K0', 'K1', 'K2', 'K4'],\n",
    "                          'C': ['C10', 'C20', 'C30', 'C40'],\n",
    "                          'D': ['D10', 'D20', 'D30', 'D40']}) \n",
    "\n",
    "df_3\n",
    "\n",
    "You are interested in merging `df_3` and `df_1`, considering only the data that is mutual to both: `how='inner'`. \n",
    "\n",
    "* Note the column names that are used as a reference to join both are different: `key` and `Column_key` \n",
    "\n",
    "df_1\n",
    "\n",
    "When merging both, you should specify a column from each DataFrame; since the column names are different, use the **left_on** and **right_on** parameters.\n",
    "\n",
    "* `left_on` uses the column named **key**, which is a column name of the DataFrame `df_1`.\n",
    "* `right_on` uses the column named **Column_key**, which is a column name of the DataFrame `df_3`.\n",
    "\n",
    "\n",
    "pd.merge(df_1, df_3, how='inner', left_on='key', right_on='Column_key')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the following datasets for practice.\n",
    "* The first dataset, df_user_usage, has users' monthly mobile usage information for a subset of users.\n",
    "* The second dataset, df_user_device has details of an individual user device, like platform type and version, and device name\n",
    "\n",
    "When you run the code to display both DataFrames, you also get the column names; use that information to get the common column name you will require to merge the DataFrames. \n",
    "\n",
    "\n",
    "\n",
    "df_user_usage = pd.read_csv(\"https://raw.githubusercontent.com/Code-Institute-Solutions/sample-datasets/main/user_usage.csv\")\n",
    "print(df_user_usage.columns)\n",
    "df_user_usage.head()\n",
    "\n",
    "df_user_device = pd.read_csv(\"https://raw.githubusercontent.com/Code-Institute-Solutions/sample-datasets/main/user_device.csv\")\n",
    "print(df_user_device.columns)\n",
    "df_user_device.head()\n",
    "\n",
    "Feel free to try out your ideas or use the following suggestion.\n",
    "\n",
    "* You want to merge both DataFrames using an inner join to access all the data you require in a single DataFrame.\n",
    "* The left DataFrame should be df_user_usage, and the right should be df_user_device. \n",
    "* You don't require all the columns from df_user_device; only the column name, common to both and two others, 'platform' and 'device'.\n",
    "\n",
    "# write your code here\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Join\n",
    "\n",
    "By default, `.join()` will attempt to join on **indices**. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html)\n",
    "\n",
    "df_left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                        'B': ['B0', 'B1', 'B2']},\n",
    "                       index=['K0', 'K1', 'K2']) \n",
    "\n",
    "df_right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "                         'D': ['D0', 'D2', 'D3']},\n",
    "                        index=['K0', 'K2', 'K3'])\n",
    "\n",
    "Note they have different **indices**\n",
    "  * both have K0 and K2\n",
    "  * df_left has K1\n",
    "  * df_right has K3\n",
    "  \n",
    "\n",
    "df_left\n",
    "\n",
    "df_right\n",
    "\n",
    "The structure is the following:\n",
    "  * write your first DataFrame\n",
    "  * The the method `.join()`\n",
    "  * The first argument the join method takes is a DataFrame or a list of DataFrames\n",
    "  * '`how`' follows the same logic from `pd.merge()`\n",
    "\n",
    "* The example below uses `how='left'` \n",
    "  * Note it displays only indices from the left DataFrame\n",
    "\n",
    "df_left.join(df_right,how='left')\n",
    "\n",
    "The example below uses `how='right' `\n",
    "  * Note it displays only indices from the right DataFrame\n",
    "\n",
    "df_left.join(df_right,how='right')\n",
    "\n",
    "The example below uses `how='inner' `\n",
    "  * Note it displays indices that both share\n",
    "\n",
    "df_left.join(df_right,how='inner')\n",
    "\n",
    "The example below uses `how='outer'` \n",
    "  * Note it displays all indices from both DataFrames\n",
    "  \n",
    "\n",
    "df_left.join(df_right,how='outer')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will work on the datasets used in the previous practice named df_user_usage and df_user_device.\n",
    "\n",
    "\n",
    "\n",
    "df_user_device = pd.read_csv(\"https://raw.githubusercontent.com/Code-Institute-Solutions/sample-datasets/main/user_device.csv\")\n",
    "print(df_user_device.columns)\n",
    "df_user_usage = pd.read_csv(\"https://raw.githubusercontent.com/Code-Institute-Solutions/sample-datasets/main/user_usage.csv\")\n",
    "print(df_user_usage.columns)\n",
    "\n",
    "\n",
    "We want to use the join method and join the two DataFrames, but there is a problem as the DataFrames share a column name. The documentation (linked to at the beginning of the join section) shows a few ways to do a join in a situation like this.\n",
    "* Try out a join using set_index with the shared column name.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A good practice\n",
    "\n",
    "As a reminder, it is always a good practice to check for missing data if you concatenate, merge or join. It helps you to avoid potential future surprises, such as your data not having the proper shape.\n",
    "* Let's keep with the last example\n",
    "\n",
    "df_result = df_left.join(df_right,how='left')\n",
    "\n",
    "The simplest way to check for missing data in your DataFrame is with `.isna().sum()`\n",
    "\n",
    "df_result.isna().sum()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit 16a\n",
    "# Pandas - Unit 16 - Pandas Data Visualisation Part 1\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Visualise data with Pandas built-in capabilities, using line plot, histogram and box plot\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pandas Data Visualisation Part 1\n",
    "\n",
    "### Disclaimer and imports\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **You will only start understanding your data objectively when you start visualising it**\n",
    "* Pandas' has built-in methods/functions for visualising data that will help you to understand your data better.\n",
    "  * Even though it is related to Pandas, we need to import matplotlib, which we will study in more detail later in the course. For now, you just need to be aware of it.\n",
    "* Pandas' capabilities/flexibilities for plotting can not be compared to Matplotlib, Seaborn or Plotly. \n",
    "  * You typically use Pandas plotting capabilities for quick visualisations. For more elaborate visualisations, we will cover other libraries later in the course\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> .plot()\n",
    "\n",
    "You can use `.plot()` to visualise data in Pandas. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)\n",
    "* A relevant argument is `'kind'`, where you determine the kind of plot to produce. The options include:\n",
    "\n",
    "  * 'line': line plot (default)\n",
    "  * 'bar': vertical bar plot\n",
    "  * 'barh': horizontal bar plot\n",
    "  * 'hist': histogram\n",
    "  * 'box': boxplot\n",
    "  * 'kde': Kernel Density Estimation plot\n",
    "  * 'area': area plot\n",
    "  * 'pie': pie plot\n",
    "  * 'scatter': scatter plot\n",
    "\n",
    "\n",
    "\n",
    "To choose an appropriate argument for `kind` in this function requires an understanding of data types and how to visualise data. Experience gained through data projects will help with this.\n",
    "  * We will start exploring  the plots and present additional arguments\n",
    "\n",
    "---\n",
    "\n",
    "For the next set of exercises, we will use datasets from Seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "sns.get_dataset_names()\n",
    "\n",
    "---\n",
    "\n",
    "We will consider the 'flights' dataset\n",
    "\n",
    "df = sns.load_dataset('flights')\n",
    "df = df.head(50)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "We need to process the data for this exercise\n",
    "  * First, we transform year and month to string using `.astype()`\n",
    "  * Then we combine the strings in the pattern:  month - year, so that we can convert this pattern to date with `pd.to_datetime()`\n",
    "  * We finally set Date as index with `.set_index()`\n",
    "\n",
    "df['year'] = df['year'].astype('str')\n",
    "df['month'] = df['month'].astype('str')\n",
    "df['Date'] = pd.to_datetime(df['month'] + '-' + df['year'] )\n",
    "df.set_index('Date',inplace=True)\n",
    "df.head()\n",
    "\n",
    "We are interested in producing a **line plot** where y values are passengers, and x values are Date.\n",
    "  * By default, x is the index. In case you want another variable, you can specify it.\n",
    "  * figsize is a tuple to set plot height, and width\n",
    "  * xlabel is x-axis label\n",
    "  * ylabel is y-axis label\n",
    "  * title is plot title.\n",
    "\n",
    "* **plt.show()**\n",
    "  * `plt.show()` gives the command to plot.\n",
    "\n",
    "df.plot(kind='line',y='passengers',\n",
    "        figsize=(10,6),\n",
    "        title='Passengers x Time', xlabel='Date', ylabel='Passengers'\n",
    "        )\n",
    "plt.show()\n",
    "\n",
    "You can process the data and look for the average levels over the years\n",
    "  * We first groupby year, set `as_index=False`\n",
    "  * Then aggregate and create AvgLevels as the mean of passengers\n",
    "  * Finally, we create a line plot, indicating x and y. We just used one argument here: figsize\n",
    "\n",
    "* We notice a trend that passengers' levels increased over time.\n",
    "  * We can't state definitively that the levels increased simply by plotting. We need statistical tests for that, which we will learn in future lessons. This plot just helps to see a trend over the years and that the trend is upwards.\n",
    "\n",
    "(df\n",
    " .groupby(by=['year'],as_index=False)\n",
    " .agg(AvgLevels=('passengers','mean'))\n",
    " .plot(kind='line',x='year',y='AvgLevels', figsize=(5,5))\n",
    " )\n",
    "plt.show()\n",
    "\n",
    "You can plot multiple lines in your line plot\n",
    "  * We create Col1 as an expression made with NumPy\n",
    "  * This variable doesn't have any particular business meaning; it is just for teaching purposes\n",
    "\n",
    "df['Col1'] = 300 + 20 * np.random.randn(df.shape[0]) + np.random.randint(low=-10, high=20, size=df.shape[0])\n",
    "df.head()\n",
    "\n",
    "We plot the dataset using `.plot(),` `kind='line'`\n",
    "  * we parse a list for y, containing passengers and Col1\n",
    "\n",
    "df.plot(kind='line',y=['passengers','Col1'],\n",
    "        figsize=(10,6),\n",
    "        title='Passengers and Col1 x Time', xlabel='Date', ylabel=' '\n",
    "        )\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will use the DataFrame below to create a bar chart.\n",
    "\n",
    "\n",
    "data = {'Product':['Bread','Bread','Milk','Milk','Milk','Butter','Butter','Butter'],\n",
    "        'Person':['Anna','Anna','Brian','John','John','Carl','Sarah','Anna'],\n",
    "        'Sales':[200,120,340,124,243,350,500,240],\n",
    "        'Quantity':[3,5,3,8,2,7,5,4],\n",
    "        'Margin':[100,20,280,50,100,67,300,200]}\n",
    "\n",
    "df_practice = pd.DataFrame(data)\n",
    "df_practice\n",
    "\n",
    "You are interested in using a line chart to analyse the quantity of each product sold by each person.\n",
    "\n",
    "On the DataFrame df_practice, you should do a \n",
    "* groupby by Product and Person\n",
    "* then aggregate and create Quantity as the sum of Quantity\n",
    "* Then use plot setting kind to line and ylabel to Quantity and use figsize with 10,5\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "In the end, your plot should look like this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 16B - Pandas Data Visualisation Part 1 Histogram\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Visualise data with Pandas built-in capabilities, using line plot, histogram and box plot\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pandas Data Visualisation Part 1\n",
    "\n",
    "### Disclaimer and imports\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **You will only start understanding your data objectively when you start visualising it**\n",
    "* Pandas' has built-in methods/functions for visualising data that will help you to understand your data better.\n",
    "  * Even though it is related to Pandas, we need to import matplotlib, which we will study in more detail later in the course. For now, you just need to be aware of it.\n",
    "* Pandas capabilities/flexibilities for plotting can not be compared to Matplotlib, Seaborn or Plotly. \n",
    "  * You typically use Pandas plotting capabilities for quick visualisations. For more elaborate visualisations, we will cover other libraries later in the course.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> .plot()\n",
    "\n",
    "You can use `.plot()` to visualise data in Pandas. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)\n",
    "* A relevant argument is `'kind'`, where you determine the kind of plot to produce. The options include:\n",
    "\n",
    "  * 'line': line plot (default)\n",
    "  * 'bar': vertical bar plot\n",
    "  * 'barh': horizontal bar plot\n",
    "  * 'hist': histogram\n",
    "  * 'box': boxplot\n",
    "  * 'kde': Kernel Density Estimation plot\n",
    "  * 'area': area plot\n",
    "  * 'pie': pie plot\n",
    "  * 'scatter': scatter plot\n",
    "\n",
    "\n",
    "\n",
    "To choose an appropriate argument for `kind` in this function requires an understanding of data types and how to visualise data. Experience gained through data projects will help with this.\n",
    "  * We will start exploring  the plots and present additional arguments\n",
    "\n",
    "---\n",
    "\n",
    "For the next set of exercises, we will use datasets from Seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "sns.get_dataset_names()\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Histograms\n",
    "\n",
    "We will consider the penguins dataset\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.head(50)\n",
    "df.head()\n",
    "\n",
    "Histogram\n",
    "  * A histogram is effective in visualizing how numerical data is distributed. It groups values into bins and displays a count of the data points whose values are in a particular bin.\n",
    "\n",
    "* We are interested to see how body mass is distributed.\n",
    "  * We set kind='hist', and y='body_mass_g'.\n",
    "  * Bins value is more like a \"trial and error\" exercise; you may start with a number and refine until you have a good visualisation\n",
    "  * We also set figsize and title\n",
    "\n",
    "* We may state that visually, the majority of the data is within the range of 3300 to 4500. There is a peak at around 4000. The data is not normally distributed (bell shape). A normal distribution is when data points tend to be around a central value with no bias to the left or the right. It's often termed a 'Bell Curve' because it looks like a bell.\n",
    "\n",
    "df.plot(kind='hist', y='body_mass_g', bins=75, figsize=(10,6), title='body mass distribution')\n",
    "plt.show()\n",
    "\n",
    "We can plot more than one distribution in one plot\n",
    "* Imagine if we are interested to see the distribution of `bill_length_mm` and `bill_depth_mm`\n",
    "  * We notice the values don't overlap; they have different distribution shapes.\n",
    "\n",
    "df.plot(kind='hist', y=['bill_length_mm',\t'bill_depth_mm'], bins=50, figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "df_practice = df_practice.head(50)\n",
    "df_practice.head(10)\n",
    "\n",
    "You are interested in finding out how often tips are given based on the value of the tip.\n",
    "* Create a chart of type hist using the tip column and having 40 bins and figsize values of 6,7.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "An alternative to a binned histogram is a **kde plot**\n",
    "  * It represents the distribution of a numeric variable.\n",
    "  * It uses the data using a density probability curve; therefore, the y-axis is called density. In the end, you will be interested in checking **which range the distribution is denser**, and **the shape of the distribution**. \n",
    "  * Compared to a histogram, kde shall draw a more interpretable plot. It doesn't need a bin argument since it automatically determines bandwidth.\n",
    "\n",
    "df.plot(kind='kde',y='body_mass_g',figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "The previous plot shows that body mass data is between roughly 2800 and 6500. And the majority, due to the peak, is between 3100 and 4100.\n",
    "\n",
    "\n",
    "Let's plot multiple distributions\n",
    "\n",
    "df.plot(kind='kde',y=['bill_length_mm',\t'bill_depth_mm'], figsize=(10,6), title='x,y,z Distribution')\n",
    "plt.show()\n",
    "\n",
    "Compare the kde with the histogram, and you will notice it conveys the same information (distribution shape and range) but in a different format\n",
    "\n",
    "df.plot(kind='hist', y=['bill_length_mm',\t'bill_depth_mm'], bins=50, figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "df_practice = df_practice.head(50)\n",
    "df_practice.head()\n",
    "\n",
    "You are interested in using a kde chart to determine what most restaurant patrons spend on their meals. You will need to use total_bill for the y parameter.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 16 - Pandas Data Visualisation Part 1C Boxplot\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Visualise data with Pandas built-in capabilities, using line plot, histogram and box plot\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pandas Data Visualisation Part 1\n",
    "\n",
    "### Disclaimer and imports\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **ou will only start understanding your data objectively when you start visualising it**\n",
    "* Pandas' has built-in methods/functions for visualising data that will help you to understand your data better.\n",
    "  * Even though it is related to Pandas, we need to import matplotlib, which we will study in more detail later in the course. For now, you just need to be aware of it.\n",
    "* Pandas' capabilities/flexibilities for plotting can not be compared to Matplotlib, Seaborn or Plotly. \n",
    "  * You typically use Pandas plotting capabilities for quick visualisations. For more elaborate visualisations, we will cover other libraries later in the course\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> .plot()\n",
    "\n",
    "You can use `.plot()` to visualise data in Pandas. The documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)\n",
    "* A relevant argument is `'kind'`, where you determine the kind of plot to produce. The options include:\n",
    "\n",
    "  * 'line': line plot (default)\n",
    "  * 'bar': vertical bar plot\n",
    "  * 'barh': horizontal bar plot\n",
    "  * 'hist': histogram\n",
    "  * 'box': boxplot\n",
    "  * 'kde': Kernel Density Estimation plot\n",
    "  * 'area': area plot\n",
    "  * 'pie': pie plot\n",
    "  * 'scatter': scatter plot\n",
    "\n",
    "\n",
    "\n",
    "To choose an appropriate argument for `kind` in this function requires an understanding of data types and how to visualise data. Experience gained through data projects will help with this.\n",
    "  * We will start exploring  the plots and present additional arguments\n",
    "\n",
    "---\n",
    "\n",
    "For the next set of exercises, we will use datasets from Seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "sns.get_dataset_names()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Box Plot\n",
    "\n",
    "A boxplot graphs data based on their quartiles. The first, second and third quartile numbers divide the data into approximately equal-sized quarters.  Boxplot is an approach to display the distribution of data based on five metrics that help to summarise a numerical distribution:\n",
    "  * minimum (min)\n",
    "  * first quartile (Q1)\n",
    "  * median\n",
    "  * third quartile (Q3)\n",
    "  * and maximum (max)\n",
    "\n",
    "    * The range between the first and third quartiles (Q1 - Q3), also known as the interquartile range (IQR), shows where your data is most frequent. \n",
    "  * The min and max show your data range (these points may be an outlier or not). In the previous plot, the min and max values are outliers.\n",
    "  * The outliers are data points that are dramatically different from other data points\n",
    "  * The upper and lower boundaries, where the data is not an outlier, are `Q3 + 1.5 x IQR` and `Q1 - 1.5 x IQR`\n",
    "  * You can check how tightly your data is grouped; the \"smaller\" the box, the more \"grouped\" the data is (lower data variance).\n",
    "  * We will study in more detail these terms in the Statistics lesson notebooks\n",
    "\n",
    "Let's consider the iris dataset. It contains records of three species or classes of iris plants, with petal and sepal measurements.\n",
    "\n",
    "df = sns.load_dataset('iris')\n",
    "df = df.head(50)\n",
    "df.head()\n",
    "\n",
    "We use `.plot()`, `kind='box'` and y are flowers measurements\n",
    "  * We notice circles or points at sepal width. These are outliers for that variable, meaning these are values that are not frequent (or are weird) for that dataset.\n",
    "\n",
    "df.plot(kind='box',y=['sepal_width'],figsize=(10,7))\n",
    "plt.show()\n",
    "\n",
    "Let's plot multiple numerical variables by parsing in a list of the desired variables\n",
    "\n",
    "df.plot(kind='box',y=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],figsize=(10,7))\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, day time, total bill, gender, if it is a smoker table or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "df_practice.head(10)\n",
    "\n",
    "You are interested in using a box plot to analyse the table bills of the restaurant. You will need to use total_bill for the y parameter.\n",
    "* By looking at the boxplot, what is the median level?\n",
    "* What is the range where the bills are more frequent? (Look for the Q1 - Q3 range)\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "At this point, you will notice that a boxplot, a histogram and kde plot convey similar information from different facets.\n",
    "* Let's plot the same information with histograms\n",
    "\n",
    "df.plot(kind='hist',bins=50 ,y=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "        figsize=(10,7),alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "And now with kde\n",
    "\n",
    "* You will notice negative values in the x-axis for petal length and width. We should be aware that these are not the actual values but the density probability.\n",
    "  * For example, for petal length, you can interpret the graph as showing you the most frequent value occurrences are roughly between [1.5 and 2.1] and  [3.5 and 6.5]. This last interval has more frequent occurrences  since the peak is higher than the other interval.\n",
    "\n",
    "df.plot(kind='kde',y=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],figsize=(10,7))\n",
    "plt.show()\n",
    "\n",
    "You may also notice that the plots from Pandas are simple in terms of layout and design. However, there are additional libraries that we will study in the upcoming lessons so that we can enhance the design and layout.\n",
    "* The idea, for now, is to make you familiar with these plots and be aware of Pandas' capabilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Unit 17 - Pandas Data Visualisation Part 2\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Visualise data with Pandas built-in capabilities, using scatter plot, area plot, bar plot and pie chart\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pandas Data Visualisation Part 2\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scatter Plot\n",
    "\n",
    "A scatter plot displays values for two numerical variables using cartesian coordinates. \n",
    "* The idea of a scatter plot is to understand the relationship between the variables and how they are distributed\n",
    "* It is possible to add more variables to the plot when these additional variables are coded as colour, shape or size.\n",
    "\n",
    "We will consider the 'penguins' dataset. It has records for three different species of penguins collected from 3 islands in the Palmer Archipelago, Antarctica\n",
    "\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(frac=0.33, random_state=1) # get a fraction of the data\n",
    "df.head()\n",
    "\n",
    "Let's plot a scattor plot setting `kind='scatter'`, `x='bill_length_mm'` and `y='bill_depth_mm'`\n",
    "\n",
    "df.plot(kind='scatter',x='bill_length_mm',y='bill_depth_mm',figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "You can add parameter `c` for colouring the plot based on another variable. The colour-map options are found [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html). In this case, we set `c='body_mass_g'`\n",
    "\n",
    "df.plot(kind='scatter',x='bill_length_mm',y='bill_depth_mm', c='body_mass_g',\n",
    "        figsize=(10,6),colormap='viridis')\n",
    "plt.show()\n",
    "\n",
    "Now we added parameter `s`, for size. The dots' size will be defined according to body_mass_g levels.  We also set alpha=0.7, meaning we set the level to 70% of transparency. That gives a better look to the plot, so the dots will not be so cluttered.\n",
    "\n",
    "df.plot(kind='scatter',x='bill_length_mm',y='bill_depth_mm',\n",
    "         c='body_mass_g',\n",
    "        s=df['body_mass_g'],\n",
    "        figsize=(10,6),\n",
    "        colormap='viridis',\n",
    "        alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "Note we divided `s` by 60 to scale better in the plot. \n",
    "* When you don't scale, the dots may be too big or small, so depending on the use case, it is worth dividing or multiplying by a factor.\n",
    "*The best factor value is more like a trial and error exercise; try a random value and play around until you reach a visually nice plot composition.\n",
    "\n",
    "df.plot(kind='scatter',x='bill_length_mm',y='bill_depth_mm',\n",
    "         c='body_mass_g',\n",
    "        s=df['body_mass_g']/60,  ### factor by 60\n",
    "        figsize=(10,6),\n",
    "        colormap='viridis',\n",
    "        alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "df_practice = df_practice.head(50)\n",
    "df_practice.head(10)\n",
    "\n",
    "You are interested in using a scatter chart to display the value of tips received to the total bill amount.\n",
    "* Use total_bill and tip for the x and y parameters and total_bill for the c parameter.\n",
    "* Use the size column for the s parameter and viridis for the colourmap\n",
    "* Choose values of your choice for the figsize.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Area Plot\n",
    "\n",
    "An area plot is very similar to a line plot. The difference is that the area between the x-axis and the line is filled with colour or shading. It gives the evolution of a numeric variable\n",
    "\n",
    "\n",
    "* We will consider a fictitious dataset considering Revenue, Margin and Volume\n",
    "\n",
    "df = pd.DataFrame(data={'Revenue': [300, 350, 500, 250, 250, 350, 400],\n",
    "                        'Margin': [150, 200, 400, 50, 180, 180, 200],\n",
    "                        'Volume': [26, 30, 40, 15, 15, 30, 25],},\n",
    "                  index=pd.date_range(start='2021/05/01',freq='D',periods=7))\n",
    "df\n",
    "\n",
    "*Let*'s use kind='area' and y as a list with the variables you are interested in plotting.\n",
    "* Note the values are stacked on each other, and for this context, it doesn't make much sense.\n",
    "* Depending on the context of your variables, it could be worth stacking.\n",
    "\n",
    "df.plot(kind='area', y=['Revenue','Margin', 'Volume'],figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "You should add the argument ``stacked=False``, so the variables are not stacked in the plots. For this context, it makes more sense not to have them stacked.\n",
    "\n",
    "df.plot(kind='area', y=['Revenue','Margin', 'Volume'],figsize=(10,5),stacked=False)\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will use the DataFrame below to create an area chart.\n",
    "\n",
    "\n",
    "\n",
    "df_practice\n",
    "data = {'Product':['Bread','Bread','Milk','Milk','Milk','Butter','Butter','Butter'],\n",
    "        'Person':['Anna','Anna','Brian','John','John','Carl','Sarah','Anna'],\n",
    "        'Sales':[200,120,340,124,243,350,500,240],\n",
    "        'Quantity':[3,5,3,8,2,7,5,4],\n",
    "        'Margin':[160,20,280,50,100,67,300,200]}\n",
    "\n",
    "df_practice = pd.DataFrame(data)\n",
    "df_practice\n",
    "\n",
    "Using an area chart, having figsize of 10,5 and not stacked, you would like to plot Sales and Margins.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Bar Plot\n",
    "\n",
    "A Bar plot shows categorical data as bars with lengths proportional to the values they represent. Bar plots can be used to compare categories. In the x-axis, we find the categories being compared, and in th y-axis, their measured values.\n",
    "\n",
    "* Let's consider the penguins' dataset. It has records for three different species of penguins collected from 3 islands in the Palmer Archipelago, Antarctica\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(frac=0.1, random_state=1) # get a fraction of the data\n",
    "df.head()\n",
    "\n",
    "We can plot the distribution of species. \n",
    "* First, we need to do a value_counts() of species as we need to be working with numerical data, and then we can plot.\n",
    "\n",
    "df.value_counts('species')\n",
    "\n",
    "Now we plot it using `kind='bar'`\n",
    "\n",
    "df.value_counts('species').plot(kind='bar',figsize=(10,4))\n",
    "plt.show()\n",
    "\n",
    "We can use `kind='barh'` to make horizontal bars\n",
    "\n",
    "df.value_counts('island').plot(kind='barh',figsize=(10,4))\n",
    "plt.show()\n",
    "\n",
    "We may be interested in plotting all categorical variables in a dataset. For that, we combine a `for`, `.select_dtypes()` and `.value_counts().plot()`\n",
    "* The logic is to loop only on variables that are object or categorical, and for these, plot the distribution levels\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "  df.value_counts(col).plot(kind='bar',figsize=(10,3))\n",
    "  print(f\"* {col} bar plot\")\n",
    "  plt.show()\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "We may be interested in stacking categorical variables to another, such as showing species distribution across the islands. \n",
    "First we `groupby().size().unstack()` to get the data in a processed format for plotting\n",
    "\n",
    "df.groupby(['island','species']).size().unstack()\n",
    "\n",
    "Then we plot as we learned, but adding the parameter `stacked=True`\n",
    "\n",
    "df.groupby(['island','species']).size().unstack().plot(kind='bar', stacked=True,figsize=(10,4))\n",
    "plt.show()\n",
    "\n",
    "We can loop over the categorical variables plotting the frequencies across the remaining categorical variables\n",
    "* The logic uses a function called ``combinations`` that combine unique pairs of categorical variables and create a stacked bar plot\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "for col1,col2 in combinations(df.select_dtypes(include='object').columns,2):\n",
    "    print(f\"* {col1} bar plot - stacked by {col2}\")\n",
    "    df.groupby([col1,col2]).size().unstack().plot(kind='bar', stacked=True,figsize=(10,4))\n",
    "    plt.show()\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "df_practice = df_practice.head(50)\n",
    "df_practice.head(10)\n",
    "\n",
    "Feel free to try out your ideas or use the following suggestion.\n",
    "\n",
    "You are interested in creating a stacked bar chart of categorical data grouped by day and time as you wish to easily see what times of the day are busiest at lunchtime or dinner time.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pie Chart\n",
    "\n",
    "A pie chart shows the data composition. It is useful when the data is static, which means there is no variation over time. It is a proportional representation of categorical data. It is a circular plot where each slice represents a part of the whole.\n",
    "\n",
    "\n",
    "* Let's use the penguins' dataset. It has records for three different species of penguins collected from 3 islands in the Palmer Archipelago, Antarctica\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.head(50)\n",
    "df.head()\n",
    "\n",
    "First we count the frequencies for a given variable using `.value_counts()` using `normalize=True`, so we get the proportions\n",
    "\n",
    "df.value_counts('sex',normalize=True)\n",
    "\n",
    "Then we plot using `kind='pie'`. `ylabel` is set to a blank space; otherwise, it would appear as 'None' on the left side of the plot. ``title`` is set to the variable name\n",
    "\n",
    "df.value_counts('sex',normalize=True).plot(kind='pie', ylabel=' ', title='sex pie chart')\n",
    "plt.show()\n",
    "\n",
    "We are again scanning over the categorical variables and making a pie plot using the previous use case\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "  df.value_counts(col,normalize=True).plot(kind='pie',ylabel=' ', title=f\"{col} Pie chart\")\n",
    "  plt.show()\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> **PRACTICE**: We will consider the tips dataset for practice. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "\n",
    "\n",
    "df_practice = sns.load_dataset('tips')\n",
    "# there are some variables with data type of 'category', we replace for 'object\n",
    "for col in df_practice.select_dtypes(include='category'):\n",
    "  df_practice[col] = df_practice[col].astype('object')\n",
    "\n",
    "df_practice.head(10)\n",
    "\n",
    "We are looking for pie charts of the categorical data from our DataFrame\n",
    "* This will be the same as what you saw in the lesson material.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
