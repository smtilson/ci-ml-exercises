{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 01 - ML Pipeline and ML tasks\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%201%20-%20Lesson%20Learning%20Outcome.png\"> Lesson Learning Outcome\n",
    "\n",
    "* **Scikit-learn Lesson consists of nine units.**\n",
    "* By the end of this lesson, you should be able to:\n",
    "  * Learn and use the workflow for training and evaluating the ML pipeline.\n",
    "  * Create a pipeline according to our dataset and ML task.\n",
    "  * Fit Regression, Classification, Cluster, PCA (Principal Component Analysis), and NLP (Natural Language Processing) considering different algorithms.\n",
    "  * Learn and use the code to fit in one turn, multiple algorithms with hyperparameters optimisation.\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Reinforce ML pipeline concepts and the ML tasks that are covered in upcoming notebooks.\n",
    "* Learn and use the workflow for training and evaluating the ML pipeline.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn allows you to train machine learning models for classification, regression or clustering. In addition, it provides a wide set of functions for data processing, dimensionality reduction, feature engineering, feature scaling, feature selection, tuning model hyperparameters, creating an ML pipeline, evaluating a model's performance and more.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png\n",
    "\">\n",
    " **Why do we study Scikit-learn?**\n",
    "  * Because it is a centralised and complete library for conventional ML, containing a suite of practical modules that helps the data practitioners from the development to the deployment of ML pipelines.\n",
    "\n",
    "\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%203%20-%20Additional%20Learning%20Context.png\"> Additional Learning Context\n",
    "\n",
    "* We encourage you to:\n",
    "  * Add **code cells and try out** other possibilities, i.e.: play around with parameter values in a function/method, or consider additional function parameters etc.\n",
    "  * Also, **add your comments** in the cells. It can help you to consolidate your learning. \n",
    "\n",
    "* Parameters in given function/method\n",
    "  * As you may expect, a given function in a package may contain multiple parameters. \n",
    "  * Some of them are mandatory to declare; some have pre-defined values, and some are optional. We will cover the most common parameters used/employed in Data Science for a particular function/method. \n",
    "  * However, you may seek additional in the respective package documentation, where you will find instructions on how to use a given function/method. The studied packages are open source, so this documentation is public.\n",
    "  * **For Scikit learn the link is [here](https://scikit-learn.org/stable/g/). We also will use the XGBoost library to train pipelines with eXtreme Gradient Boosting, which is a tree-based algorithm. The documentation is [here](https://xgboost.readthedocs.io/en/latest/index.html)**.\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 01 - ML Pipeline and ML tasks\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Introduction\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In a nutshell, Machine learning is a data-driven approach that uses algorithms to learn patterns and relationships from the data, without being explicitly programmed. \n",
    "* The developer gives the algorithm data and an objective. The algorithm is trained and figures out how to match the objective based on the provided data.\n",
    "* This creates a model, and the trained model is used for predicting behaviours and outputs, allowing decision-making on unseen data.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> ML is heavily applied in practical terms for multiple use cases in many industries, examples include:\n",
    "* E-mail spam detection\n",
    "* Customer Churn\n",
    "* Text Sentiment Analysis\n",
    "* Fraud Detection\n",
    "* Real-time Ads\n",
    "* Recommendation Engine (i.e.: While watching streaming movies and after finishing one movie, you receive suggestions on what to watch next.)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will explore\n",
    "* Pipeline concepts\n",
    "* Data Cleaning and Feature Engineering\n",
    "* Feature Scaling and Feature Selection\n",
    "* ML tasks covered in this lesson\n",
    "* General Workflow\n",
    "\n",
    "**Note**\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">\n",
    "* The overall perception of difficulty may escalate in the following notebooks since we will start using a series of concepts we covered in the videos and the previous notebooks but in more practical terms.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pipeline concepts\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In the previous lesson, we introduced scikit-learn that creates a Pipeline, a sequence of tasks.\n",
    "* In ML, we are interested in arranging a sequence of tasks that are in line with the ML process of **data cleaning, feature engineering, feature scaling, feature selection and model**\n",
    "* In an ML pipeline, the last step is typically the model, and the preceding steps prepare the data for the model\n",
    "\n",
    "We import Pipeline from sklearn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In addition, the pipeline should identify two outcomes; the training outcome and the prediction outcome. \n",
    "* For that, we use estimators as part of the pipeline steps. There are two types of estimators mainly used: predictors and transformers.\n",
    "  * A predictor estimator, uses methods like **.fit()** and **.predict()**. An ML model uses these methods to learn patterns from the data and is used for subsequent predictions.\n",
    "  * On the other hand, the transformer estimator uses the methods **.fit()** and **.transform()** because it learns from the data and later transforms the data with better distribution. \n",
    "  \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will demonstrate the differences between fitting models with and without a pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Data Cleaning and Feature Engineering\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We studied in the feature-engine lesson common techniques to handle data cleaning and feature engineering tasks, using feature-engines built-in transformers or creating your own transformer.\n",
    "* In addition, we arranged this transformer in a pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Scaling and Feature Selection\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Once the data is cleaned and engineered, you should consider feature scaling and feature selection. We have studied the definitions in the Module: Machine Learning Essentials / Section: ML Pipeline. Please refer to it if you need refreshing.\n",
    "\n",
    "* In this section, we will cover the practical step of feature scaling and feature selection.\n",
    "\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Scaling\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The scale of a feature is an important aspect when fitting a model. For example, there are algorithms like K-means clustering, Linear and Logistic Regression, and Neural Networks that are highly affected by the scale of their features.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit-learn [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html), feature scaling can be an important preprocessing step for many machine-learning algorithms. Standardisation involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "* The idea behind scaling the features is to make all features have a similar scale.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will present `StandardScaler()`, which standardises the data: it centres the variable at zero. It sets the variance to 1, by subtracting the mean from each observation and dividing by the standard deviation. It is also known as the Z-score. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "* We will cover the StandardScaler transformer in the course as a first go-to option for feature scaling. However, there are other alternatives, and you may check the [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) to learn more. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The tradeoff of feature scaling is that the variable distribution will be slightly different. Still, we will create better conditions for the algorithm to learn the patterns and relationships in the data and generalize on unseen data.\n",
    "\n",
    "Let's use the iris dataset\n",
    "\n",
    "df =  sns.load_dataset('iris')\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "We will import `StandardScaler()`\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "We create a pipeline with a step called 'feature_scaling' and attach `StandardScaler()`. When you don't parse any variables to it, it scales all variables\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "      (\"feature_scaling\", StandardScaler()) \n",
    "  ])\n",
    "\n",
    "We will apply this pipeline to the features in the train set. We will learn how to split data soon, but for now, we will manually create a train set and a test set, where each has a set for features and the target variable.\n",
    "* In this dataset, features are `['sepal_length', 'sepal_width', 'petal_length', 'petal_width']` and target is `['species']`. We shuffle the data and will get the first 100 rows and set them as the train set. The remaining goes to the test.\n",
    "* <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> There is a proper way to split a train and test set. We will cover that soon.\n",
    "* The central point is to have 2 sets (Train and Test) and have features and the target separated.\n",
    "\n",
    "\n",
    "Let's shuffle the data. we use `.sample(frac=1)`, the documentation link is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html). It returns a random sample from the data.\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "df.head()\n",
    "\n",
    "The train set features are X_train and have the first 100 rows. The train set target is y_train and has the last 50 rows from species. The same rationale goes for the test set, x_test has the first 100 rows and y_test the last 50 rows.\n",
    "\n",
    "X_train = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']][:100]\n",
    "y_train =  df[['species']][:100]\n",
    "X_test =  df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']][100:]\n",
    "y_test =  df[['species']][100:]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "We check the DataFrames dimensions\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "When applying pipelines to ML, we fit the pipeline to the train set (so it will learn the parameters) and based on this learning, transform the data on the train and test set\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "X_train_scaled = pipeline.transform(X_train)\n",
    "X_test_scaled = pipeline.transform(X_test)\n",
    "\n",
    "One caveat of using sklearn transformers is that they output NumPy arrays, instead of Pandas DataFrames. You may remember that the feature-engine outputs DataFrames. \n",
    "\n",
    "type(X_train_scaled)\n",
    "\n",
    "So we need an additional step to convert the scaled data back to a DataFrame.\n",
    "\n",
    "X_train_scaled = pd.DataFrame(data= X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(data= X_test_scaled, columns=X_train.columns)\n",
    "\n",
    "Now we are fine to move on, the dataset is a DataFrame\n",
    "\n",
    "type(X_train_scaled)\n",
    "\n",
    "We are now interested to see the difference in each feature before and after applying StandardScaler().\n",
    "* We create a logic to loop on each feature and plot two histograms in the same plot. One shows the data distribution before applying ``StandardScaler()`` and the other after applying it.\n",
    "* The blue plot is before applying, and the red is after. Note that the red histograms are centred at zero on the x-axis. You will notice the distribution may change a bit, but that is part of the tradeoff we mentioned earlier.\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "for col in X_train.columns:\n",
    "  fig, axes = plt.subplots(figsize=(8,5))\n",
    "  sns.histplot(data=X_train, x=col, kde=True, color='b',  ax=axes)\n",
    "  sns.histplot(data=X_train_scaled, x=col, kde=True,color='r', ax=axes)\n",
    "  axes.set_title(f\"{col}\")\n",
    "  axes.legend(labels=['Before Scaling', 'After Scaling'])\n",
    "  plt.show()\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Selection\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> The primary goal of feature selection is to have a process to select the relevant features for fitting an ML model. \n",
    "\n",
    "That is important since: \n",
    "* Models with fewer and more relevant features are simpler to interpret.\n",
    "* You reduce the chance of overfitting by removing features that may add little information or noise.\n",
    "* You reduce the time needed to train the models.\n",
    "* You reduce the feature space. You require less effort from the software development team to design and implement the interface (either API or dashboard) in the production environment.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> This step can be seen as a combination of search techniques to look for a subset of features and an evaluation measure that scores the different feature subsets. There are a few methods for feature selection:\n",
    "* Filter Method\n",
    "* Wrapper Method\n",
    "* Embedded Method\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In the course, and as a starting point in your career, you will use the Embedded method.\n",
    "* It is named the embedded method since it performs feature selection during the model training. It finds the feature subset for the algorithm that is being trained.\n",
    "* The method automatically trains an ML model, and then derives feature importance from it, removing non-relevant features using the derived feature importance.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> For example:\n",
    "* Suppose your pipeline is considering a Decision Tree algorithm in the model step. In that case, you can add before the model step a feature selection step using an embedded method considering a Decision Tree.\n",
    "\n",
    "\n",
    "Let's reuse the same data from the previous exercise: the iris dataset.\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "\n",
    "We are using `SelectFromModel()` as the method. Its documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html). \n",
    "* The argument is the algorithm you are considering in the pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "We create a pipeline using a Decision Tree algorithm that contains three steps:\n",
    "* `feature_scaling`: like we saw in the previous example.\n",
    "* `feature_selection`: use SelectFromModel considering the same algorithm from the model step.\n",
    "* `model`: uses a Decision Tree algorithm (we will get into more details in upcoming units, for now, take this step as the model step and let's use a decision tree for the example.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( \"feature_scaling\", StandardScaler() ),\n",
    "      ( \"feature_selection\", SelectFromModel(DecisionTreeClassifier(random_state=101)) ),\n",
    "      ( \"model\", DecisionTreeClassifier(random_state=101) ),\n",
    "  ])\n",
    "\n",
    "pipeline\n",
    "\n",
    "We fit the pipeline with the Train set.\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "And access the feature_selection step using bracket notation as we saw in the feature-engine lesson.\n",
    "\n",
    "pipeline['feature_selection']\n",
    "\n",
    "That was not informative. We need to use `.get_support()` to access which features were selected by this step. \n",
    "* The output is a boolean list, where its length and order are related to the original feature space.\n",
    "* For example, the train set has four features. We see that the feature_selection step selected the last two steps since they are True. The first two features were not considered since they are False in the boolean list.\n",
    "\n",
    "pipeline['feature_selection'].get_support()\n",
    "\n",
    "However, we want to know the features list that was selected, not a boolean list.\n",
    "* We then use this boolean list to subset the features.\n",
    "* A quick recap on the features list.\n",
    "\n",
    "X_train.columns\n",
    "\n",
    "We use the boolean list to subset the previous list.\n",
    "* And here we have the features that were considered important for that given dataset using that given algorithm.\n",
    "\n",
    "X_train.columns[pipeline['feature_selection'].get_support()] \n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> ML tasks\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this lesson, we will explore business cases that involve the following ML tasks:\n",
    "* Regression\n",
    "* Classification (Binary and Multi-class)\n",
    "* Clustering\n",
    "* NLP (Natural Language processing)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will use structured and tabular datasets from ML libraries like Seaborn, Plotly, Scikit-learn and Yellow-brick.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> General Workflow\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In a practical project, you can use CRISP-DM workflow to manage project steps. In case you want a refresher on the workflow, revert to the Module Delivering Data Science projects.\n",
    "* For this lesson, we will focus on the following CRISP-DM steps: data understanding, data preparation, modelling and evaluation.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Therefore, when you reach the modelling phase in a project, it is assumed you have collected the data, conducted an EDA, and defined the pipeline steps.\n",
    "\n",
    "* When modelling, for supervised learning, you will typically use an overall workflow like:\n",
    "  * Split the dataset into train and test set\n",
    "  * Fit the model (either a pipeline or not) \n",
    "  * Evaluate your model. If performance is not good, revisit the process, starting from collecting the data, conducting EDA etc\n",
    "\n",
    "\n",
    "There are some potential small variations to this workflow, but this is the starting point we consider in your journey of modelling\n",
    "\n",
    "\n",
    " **HUGE WARNING** <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> \n",
    "* **Reflect** for a second on how many steps and considerations you need before fitting a model. You will be surprised that the modelling phase will take a small percentage of your time and attention in a project where a person is responsible from end to end.\n",
    "\n",
    "* Even though, this phase is critical to your project, **so let's stop the reading/talking and let's fit some models**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn - Unit 02 - Split your data, fit a model, predict and save the model\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and implement the basic workflow for splitting the data, fitting a model, predicting on data and saving the model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 02 - Split your data, fit a model and predict\n",
    "\n",
    "In this unit, we will cover how to:\n",
    "  * Split your data\n",
    "  * Fit a model\n",
    "  * Run predictions with the fitted model\n",
    "  * Save the model, so you can use it later\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In supervised learning, you are interested in splitting your data. In conventional ML, like in Scikit-learn, we will split the data into Train and Test sets.\n",
    "* The validation set is a part of the Train set. When using a specific Scikit-learn function for hyperparameter optimization, the validation set is grabbed automatically. Therefore we will split into Train and Test sets only.\n",
    "* If you want a refresher on Train, Validation, and Test sets, refer to Module 2 - ML Essentials.\n",
    "\n",
    "Let's consider the iris dataset. It contains records of three classes of iris plants, with petal and sepal measurements.\n",
    "\n",
    "df = sns.load_dataset('iris')\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> How do you know which variables are features and which variable is a target variable?\n",
    "* It will depend on the context of your ML project. You will need to know or need to investigate the objective of your ML project to determine features and the target.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For the purposes of this notebook, species is our chosen target variable. There are three species. We need to classify the species according to the flower's petal and sepal. Our ML task then will be a classification.\n",
    "\n",
    "df['species'].value_counts()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Split your data\n",
    "\n",
    "We use `train_test_split()` to split the data. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). The parameters we will use are:\n",
    "* The first two are the features and target, respectively. In this case, for the features, you drop species, and for the target, you subset species.\n",
    "* ``test_size:`` it represents the data proportion to include in the test set. We set it at 0.2\n",
    "* ``random_state:`` according to the documentation, it controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. It can be any positive integer. We suggest keeping the same random_state value across your project. We will select here 101\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> `random_state` is a critical parameter in ML, which we will use in other use cases. It essentially gives **REPRODUCIBILITY** to your project. That means the same result you get here right now, another person will get elsewhere at another time.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['species'],axis=1),\n",
    "                                                    df['species'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=101)\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "Let's have look at `X_train`\n",
    "* Those will be the features used to train the model\n",
    "* Note the features are numbers. Scikit-learn uses numbers to fit models. That is why we have to encode categorical data\n",
    "* In this dataset, we don't need any data cleaning or categorical encoding\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "Let's inspect `y_train`. These are categories.\n",
    "* When the ML task is classification, Scikit-learn handles either numbers or categories for the target variable.\n",
    "\n",
    "y_train\n",
    "\n",
    "In addition, `y_train` is a Pandas Series\n",
    "\n",
    "type(y_train)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit your model\n",
    "\n",
    "You get a preview of tree-based algorithms in the 'Machine Learning Essentials' Algorithms unit. Even though we have a dedicated unit for tree-based algorithms, here we will use a decision tree algorithm to fit a model to demonstrate the basic workflow for fitting a model.\n",
    "* We will use `DecisionTreeClassifier()`, the documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* We create a python object/variable called model, and instantiate `DecisionTreeClassifier()`. A common convention is to set the object name as a model.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">  Note: we created a model and fit. We can do that since the data doesn't require a pre-processing step, like data cleaning or categorical encoding, for the fitting.\n",
    "\n",
    "* Fitting the model on its own is fine as a learning experience. However, in our later exercises, we will not fit the model but instead use a pipeline that contains a series of steps, where typically, the last step will be the model.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "Next, we fit the model with the train set - features (`X_train`) and target (`y_train`)\n",
    "* We use `.fit()` method and parse `X_train` and `y_train`. Simple as that.\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Run predictions\n",
    "\n",
    "Let's predict the test set using our model.\n",
    "* We use `.predict()` and parse the test set features (`X_test`)\n",
    "* The answer is an array\n",
    "\n",
    "model.predict(X_test)\n",
    "\n",
    "You can predict the probability (between 0.0 and 1.0) for each class for a given observation using `.predict_proba()`\n",
    "\n",
    "model.predict_proba(X_test)\n",
    "\n",
    "Ideally, we should predict on the Train and Test set, set a performance metric and evaluate model performance.\n",
    "* We will not evaluate the model yet. We will leave it until another unit\n",
    "* The idea here is to feel how it works 'under the hood' when doing a basic training and predicting process.\n",
    "\n",
    "Let's assume now you want to predict on real-time data.\n",
    "* In an application, you will likely create an interface to collect the data or will get the data from somewhere else, from an API, for example.\n",
    "* In this case, we will manually create a DataFrame that contains the features. We call that X_live. It will have one row only (you could have a set of rows, that would mean running predictions in a batch, in our case, it is only one prediction)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In theory, you can set any value to the variable, But in practice, the values will follow the actual data distribution.\n",
    "\n",
    "X_live = pd.DataFrame(data={'sepal_length':6.0,\n",
    "                            'sepal_width':3.9,\n",
    "                            'petal_length':2.5,\n",
    "                            'petal_width':0.9},\n",
    "                      index=[0] # the DataFrame needs an index (either number or category), we just parsed the number 0\n",
    "                      )\n",
    "X_live\n",
    "\n",
    "Let's predict using this live data.\n",
    "\n",
    "model.predict(X_live)\n",
    "\n",
    "The model is 100% confident it is a determined class\n",
    "\n",
    "model.predict_proba(X_live)\n",
    "\n",
    "We saw already this class is Versicolor, but you cross-check the labels orders with .unique()\n",
    "\n",
    "df['species'].unique()\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Save your model\n",
    "\n",
    "We can save either an ML model or an ML pipeline as a .pkl file with a library called joblib.\n",
    "* You need the function `joblib.dump()`, the documentation is [here](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html). We will parse the `value` of the arguments as the file we want to save and the `filename` as the directory + filename + .pkl (we are saving at the root level) \n",
    "\n",
    "import joblib\n",
    "joblib.dump(value=model , filename=\"my_first_ml_model.pkl\")\n",
    "\n",
    "Once you are in an application or in another notebook, you can load with `joblib.load()`. The documentation is [here](https://joblib.readthedocs.io/en/latest/generated/joblib.load.html). You will parse the argument `filename` as the directory + filename + .pkl\n",
    "\n",
    "loaded_model = joblib.load(filename=\"my_first_ml_model.pkl\")\n",
    "loaded_model\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Awesome!! \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Does that mean I am ready to create an ML model for the world and solve big challenges?\n",
    "* Almost. We've started the ML journey now! \n",
    "* We still need to cover more topics. Now let's have some fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn - Unit 03 - Linear Models for Regression and Classification\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Implement and Evaluate Linear Models for Regression and Classification\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 03 - Linear Models for Regression and Classification\n",
    "\n",
    "\n",
    "In this unit, we will cover the practical steps and code to fit a pipeline considering Linear Regression and Logistic Regression.\n",
    "* In case you want a reminder of the theory, refer back to the Introduction to Predictive Analytics And Machine Learning and particularly to the Machine Learning Essentials > Machine Learning Terminology > Train/Fit a Model.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A typical workflow used for supervised learning is:\n",
    "* Split the dataset into train and test set\n",
    "* Fit the model (either using a pipeline or not) \n",
    "* Evaluate your model. If performance is not good, revisit the process, starting from collecting the data, conducting EDA (Exploratory Data Analysis) etc.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Regression: Linear Regression\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the Boston dataset from sklearn. It has house price records and characteristics, like the average number of rooms per dwelling and the per capita crime rate in Boston.\n",
    "\n",
    "* The approach to load the data from sklearn is a bit different from seaborn.\n",
    "* In this case, data comes as a \"dictionary\" where you need to grab different pieces (like data.data, data.features_names, data.target) to arrange the DataFrame\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Just as an aside this data won't need cleaning or feature engineering to train a model.\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = pd.Series(data.target)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> As our workflow suggests, we split the data into train and test set\n",
    "* we parse the features (`the full data dropping the target`) and the target (`df['target']`)\n",
    "* test_size is 20%, random_state is 101 - from now on, we want always to use  these values\n",
    "* It is a good practice to inspect the train and test set shape, just a sanity check.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df.drop(['target'],axis=1),\n",
    "                                    df['target'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Our target variable is the house price, which is a continuous variable. So we will create a pipeline to handle that.\n",
    "\n",
    "* We import Pipeline, StandardScaler and select from the model\n",
    "* To speed up the process, we know the dataset doesn't require any data cleaning or feature engineering. When we work with a dataset that needs it, we will inform you and suggest a transformer for that. In the workplace, that will be the data practitioner's task. But for learning purposes, we focus on the modelling and evaluation aspects.\n",
    "* We also import the linear regression algorithm. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* We define a function to create a pipeline with three steps: feature scaling, feature selection and model. It is convenient to arrange everything in a custom function for a given pipeline.\n",
    "* Just to emphasise, in the feature selection, we parse to ``SelectFromModel()`` the model we will use, in this case, Linear Regression.\n",
    "\n",
    "**WARNING** <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> \n",
    "* The code is already written here, but you will likely write it for yourself for your milestone project and in the workplace.\n",
    "* We are familiar with the idea of arranging the pipeline in a series of steps, however mistyping code is super common, as you may already know, so when you write the pipeline, please remember you will  almost certainly mistype or miss out on commas or parenthesis \"(\". If you mistype something, don't worry, as the code will alert you with an error. \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def pipeline_linear_regression():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(LinearRegression()) ),\n",
    "      ( \"model\", LinearRegression()),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "pipeline_linear_regression()\n",
    "\n",
    "We define the object `pipeline` based on `pipeline_linear_regression()`, then fit the train set (X_train and y_train)\n",
    "\n",
    "pipeline = pipeline_linear_regression()\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Great. Once the pipeline is fitted, we want to start evaluating it. First, we want to know the linear model coefficients, by extracting from the model the attribute `.coef_`\n",
    "* We create a custom function to grab that in place in a DataFrame together with the columns and sort by the absolute values from the coefficients\n",
    "\n",
    "def linear_model_coefficients(model, columns):\n",
    "  print(f\"* Interception: {model.intercept_}\")\n",
    "  coeff_df = (pd.DataFrame(model.coef_,columns,columns=['Coefficient'])\n",
    "            .sort_values(['Coefficient'],key=abs, ascending=False)\n",
    "            )\n",
    "  print(\"* Coefficients\")\n",
    "  print(coeff_df)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> As we have seen, we need to parse the model and the columns that the pipeline is trained on\n",
    "* To parse the model only, we subset the model from the pipeline with `pipeline['model']` (in this case, we named this step as 'model', but you could have named it as 'ml_model', so you would use the step name notation)\n",
    "* To parse the columns, we subset the feature selection step where we grab a boolean array informing which features hit the model - `pipeline['feat_selection'].get_support()`. Then this array is used to subset the features from train set columns.\n",
    "\n",
    "Let's make one exercise to visualise everything we read.\n",
    "* here we subset the model step from the pipeline\n",
    "\n",
    "pipeline['model']\n",
    "\n",
    "Here we subset the boolean array that tells which features hit the model\n",
    "* Note the first element is False, meaning the first feature from the train set was removed in this step.\n",
    "\n",
    "pipeline['feat_selection'].get_support()\n",
    "\n",
    "Here we parse this array to train set columns\n",
    "\n",
    "X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Now that we are comfortable with what happens on the back end for extracting information from the pipeline, we want to learn the model coefficients.\n",
    "* Do you remember the intercept and beta coefficients in the algorithms lesson? Here they are. In this case, it is a multiple linear regression since we have multiple features hitting the model.\n",
    "* We notice that LSTAT has the highest absolute value. That indicates it is the most important feature for this model. But then we ask: is this model good?\n",
    "\n",
    "linear_model_coefficients(model=pipeline['model'],\n",
    "                          columns=X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we want to evaluate how good the pipeline fits the train and test set\n",
    "* In case you want to revise the performance metrics for regression, refer to the Performance Metrics video in Machine Learning Essentials > Machine Learning Terminology.\n",
    "* Read the pseudo code to understand the logic better. The main aspect now is to understand the logic and why it is important for us now.\n",
    "* We will use these functions in the rest of the course when we evaluate regression models.\n",
    "\n",
    "\n",
    "# import regression metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "# we will use numpy to calcuate RMSE based on MSE (mean_squared_error)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets train/test sets and pipeline and evaluates the performance\n",
    "  - for each set (train and test) call regression_evaluation()\n",
    "  which will evaluate the pipeline performance\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Model Evaluation \\n\")\n",
    "  print(\"* Train Set\")\n",
    "  regression_evaluation(X_train,y_train,pipeline)\n",
    "  print(\"* Test Set\")\n",
    "  regression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets features and target (either from train or test set) and pipeline\n",
    "  - it predicts using the pipeline and the features\n",
    "  - calculates performance metrics comparing the prediction to the target\n",
    "  \"\"\"\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  \"\"\"\n",
    "  # Gets Train and Test set (features and target), pipeline, and adjust dots transparency \n",
    "  at scatter plot\n",
    "  - It predicts on train and test set\n",
    "  - It creates Actual vs Prediction scatterplots, for train and test set\n",
    "  - It draws a red diagonal line. In theory, a good regressor should predict\n",
    "  close to the actual, meaning the dot should be close to the diagonal red line\n",
    "  The closer the dots are to the line, the better\n",
    "\n",
    "  \"\"\"\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "Let's use the custom regression evaluation function.\n",
    "* Note the performance on the train and test set are not too different. That is an indication that the model didn't overfit.\n",
    "* At the same time, the test set performance (which is the best data to simulate real data since the model has never seen it) has an R2 performance of 0.67. This is not too good and not too bad. You may want to look for something better, but it is a good R2 value for a first model.\n",
    "* We also note in the plots that Prediction x Actual plot, the predictions tend to follow the actual value (since it kind of follows the red diagonal line)\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train,\n",
    "                            X_test, y_test,\n",
    "                            pipeline,alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Classification: Logistic Regression\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the breast cancer dataset from sklearn. It shows records for a breast mass sample and a diagnosis informing whether it is a 0 (Malignant), 1 (Benign)\n",
    "* The approach to load the data from sklearn is a bit different from seaborn.\n",
    "* In this case, `data` comes as a \"dictionary\" where you need to grab different pieces (like data.data, data.features_names, data.target) to arrange the DataFrame \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> As an aside, this data won't need data cleaning or feature engineering to train a model.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df['target'] = pd.Series(data.target)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  As usual, we start our workflow by splitting the data into train and test sets.\n",
    "* We use the same pattern as the previous section\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df.drop(['target'],axis=1),\n",
    "                                    df['target'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Our target variable is 0 (Malignant) and 1 (Benign), which is a categorical variable. We will create a pipeline to handle that, it will be a binary classifier.\n",
    "* We also import the logistic regression algorithm. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* We define a function to create a pipeline with three steps: feature scaling, feature selection and model. It is convenient to arrange everything in a custom function for a given pipeline.\n",
    "* Just a reminder, we parse the model to `SelectFromModel()` in the feature selection. We will use this pattern all the time. In this case, we will use Logistic Regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def pipeline_logistic_regression():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(LogisticRegression(random_state=101)) ),\n",
    "      ( \"model\", LogisticRegression(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We define the object pipeline based on pipeline_logistic_regression(), then fit to the train set (X_train and y_train)\n",
    "\n",
    "pipeline = pipeline_logistic_regression()\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Great. Once the pipeline is fitted, we want to start evaluating it. First, we want to know the model coefficients, by extracting from the model the attribute `.coef_`\n",
    "* We create a custom function to grab that in place in a DataFrame together with the columns, then we transpose it and sort by the absolute values from the coefficients\n",
    "\n",
    "def logistic_regression_coef(model, columns):\n",
    "  coeff_df = (pd.DataFrame(model.coef_,index=['Coefficient'],columns=columns)\n",
    "            .T\n",
    "            .sort_values(['Coefficient'],key=abs, ascending=False)\n",
    "            )\n",
    "  print(coeff_df)\n",
    "\n",
    "We parse the data in a similar way to the previous section:\n",
    "* the model as the model step from the pipeline\n",
    "* the columns as the train set features, subset by an array that tells which features were selected by `feat_selection` pipeline step\n",
    "\n",
    "logistic_regression_coef(model=pipeline['model'],\n",
    "                         columns=X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we want to evaluate how good the pipeline fits the train and test set\n",
    "* In case you want to revise the performance metrics for regression, refer to the Performance Metrics video in Machine Learning Essentials > Machine Learning Terminology.\n",
    "* Read the pseudo code to understand the logic better. The main aspect now is to understand the logic and why it is important for us now.\n",
    "* We will use these functions in the rest of the course when we evaluate classification models.\n",
    "\n",
    "# loads confusion_matrix and classification_report from sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  # Gets features, target, pipeline and how the levels from your target are labelled (named)\n",
    "  in this case, 0 (Malignant) and 1 (Benign), so you parse a list ['Malignant' , 'Benign']\n",
    "\n",
    "  - it predicts based on features\n",
    "  - compare predictions and actuals in a confusion matrix\n",
    "    - the first argument stays as rows and the second stay as columns in the matrix\n",
    "    - we will use the pattern where the predictions are in the row and actual values are in the columns\n",
    "    - for a  refresher on that, revert to the Performance Metric video in Module 2\n",
    "  - show classification report\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  # gets the features and target from train and test set, pipeline, and how\n",
    "  you labelled (named) the levels from your target\n",
    "  in this case, 0 (Malignant) and 1 (Benign), so you parse a list ['Malignant', 'Benign']\n",
    "  - for each set (train and test), it calls the function above to show the confusion matrix\n",
    "  and classification report for both train and test set\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's use the custom classification evaluation function.\n",
    "* Note the performance on the train and test set are not too different. That is an indication that the model didn't overfit.\n",
    "* Just a side note, look at the confusion matrix, the actual values are in the columns, and the prediction is in the row. That is the explanation we gave in the pseudo-code. In the workplace, you may see that switch. That is fine, and you just have to pay attention to where you see the actual and prediction :)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The confusion matrix shows the counts of when the classifier predicted properly or not for a given class. \n",
    "* For example, how many times did the model predict an actual malignant as malignant for the train set? That is 164.\n",
    "* How many times has the model predicted a malignant as benign for the train set? That is 6.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The classification report shows the main metrics for the classification\n",
    "* We see for each class the precision, recall and f1-score.\n",
    "* Support means how many observations.\n",
    "* We also see the accuracy.\n",
    "* macro avg: it computes the average without considering the proportion. For example, on the train set in the precision column, it takes all precisions and calculates the average: `(0.99 + 0.98)/ 2`\n",
    "* weighted avg: it computes the average considering the proportion. For example, on the train set in the precision column, it takes `[ 170/(170+285) * 0.99 ] + [ 285/(179-+285) * 0.98 ]`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> However, we will tend to use more precision, recall, f1-score and accuracy as metrics for classification\n",
    "\n",
    "Let's comment now on the results.\n",
    "* Since the classes are not balanced (we have more benign and malignant) we probably will not choose accuracy. But let's assume we chose accuracy. We see the accuracy is very good on the train and test set.\n",
    "* We could also interpret and consider that for this case we are interested (due to some particular business reason) to use as a metric the recall on malignant since we don't want to tell that a patient is benign when it is malignant. In this case, your performance on the train set is 0.96 and on the test set is 0.95. That means when you have live data, you should expect that 95% of the time you will not misclassify a patient that has a malignant tumour. It will be up to your business problem and context to tell if this level is acceptable. Your heuristics also will play a role to answer this question.\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                        X_test=X_test, y_test=y_test,\n",
    "                        pipeline=pipeline,\n",
    "                        label_map= [\"Malignant\",\"Benign\"] )\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> What if I don't know how to map my target variable for this custom function?\n",
    "* In this example, we know that the number 0 for the target means 'Malignant' and 1 is 'Benign'. But what if I didn't?\n",
    "* That is okay, and you just have to parse in a list of the ordered sequence of the classes as strings, like: ``[\"0\", \"1\" ]``\n",
    "*Let's try below. It will display the same result, and the difference is the ``label_map``\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                        X_test=X_test, y_test=y_test,\n",
    "                        pipeline=pipeline,\n",
    "                        label_map= [\"0\", \"1\" ] ) # it will display the classes as 0 and 1\n",
    "                                                  # but \"0\" and \"1\" should be a string\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 04 - Tree-based models for Regression and Classification\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Implement and Evaluate Tree-Based Models for Regression and Classification\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 04 - Tree-based models for Regression and Classification\n",
    "\n",
    "In this unit, we will cover the practical steps and code to fit a pipeline considering Tree-based models, like Decision Trees, Random Forests.\n",
    "* If you want to revise the algorithm content, refer to the Machine Learning Essentials > Algorithm units. \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A typical workflow used for supervised learning is\n",
    "* Split the dataset into train and test set\n",
    "* Fit the pipeline\n",
    "* Evaluate your model. If the performance is not good, revisit the process, starting from defining the business case, collecting the data, conducting EDA (Exploratory Data Analysis) etc.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> For teaching purposes, **we will use a fixed dataset for Regression and a fixed dataset for Classification across the different algorithms used in this notebook.**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the Boston dataset from sklearn for the **Regression task**. \n",
    "* It has house price records and characteristics, like the average number of rooms per dwelling and the per capita crime rate in Boston.\n",
    "* We'll use the same code from the previous unit.\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df_reg = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_reg['price'] = pd.Series(data.target)\n",
    "\n",
    "df_reg = df_reg.sample(frac=0.5, random_state=101)\n",
    "\n",
    "print(df_reg.shape)\n",
    "df_reg.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the Iris dataset from seaborn for the **Classification task**. \n",
    "* It contains records of three varieties of iris plants, with their petal and sepal measurements.\n",
    "\n",
    "df_clf = sns.load_dataset('iris').sample(frac=0.7, random_state=101)\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We will cover the following tree algorithms, which include ensemble tree algorithms.\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting\n",
    "* Ada Boost\n",
    "* XG Boost (eXtreme Gradient Boost)\n",
    "* Extra Tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For teaching purposes, we will use:\n",
    "* **Classification** task for Decision Tree, Gradient Boosting and XG Boost.\n",
    "* **Regression** task for Random Forest, Ada Boost and Extra Tree.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> That speeds up our learning process. And, if you do Regressor using a Decision Tree, the code and workflow are the same as you would do for Classification using a Decision Tree.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Decision Tree\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer to module 2 - ML Essentials - in the Algorithms lesson to refresh the algorithms we cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "* In a nutshell, a decision tree is like a flow chart where each question has a yes/no answer. This brings you from a general question to a very specific question as you get deeper. The questions asked must be ones where the yes or no answer gives useful insights into the data.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Depending on your task (Regression or Classification) for using the Decision Tree algorithm in Sckit learn, you will import a different estimator.\n",
    "* There is the suffix \"`Regressor`\" in the estimator when the algorithm will be used for a regression task, and, as you may expect, there is the suffix \"`Classifier`\" in the estimator when the algorithm is used for the classification task.\n",
    "* That pattern repeats for the other tree-based algorithm.\n",
    "* The difference is subtle, however, it is worth pointing out.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Find here the documentation for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "* We will import both but will use the `DecisionTreeClassifier` for the exercise.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Let's reinspect our data again.\n",
    "* The target variable is 'species' and we don't have missing data.\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "We are getting more comfortable with ML, but it is worth remembering that this exercise is an example of supervised learning, where the ML task is classification. The same principle applies when the ML task is Regression.\n",
    "* For that workflow, it is wise to split the data into train and test sets.\n",
    "* In the previous units, we explained the `train_test_split() `function. From now on, we will just state \"We split the data into train and test sets\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "As we are using a clean complete data set in this example, we will not need the data cleaning or the feature engineering steps.\n",
    "* We then set feature scaling, feature selection and modelling using the DecisionTreeClassifier. We set random_state, so the results will be reproducible anywhere. We chain these steps in a sklearn Pipeline.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def pipeline_decision_tree_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "\n",
    "      ( \"feat_selection\",SelectFromModel(DecisionTreeClassifier(random_state=101)) ),\n",
    "      \n",
    "      ( \"model\", DecisionTreeClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "pipeline_decision_tree_clf()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> It is time to fit the pipeline, so the model can learn the relationships between the features and the target. We create a variable pipeline (it could have any name) and call the function where we set our pipeline.\n",
    "\n",
    "pipeline = pipeline_decision_tree_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "Like in the previous notebook, we are now interested in starting to evaluate the pipeline. Since it is a tree-based model, we can assess the importance of the features in the model using `.features_importance_`\n",
    "* We created a custom function to assess feature importance on tree-based models. It takes the model and the variables that \"hit\" the model. Check the pseudo-code, comments and docstrings to understand the logic.\n",
    "* Don't worry if, at first, you don't understand.  Expect it to take some time to absorb.\n",
    "\n",
    "def feature_importance_tree_based_models(model, columns):\n",
    "  \"\"\"\n",
    "  Gets the model, and the columns used to train the model\n",
    "  - we use the model.feature_importances_ and columns to make a\n",
    "  DataFrame that shows the importance of each feature\n",
    "  - next, we print the features name and its relative importance order,\n",
    "  followed by a barplot indicating the importance\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # create DataFrame to display feature importance\n",
    "  df_feature_importance = (pd.DataFrame(data={\n",
    "      'Features': columns,\n",
    "      'Importance': model.feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "  best_features = df_feature_importance['Features'].to_list()\n",
    "\n",
    "  # Most important features statement and plot\n",
    "  print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "        f\"The model was trained on them: \\n{df_feature_importance['Features'].to_list()}\")\n",
    "\n",
    "  df_feature_importance.plot(kind='bar',x='Features',y='Importance')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "Let's check that.\n",
    "* The `model` argument is the 'model' step from the pipeline (we don't parse the pipeline, since we need only the model step)\n",
    "* In the `columns` argument, we subset the feature selection step where we grab a boolean array informing which features hit the model - pipeline['feat_selection'].get_support(). This array is used to subset the features from train set columns.\n",
    "* Note that only 2 features - `['petal_width', 'petal_length']` - out of 4, were used to train the model and they have roughly similar relevance\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "It is time to evaluate the classifier. We are using the same custom function for evaluating the classifier as used in the last notebook. \n",
    "\n",
    "# loads confusion_matrix and classification_report from sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  Gets features, target, pipeline and how labelled (named) the levels from your target\n",
    "\n",
    "  - it predicts based on features\n",
    "  - compare predictions and actuals in a confusion matrix\n",
    "    - the first argument stays as rows and the second stay as columns in the matrix\n",
    "    - we will use the pattern where the predictions are in the row and actual values are in the columns\n",
    "    - to refresh that, revert to the Performance Metric video in Module 2\n",
    "  - show classification report\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  gets the features and target from train and test set, pipeline how\n",
    "  you labelled (named) the levels from your target\n",
    "  - for each set (train and test), it calls the function above to show the confusion matrix\n",
    "  and classification report for both train and test set\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "You will notice that in this dataset, the target variable wasn't a set of numbers referring to classes, but rather, are strings.\n",
    "* We are parsing, from df_clf, the unique values from the target as the `label_map` parameter.\n",
    "\n",
    "df_clf['species'].unique()\n",
    "\n",
    "Let's evaluate the classifier then\n",
    "* Note the model aced all predictions in the train set, which is an indication that it learned all the relationships from the training data. That is good, but let's check on the test set\n",
    "* As we may expect, on the test set the performance was a bit lower (we noticed that in the confusion matrix, where  Virginica and Versicolor have the wrong predictions). At the same time, it is still very good, and it is not much of a difference from the train set. It is a good indication that the model didn't overfit \n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "One additional aspect when using DecisionTree, is to visualise the created tree.\n",
    "* Sckit learn has `plot_tree()` function that is okay and can help us, the documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). We parse:\n",
    "* decision_tree as the model step in our pipeline\n",
    "* feature_names as the variable used to train the model. That is done by extracting the information from the feature selection step\n",
    "* class_names are taken from unique values from species\n",
    "* The remaining arguments help us to get a cleaner visualisation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Just a side note, this decision tree is simple, however, when it comes to big trees, the visualization might become too big or more difficult to interpret.\n",
    "* In this example the decision is made first on petal_width, if it is smaller than -0.47, it is Setosa, if not it goes to another decision-making point. The other decision is for petal_length, if it is smaller than -0.57, it is Virginica, otherwise is Versicolor.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **Note the beauty: the algorithm computed by itself the pattern and now can predict. That is the major difference between ML and traditional programming. Here, we have data and an objective (predict species), and then the computer finds the best rule for that. In traditional programming, the developer has to set the rules**\n",
    "\n",
    "* However the decision points are still weird, what does a -0.47 mean for petal_width? Negative value. Let's explore the next cell\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "tree.plot_tree(decision_tree = pipeline['model'], \n",
    "               feature_names = X_train.columns[pipeline['feat_selection'].get_support()],\n",
    "               class_names = df_clf['species'].unique(),\n",
    "               filled=True,\n",
    "               rounded=True,\n",
    "               fontsize=9,\n",
    "               impurity=False)\n",
    "plt.show()\n",
    "\n",
    "The negative values from the previous case happen due to the feature scaling step, where it scaled the data using a standard scaler. We can grab this pipeline step and use .inverse_transformation() to convert the scaled value to the original.\n",
    "* We create a DataFrame that relates to the original data. For petal_width and petal_length we set the decision points from the previous map. We parse the DataFrame to .inverse_transform\n",
    "* The decision points are actually 5.4 for petal_width, 3.3 for petal_length\n",
    "\n",
    "scaled_data = pd.DataFrame(data={'petal_width':-0.472,\n",
    "                                 'petal_length':0.578,\n",
    "                                 'sepal_length':1.0, # this value doesn't matter, but needs to be here\n",
    "                                 'sepal_width':1.0}, # this value doesn't matter, but needs to be here\n",
    "                           index=[0])\n",
    "\n",
    "\n",
    "pipeline['feat_scaling'].inverse_transform(scaled_data)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Random Forest\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer back to module 2 (ML Essentials) in the Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* The random forest is made of many decision trees and it is an ensemble method. It uses bagging and feature randomness when building each individual tree, aiming to create an uncorrelated collection of trees, where the prediction from the set of trees is more accurate than that of any individual tree.\n",
    "\n",
    "\n",
    "\n",
    "Once again, the same algorithm has a different estimator depending on the tasks: Regression or Classification. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "* We will import both but will use `RandomForestRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    " We will use the Boston dataset to fit an ML pipeline to predict the sales price using the Random Forest Algorithm\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "We split the train and  test sets. The target variable is 'price' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using a similar structure as the previous example. There are 3 steps: scaling, feature selection and modelling. \n",
    "* We know in advance the data doesn't require data cleaning.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def pipeline_random_forest_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestRegressor(random_state=101)) ),\n",
    "      ( \"model\", RandomForestRegressor(random_state=101)),\n",
    "\n",
    "  ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "pipeline_random_forest_reg()\n",
    "\n",
    "We will fit the pipeline to the train set (features and target) using `.fit()`\n",
    "\n",
    "\n",
    "pipeline = pipeline_random_forest_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "Since it is a tree-based model, we can assess in the model the importance of the features with .features_importance_, using the custom function from the previous section\n",
    "* Note that from 13 features, the model was trained on 2: LSTAT and RM, where LSTAT is more important to the model\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "We will evaluate the regressor pipeline using the same custom function from the last unit notebook \n",
    "\n",
    "# import regression metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "# we will use numpy to calcuate RMSE based on MSE (mean_squared_error)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets train/test sets and pipeline and evaluates the performance\n",
    "  - for each set (train and test) call regression_evaluation()\n",
    "  which will evaluate the pipeline performance\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Model Evaluation \\n\")\n",
    "  print(\"* Train Set\")\n",
    "  regression_evaluation(X_train,y_train,pipeline)\n",
    "  print(\"* Test Set\")\n",
    "  regression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets features and target (either from train or test set) and pipeline\n",
    "  - it predicts using the pipeline and the features\n",
    "  - calculates performance metrics comparing the prediction to the target\n",
    "  \"\"\"\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  \"\"\"\n",
    "  # Gets Train and Test set (features and target), pipeline, and adjust dots transparency \n",
    "  at scatter plot\n",
    "  - It predicts on train and test set\n",
    "  - It creates Actual vs Prediction scatterplots, for train and test set\n",
    "  - It draws a red diagonal line. In theory, a good regressor should predict\n",
    "  close to the actual, meaning the dot should be close to the diagonal red line\n",
    "  The closer the dots are to the line, the better\n",
    "\n",
    "  \"\"\"\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "* We notice that the performance on the train set is pretty good (0.95 of R2, MAE of 1.4, the actual vs prediction plot is dense around the diagonal red line), however, R2 on the test set is still ok (0.71) but much lower than on the train set, there is a notable difference. That may be a sign of overfitting.\n",
    "* We note for the actual vs predictions plots that in the train set, the dots are closer around the diagonal line than they were in the test set. That reinforces the previous point.\n",
    "* Following the diagonal line means the predictions tend to follow the actual value.\n",
    "* This pipeline was trained on the default algorithm hyperparameters (like the number of trees, max depth etc). It is a matter of making sense of the hyperparameter and its common impact on algorithm performance. We will cover how to train with multiple hyperparameters in an upcoming lesson \n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test, pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Gradient Boosting\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer back to module 2 (ML Essentials) in the Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "* Gradient boosting is a type of machine learning boosting. The idea of a boosting technique is based on building a sequence of initially weak models into increasingly more powerful models. You add the Models sequentially until no further improvements can be made. Gradient boosting aims to minimize the loss function by adding weak learners using a gradient of a loss function that captures the performance of a model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html).\n",
    "* We will import both but will use  `GradientBoostingClassifier`for the exercise.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "Let's consider the iris dataset again for the classification task\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "As usual, we split the data into train and test sets, considering 'species' as the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "The pipeline is similar to that used in the  previous section where we considered the iris dataset.\n",
    "* There are 3 steps: feature scaling, feature selection and modelling, and here we consider the Gradient Boosting Classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "\n",
    "def pipeline_gradient_boost_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=101)) ),\n",
    "      ( \"model\", GradientBoostingClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline with the train set.\n",
    "\n",
    "pipeline = pipeline_gradient_boost_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And check feature importance using the same function we used previously since, for this algorithm, feature importance is assessed using the same attribute\n",
    "* Note it considers only petal_length. Note also the difference; the same data in the decision tree had 2 features as the most important features. That happens since different algorithms have different mechanisms.\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "Let's evaluate the data using the same custom function that shows the confusion matrix and classification report for the train and test sets\n",
    "* The results are the same compared to a Decision Tree (considering, the same dataset).\n",
    "* The only difference is that we needed only 1 feature to reach that result for the Gradient Boost; for the decision tree, we needed 2. So the Gradient Boost is better for this data since it is simpler and easier to have a system with fewer features.\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Ada Boost\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer back to module 2 (ML Essentials) in the Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* AdaBoost (or Adaptive Boosting) is an ensemble learning used to build a strong model from several weak models. It uses multiple iterations to generate a single strong learner by iteratively adding weak learners. The result is a model that has higher accuracy than the weak learner itself.\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
    "\n",
    "\n",
    "* We will import both but will use `AdaBoostRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    " We will use the Boston dataset to fit an ML pipeline to predict the sales price using the Ada Boost Algorithm\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "We split the train and test sets. The target variable is 'price'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering the Ada Boost Regressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "def pipeline_adaboost_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(AdaBoostRegressor(random_state=101)) ),\n",
    "      ( \"model\", AdaBoostRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the data to the train set (in the same manner we did previously)\n",
    "\n",
    "pipeline = pipeline_adaboost_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And assess feature importance using our custom function\n",
    "* Note this pipeline selects 3 variables to train the model: `['LSTAT', 'RM', 'DIS']`\n",
    "\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "We now evaluate the data using the custom function. \n",
    "* The R2 score on the train set is 0.9 and on the test set is 0.78. Ideally, it could be less, but this difference is lower than the difference we see for Random Forest\n",
    "* We note for the actual vs predictions plots, that in the train set, the dots are around the diagonal line (not so close as in the Random Forest). \n",
    "* Following the diagonal line means the predictions tend to follow the actual value.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  XG Boost\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer back to module 2 (ML Essentials) in the Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* XGBoost stands for eXtreme Gradient Boosting and is an extension to gradient-boosted decision trees, specially designed to improve speed and performance. It has regularisation features that help to avoid over-fitting. It is a dedicated software library that you should install, it doesn't belong to the Scikit-learn library.\n",
    "\n",
    "\n",
    "We import the algorithms. Find [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) the documentation for both\n",
    "* We will import both but will use `XGBClassifier` for the exercise.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "Let's consider the iris dataset again for the classification task\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "Let's split the data into train and test sets, where the target variable is 'species' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering XGBoost\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def pipeline_xgboost_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(XGBClassifier(random_state=101)) ),\n",
    "      ( \"model\", XGBClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline to the train data\n",
    "\n",
    "pipeline = pipeline_xgboost_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And assess the feature importance\n",
    "* Note only petal_length is relevant to fit the model. \n",
    "\n",
    "\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "Let's assess the pipeline performance\n",
    "* The performance is the same as Gradient Boost on the train and test set. So for Classification, of the three algorithms we tested, decision tree, gradient boost, and XG boost - the last 2 are good candidates and best suit the data. However, we will study another method to test more algorithms simultaneously and avoid this segregated analysis we are doing now.\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  ExtraTree\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may refer back to module 2 (ML Essentials) in the Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* Extra Trees (or Extremely Randomized Trees) is an ensemble algorithm. It works by creating a large number of unpruned trees. Predictions are made by averaging the prediction of the decision trees when it is regression or using majority voting when it is classification.\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html).\n",
    "* We will import both but will use `ExtraTreesRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "We will use the Boston dataset to fit an ML pipeline to predict the sales price\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "Let's split the data into train and test sets using 'price' as a target variable \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering the Extra Tree Regressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def pipeline_extra_tree_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(ExtraTreesRegressor(random_state=101)) ),\n",
    "      ( \"model\", ExtraTreesRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline with the train set.\n",
    "\n",
    "pipeline = pipeline_extra_tree_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And evaluate feature importance using our custom function\n",
    "* It used `['LSTAT', 'RM']` and LSTAT is more important.\n",
    "* Just to reinforce, different algorithms consider different features to find patterns in the data, Random Forest selected the same features, and Ada Boost added to the selected list the variable DIS \n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "Let's now evaluate the pipeline\n",
    "* Note the pipeline was perfect on the train set (R2 score of 1), and on the test set, was poor (R2 score of 0.68 is poor compared to a score of 1 in the train set).\n",
    "* This is a sign the model overfits since it performs better in the train set and doesn't generalise well to other sets, like the test set.\n",
    "* After all, for this dataset and among Random Forest, Ada Boost and Extra Tree, Ada Boost performed better since it can generalize better (the difference between performance on train and test set is smaller).\n",
    "* Again, we analyse each algorithm separately for learning purposes; in the next unit, we will learn how to evaluate all the algorithms simultaneously.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05A - Cross Validation Search (GridSearchCV) and Hyperparameter Optimization Regression - Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05A - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimisation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Good job! You fitted multiple pipelines considering different algorithms separately for regression and classification tasks. However, how do you know which was better for a given ML task? \n",
    "* Imagine for the classification task on the iris dataset, you fitted three individual pipelines, evaluated each separately, and concluded a given algorithm was better. That is fair enough, but you want a more effective way to assess multiple algorithms.\n",
    "* However, you also fitted the models with the default hyperparameters and may wonder: what if for a given algorithm, I could fit multiple models using different hyperparameters and find a model with even better performance?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%208-%20Challenge.png\"> Let's learn how to use GridSearchCV and do Hyperparameter Optimization using multiple algorithms. \n",
    "* This is the heart of conventional ML. We will split this topic into two parts: \n",
    "  * In the next three notebooks, we will show how to conduct hyperparameter optimization using one algorithm (for Regression, Binary Classification and Multiclass Classification).\n",
    "  * Then we will cover how to do hyperparameter optimization using multiple algorithms at once.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimization with one algorithm\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this section, we will select a given algorithm and fine-tune it, by defining a set of hyperparameters. \n",
    "* For each possible hyperparameter combination, a set of models will be fitted - based on the cross-validation parameter. For example, if the developer sets cross-validation as 5, it will fit five models for a given hyperparameter combination. \n",
    "* These five models are scored against a performance metric (i.e., if it is regression, it could be the R2 score), and average performance is computed. This average is the cross-validated performance for a given configuration of hyperparameters. \n",
    "* This process is repeated then for every combination of hyperparameters. \n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">  Let's quickly recap how we use the data for fitting supervised models\n",
    "* The training subset of the data is used to fit or train the model.\n",
    "* A subset of the training set is known as the validation set. This is used during fitting to compare one model against another, and when choosing or tuning hyperparameters. \n",
    "* The final subset of data used to test the model is known as the test set. This assesses the final model's performance and it must be data that is new to the model to give an unbiased result. The test set closely replicates what the deployed model will see in real-time usage. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\">   When we do Hyperparameter Optimisation, a part of the train set is automatically subset as a validation set and the model is fitted using cross-validation. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> How can I do that in Scikit-learn? \n",
    "* We use a function called **GridSearchCV**, which fits multiple models looping through a hyperparameter list over each model. \n",
    "* Note: CV here means cross-validation. It uses cross-validation to  compare different algorithm and hyperparameter combinations. So, at the end, we can select the best parameters from the listed hyperparameters that achieve a better performance.\n",
    "* Ultimately, it helps to automate the process to find the best combination of hyperparameters for a given algorithm in a given dataset. The documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will split this section showing uses cased for GridSearchCV on Regression, Binary Classification, Multiclass classification task\n",
    "* When using GridSearchCV, the difference between these ML tasks relies on the scoring parameters, which tell which performance metric should be used to select the best model.\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Regression\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We are going to consider a similar workflow we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline \n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">The differences now are:\n",
    "* we decide on a list of hyperparameters to optimise our model while fitting it\n",
    "* we need a performance metric to decide which model is the best in cross-validating the models.\n",
    "\n",
    "\n",
    "We will use the Boston dataset from sklearn. It has house price records and characteristics, like the average number of rooms per dwelling and Boston's per capita crime rate.\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df_reg = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_reg['price'] = pd.Series(data.target)\n",
    "\n",
    "df_reg = df_reg.sample(frac=0.5, random_state=101)\n",
    "\n",
    "print(df_reg.shape)\n",
    "df_reg.head()\n",
    "\n",
    "We split the data into train and test sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Similar to previous examples, we set the pipeline with three steps; feature scaling, feature selection and modelling. \n",
    "* For the purpose of learning hyperparameter optimisation, we just set the algorithm used to RandomForestRegressor. However, we encourage you to try additional algorithms. There are example algorithms commented out that you can try.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def pipeline_adaboost_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestRegressor(random_state=101)) ),\n",
    "      ( \"model\", RandomForestRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We should parse the **algorithm's hyperparameters**, in a dictionary, with support from its documentation, which is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "* Since we are fitting a pipeline, that contains a set of steps, you should state in your hyperparameter list to which step your hyperparameter belongs. In our case, we named the modelling step `\"model\"`, so we add the suffix `\"model__\" `before the hyperparameter name.\n",
    "* For this example, we picked only one hyperparameter: n_estimators. For n_estimator, we parse in a list with 10 and 20 (the default value is 50, but for faster computation, in this teaching example we set it to 10 and 20)\n",
    "* It will take time and practical experience to make sense of which hyperparameters are more useful for each algorithm and what are the typical ranges to consider when listing hyperparameter's values\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html  # documentation is here\n",
    "param_grid = {\"model__n_estimators\":[10,20],\n",
    "              }\n",
    "\n",
    "param_grid\n",
    "\n",
    "We import GridSearchCV. Its documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). We parse\n",
    "*  `estimator` as the pipeline, and `param_grid` as the dictionary we stated above. \n",
    "* `cv` sets the number of cross-validation you want to try for each selected set of hyperparameters. It uses k-fold cross-validation, where we subdivide the whole dataset into multiple randomly chosen data sets known as k-fold cross-validation where k refers to the number of data sets.  \n",
    "* `n_jobs`, according to the documentation, is the number of jobs to run in parallel. -1 means using all processors, whereas -2 uses all but one.\n",
    "* `scoring` is the evaluation metric that you want to use.  That will depend on the ML task you are considering. In this case, it is regression, so we set the R2 score as the metric. Other options would include: `'neg_mean_absolute_error'`, `'neg_mean_squared_error'`\n",
    "* `verbose`, according to the documentation, controls the verbosity: the higher, the more messages. As this is a teaching example we set it as 3, so you get more information returned about the process.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> We create an object (called the grid) that contains a GridSearchCV using the above parameters. Next, we fit this object to the train set (features and target).\n",
    "\n",
    "* That will fit multiple RandomForestRegressor models. Considering cv=2 and the hyperparameters we listed above, it will train two models. It trains two models since we have two possible combinations of hyperparameters.\n",
    "* For each model, we use 2-fold cross-validation, since cv=2. Therefore each model will be fitted twice.\n",
    "* In total, this operation will fit four models, two models where each model is fitted two times, due to 2-fold cross-validation.\n",
    "* Note, the two scores for `model__n_estimators=10` (the first two test_scores) are 0.618 and 0.695. The mean is 0.656. We will highlight this mean to you when computing the cross-validation results for this hyperparameter combination in upcoming cells.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Remember, the validation set is automatically defined using GridSearchCV. You parse the training set and it will subset the validation set as a part of the training set.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=pipeline_adaboost_reg(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3,  # for learning, we set 3 to print the score from every cross-validation\n",
    "                    scoring='r2')\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "The results of all models and their respective cross-validations are stored in the attribute `.cv_results`\n",
    "* When you access this attribute, you will see it is a dictionary and when displayed as is, is not very informative.\n",
    "\n",
    "grid.cv_results_\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> One way to make it more informative is to parse it to a DataFrame, sort the values by 'mean_test_score', filter 'parameters' and 'mean_test_score' columns and convert it to an array, using .values\n",
    "* In the end, you print a simplified ordered list showing the results for optimising a model with multiple hyperparameter combinations.\n",
    "  * For example, we see the best hyperparameter configuration is `n_estimators 10`, which gave an R2 score of 0.65\n",
    "  * Note the first hyperparameter combination: `model__n_estimators=10`. Previously we commented on its average performance: 0.656. That is a result of the mean of the 2 cross-validated models for this hyperparameter combination. In this particular example considering this set of hyperparameter combinations, its performance was the best compared to the others.\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "Additionally, we can get the best parameters with `.best_params_`\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "This is interesting, but we want to have the pipeline with the\n",
    "highest score, for real-world usage. To get it, use the attribute `.best_estimator_`\n",
    "* This is the most important aspect of the grid search, where we grab the pipeline, which we will evaluate and potentially use.\n",
    "* Note, when fitting the pipelines, you saw the score for each cross-validated model for each hyperparameter combination. For the best hyperparameter combination, it takes the best cross-validated model, in this case, it takes the last.\n",
    "  * `[CV 1/2] END ............model__n_estimators=10;, score=0.618 total time=   0.2s`\n",
    "  * `[CV 2/2] END ............model__n_estimators=10;, score=0.695 total time=   0.2s`\n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "You can now evaluate the pipeline that you fit using hyperparameter optimisation using the techniques we covered already. We will import the custom function for regression evaluation.\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "import numpy as np\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "\tprint(\"Model Evaluation \\n\")\n",
    "\tprint(\"* Train Set\")\n",
    "\tregression_evaluation(X_train,y_train,pipeline)\n",
    "\tprint(\"* Test Set\")\n",
    "\tregression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Next, we parse the train set, test set and our pipeline to the function\n",
    "* Note the performance on the train test is good (0.938). The test set is not so good (0.685). However, these values are mismatched, which may indicate overfitting.\n",
    "* We may have to consider additional values for the hyperparameters or even consider other hyperparameters. Or maybe we need more data so the algorithm can find the patterns and generalise on unseen data.\n",
    "* Or maybe this algorithm is not the best for this dataset. But don't worry, soon we will discover an approach to train multiple algorithms at once.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05 - Cross Validation Search (GridSearchCV) and Hyperparameter Optimisation Binary Clf- Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05 - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimisation\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimisation with one algorithm\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Binary Classification\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In the last section, we saw how to conduct hyperparameter tuning using one algorithm to solve a Regression problem.\n",
    "* There is a tiny difference in using GridSearch CV when your ML task is classification, we will cover that now.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are going to consider a similar workflow to the one we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline and hyperparameter\n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "Let's load the breast cancer data from sklearn. It shows records for a breast mass sample and a diagnosis informing whether it is a malignant or benign tumour, where 0 is malignant and 1 is benign.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df_clf = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_clf['diagnostic'] = pd.Series(data.target)\n",
    "df_clf = df_clf.sample(frac=0.5, random_state=101)\n",
    "\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "We split the data into train and test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['diagnostic'],axis=1),\n",
    "                                    df_clf['diagnostic'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "And create a pipeline with three steps, feature scaling, feature selection and modelling using RandomForestClassifier.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def pipeline_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestClassifier(random_state=101)) ),\n",
    "      ( \"model\", RandomForestClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We define our hyperparameter list based on the algorithm documentation. One method could be to consider the default parameter value and a set of values around the default value.\n",
    "* In this case, there are two possible combinations of hyperparameter.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"model__n_estimators\":[50,20],\n",
    "              }\n",
    "\n",
    "param_grid\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> When we move to Classification, there will be a different GridSearchCV scoring argument. \n",
    "\n",
    "\n",
    "\n",
    "* We consider that in our classification projects, the potential performance metrics are: accuracy, recall, precision, and f1 score.\n",
    "  * When the metric is either recall, precision or f1 score, we need to inform which class we want to tune for and use `make_scorer()` as an \"auxiliary\" function to help define the metric and the class to tune. The documentation for make_scorer is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)\n",
    "  * When your performance metric is recall, you need to import [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), if it is precision, [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and if it is f1 score, you need to import [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html); so you can parse the metric to the `make_scorer()` function.\n",
    "  * When your performance metric is accuracy, you simply write \"accuracy\" for scoring: `scoring='accuracy'`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this exercise, we have 0 and 1 as diagnostics for breast cancer. \n",
    "* We assume that when defining the ML business case, it was agreed that the performance metric is recall on malignant (0) since the client needs to detect a malignant case. \n",
    "* The client doesn't want to miss a malignant case, even if that comes with a cost where you misidentify a benign tumour, and state it is malignant. For this client, this is not as bad as misidentifying a malignant tumour as benign. Therefore, the model is tuned on recall for malignant (0).\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.metrics import f1_score # in case your metric is f1 score, you would need this import\n",
    "from sklearn.metrics import precision_score # in case your metric is precision, you would need this import\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\">  The arguments `estimator, param_grid, cv, n_jobs,` and `verbose` are similar to the previous example.\n",
    "\n",
    "* The focus is now on `scoring` when creating the object to conduct a grid search. You will need `make_scorer()` to parse your tune on recall for class 0 for this binary classifier. \n",
    "  * Pass two arguments to `make_scorer()` for recall_score as your metric and pos_label to identify which class you want to tune recall. In this case, it is 0.\n",
    "* Next, you fit the grid search with the train set (features and target) as usual.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Since `cv=2`, we will fit two models for each hyperparameter combination using k-fold cross-validation. Therefore, four models (two times two) are trained in the end. \n",
    "* The same dynamic repeats: compute the performance for each cross-validated model and get the average performance for a given hyperparameter combination, then iterate for each hyperparameter combination.\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=pipeline_clf(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3,\n",
    "                    # in the workplace we typically set verbose to 1, \n",
    "                    # to reduce the amount of messages when fitting the models\n",
    "                    # for teaching purpose, we set to 3 to see the score for each cross validated model\n",
    "                    scoring=make_scorer(recall_score, pos_label=0)\n",
    "                    )\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "Next, we check the results for all four different models  with `.cv_results_` and use the same code from the previous section\n",
    "* Note that `'model__n_estimators': 50` gave an average recall score on class 0 of 0.86 and is superior to the other combination.\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "Let's check the best parameters with `.best_params_`\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "And finally grab the pipeline that has the best estimator, the one which gave the highest score. \n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "As usual in our workflow, we will evaluate the pipeline using our custom function for classification problems\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "We parse the parameters, as usual, considering that class 0 is malignant and class 1 is benign. Therefore, label_map receives an ordered list that matches the class value and its meaning:  `['malignant', 'benign']`\n",
    "* Note the recall on malignant on the train set is 100% and on the test set is 90%. In a project, you set the threshold you would accept. \n",
    "* In case the threshold you agreed with the client is 90%, this pipeline is the solution. In a case where your agreed threshold is 98%, you would still have to look for other algorithms or hyperparameters combinations to improve your pipeline performance as recall weighted average is at 95%. \n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= ['malignant', 'benign'] \n",
    "                )\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05C - Cross Validation Search (GridSearchCV) and Hyperparameter Optimisation Multiple Clf- Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05C - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimisation\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimisation with one algorithm\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Multiclass Classification\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In the last section, we saw how to conduct hyperparameter tuning using one algorithm to solve a Binary Classification problem.\n",
    "* There is a tiny difference in using GridSearchCV when your ML task is multi-class classification, we will cover that now.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are going to consider a similar workflow we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline and hyperparameters\n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "We load the iris dataset for this exercise. It contains records of three species or classes of iris plants, with their petal and sepal measurements.\n",
    "\n",
    "df_clf = sns.load_dataset('iris')\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "As usual, we split the data into train and test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "And create a pipeline using three steps: feature scaling, feature selection and modelling with RandomForestClassifier.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def pipeline_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestClassifier(random_state=101)) ),\n",
    "      ( \"model\", RandomForestClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We define our hyperparameter list based on the algorithm documentation.\n",
    "* In this case, there will be two hyperparameter combinations.\n",
    "* As the intention of the unit is to learn hyperparameter optimisation, we will reduce the number of hyperparameter combinations, so the code runs faster. However, we encourage you to try additional larger combinations to consolidate your learning.\n",
    "\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"model__n_estimators\":[10,20],\n",
    "              }\n",
    "param_grid\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  Let's assume for this project, the client is interested particularly in the Virginica species, and needs the predictions for this class to be precise. Although this is an arbitrary choice, in this case, it is an example of the type of business requirements given to you by the product owner or business expert.  \n",
    "* In this case, your scoring parameter is `precision_score` to the class Virginica.\n",
    "  * In a Multiclass classification, when your performance metric is accuracy, you just pass scoring='accuracy' as an argument, as done with a binary classifier.\n",
    "  * In our case, we need to pass arguments to the make_scorer() method to fine-tune the model using precision on the Virginica species. We pass to make_scorer as an argument the metric we want - `precision_score`. The next argument is `labels`, where you set the class you want to tune as a list. Note, in this dataset, the species is not encoded as numbers but as categories. If it were numbers, you would pass the number related to the class you want to tune. The last argument is `average`, and it should equal `None` since you compute the precision from one class only (in this case Virginica) and you don't need to average.\n",
    "* Finally, you fit the grid search to the training data.\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "grid = GridSearchCV(estimator=pipeline_clf(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3, # In the workplace we typically set verbose to 1, \n",
    "                    # to reduce the number of messages when fitting the models.\n",
    "                    # For teaching purposes, we set it to 3 to see the score for each cross-validated model.\n",
    "                    scoring=make_scorer(precision_score,\n",
    "                                        labels=['virginica'],\n",
    "                                        average=None)\n",
    "                    )\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "Next, we check the results for all four different models with `.cv_results_` and use the same code from the previous section\n",
    "* Note this combination `''model__n_estimators': 10` gave an average precision score on virginica of 0.91. In this case, both options look to give the same performance, and the grid search picked the model with n_estimator as 10.\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "We grab programmatically the best hyperparameter combination for a quick check.\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "And finally grab the best pipeline, considering the best cross-validated model for the best hyperparameter combination.\n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "Finally, we evaluate the pipeline.\n",
    "* Note the precision on Virginica, on the train set is 98% and on the test set is 100%. It is a very good sign that the precision is maximised for the test set since it shows the pipeline can generalise on unseen data.\n",
    "* Again, the client will accept the pipeline based on the performance criteria you both set in the ML business case.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "    \n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Congratulations! You now know how to get a given algorithm and do a hyperparameter optimization for Regression and Classification!\n",
    "  * The **next level** is to define a set of algorithms and a set of hyperparameters for each algorithm and do a hyperparameter optimization for Regression and Classification tasks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 06 - Cross Validation Search (GridSearchCV) and Hyperparameter Optimisation - Part 02\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Do Hyperparameter Optimisation using multiple algorithms. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 06 - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimisation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Good job! You fitted multiple pipelines using a single algorithm while looking for the best hyperparameter combination, for regression and classification tasks. However, how do you know which was better for a given ML task? \n",
    "* Let's learn how to use GridSearchCV and do Hyperparameter Optimization using **multiple algorithms**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We will cover in this notebook:\n",
    "* A technique to do a hyperparameter optimisation with multiple algorithms.\n",
    "* A strategy for using this technique that typically reduces the time needed to train all algorithms.\n",
    "* A strategy to refit the pipeline with only the most relevant features, so you can deploy a pipeline that contains only the best features.\n",
    "* **BONUS**: Here we list the values of the most common hyperparameters for the algorithms we have covered in the course. You can use them as a starting point and as a reference for the Portfolio Project or in your future workplace.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimisation with multiple algorithms\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We are going to consider a similar workflow to the one we studied earlier:\n",
    "\n",
    "* Split the data.\n",
    "* Define the pipeline and hyperparameter.\n",
    "* Fit the pipeline (using a strategy that typically trains all the algorithms faster).\n",
    "* Evaluate the pipeline.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> The exercise we are conducting in this notebook is specifically for a multiclassification task but extends to regression and binary tasks. The concepts we cover here are also applicable to these other tasks.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will use the 'penguins' dataset for this exercise. It has records for three different species of penguins, collected from three islands in the Palmer Archipelago, Antarctica. \n",
    "* Here, we are interested in predicting the species of a given penguin.\n",
    "\n",
    "df_clf = sns.load_dataset('penguins')\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "We split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " And define the pipeline steps considering:\n",
    "* data cleaning (median imputation, categorical imputation).\n",
    "* feature engineering (categorical encoding).\n",
    "* feature scaling.\n",
    "* feature selection (note we don't specify the algorithm, we pass in a variable called `model`).\n",
    "*  and modelling (note we don't specify the algorithm, we pass in a variable called `model`).\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> Previously we covered the definition of data cleaning and feature-engineering/scaling/selection steps, and in this exercise, we provide the appropriate actions (imputations and encoding).\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Data Cleaning and Feature Engineering\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline_base = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                                'flipper_length_mm', 'body_mass_g']) ),\n",
    "\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='frequent',\n",
    "                                                        variables=['sex']) ),\n",
    "\n",
    "      ( \"ordinal\",OrdinalEncoder(encoding_method='arbitrary', \n",
    "                                 variables = ['island',\t'sex']) ), \n",
    "\n",
    "      (\"feat_scaling\", StandardScaler() ),\n",
    "\n",
    "      (\"feat_selection\",  SelectFromModel(model) ),\n",
    "\n",
    "      (\"model\", model ),\n",
    "\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we create a Python class (HyperparameterOptimizationSearch) which aims to fit a set of algorithms with multiple hyperparameters. The logic is: \n",
    "* The developer defines a set of algorithms and their respective hyperparameter values.\n",
    "* The code iterates on each algorithm and fits pipelines using GridSearchCV considering their respective hyperparameter values. The result is stored. \n",
    "* That is repeated for all algorithms that the user listed.\n",
    "* Once all pipelines are trained, the developer can retrieve a list with a performance result summary and an object that contains all the trained pipelines. The developer can then subset the best pipeline.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> Let's explain the major parts of the Python class.\n",
    "\n",
    "* In the \\__init__ method, you pass in models, and params as a dictionary of algorithms and their respective hyperparameters.\n",
    "* In the `fit` method, we loop on each algorithm, and pass the algorithm to PipelineOptimization(). As a result, it will do a grid search on a set of hyperparameters for that given model. The result is stored and the loop continues on.\n",
    "* The `score_summary` method returns all pipelines, and a DataFrame with a performance summary for all of the algorithms.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> Again, at first, it will take some time to understand the code of this class, but what's most important for now is to understand what it does. \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\">\n",
    " We now define a list of models and their respective hyperparameters. \n",
    "* The first dictionary is related to the algorithms.\n",
    "  * We create a dictionary where the key is the model name (you can use any name here, but we suggest using the estimator name), and the value is the estimator object. For example, for the decision tree we use DecisionTreeClassifier(random_state=0).\n",
    "* It is a multiclass classification, so we consider all algorithms bar logistic regression (since that is more suitable for binary classification).\n",
    "\n",
    "models_search = {\n",
    "    \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0),\n",
    "    \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=0),\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "    \"AdaBoostClassifier\":AdaBoostClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> The other dictionary relates to the hyperparameter values.\n",
    "  * Its keys should map the keys from the models' dictionary.\n",
    "  * For each key, the value will be a dictionary, whose keys will be the hyperparameter names and their values as a list of hyperparameter values.\n",
    "  * Look at the example, and see that RandomForestClassifier has two hyperparameters: n_estimators and max_depth. For each hyperparameter, we set a list with the determined values.\n",
    "  * When you want to consider only the default hyperparameters, you just pass in an empty dictionary for a given algorithm. You will see that the other algorithms have an empty dict `{ }` as their hyperparameters, which means it will only consider the default hyperparameters. **But you may ask: why would we do that?**\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{\"model__n_estimators\":[50,20],\n",
    "                               \"model__max_depth\":[None,3,10]},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> That is useful when we define a strategy to speed up the hyperparameter optimisation process.\n",
    "* You noticed the idea is to fit multiple models with multiple hyperparameter options. But the time needed to compute all of that based on your hardware capability has a cost. \n",
    "* It would make sense to do a quick search using the default hyperparameters across all listed algorithms. The result will show the algorithms that look to fit your data the best, and this training process tends not to take long since it uses the default hyperparameters.\n",
    "* Then you use the best two or three algorithms and finally do an extensive search so that you can fine-tune your pipeline performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's do a hyperparameter optimisation search using the **default hyperparameters values first**.\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We now use our custom class `HyperparameterOptimizationSearch` to assign an object called search (you can use whatever name you wish).\n",
    "* We pass in two arguments: models and params, which are the two dictionaries we set in the previous cells: models_search and params_search.\n",
    "* The goal here is to use the default hyperparameters to find the type of algorithms that look to best fit your data.\n",
    "* Next, we fit this object, meaning we will fit all the algorithms using GridSearchCV. Therefore we pass in the `training data` (X_train, y_train), `scoring` (in this case, as it's a teaching example we arbitrarily chose accuracy) and `cv` (we defined 2 to speed up the process).\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note you will see the code looping on each algorithm. There is one candidate since you are fitting with the default hyperparameter. It totals two fits per model since cv=2\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Running GridSearchCV for DecisionTreeClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for RandomForestClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for GradientBoostingClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for ExtraTreesClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for AdaBoostClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1, # use all processors, but one\n",
    "           cv=2)\n",
    "\n",
    "Our method `.score_summary` returns a DataFrame with all the training results summary and a dictionary containing all pipelines.\n",
    "* We grab both and first check the results summary.\n",
    "* Note that ExtraTreesClassifier had an average accuracy performance (using two cross-validated models with default hyperparameters values) of 0.98\n",
    "* The second best was RandomForestClassifier with 0.95. Then GradientBoostingClassifier with 0.92.\n",
    "* AdaBoostClassifier had the lowest performance here, with 0.82 of average accuracy. \n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "**On which algorithms would you spend time doing an extensive hyperparameter search?**\n",
    "* It depends on how distant the performance distribution is amongst the top performers.\n",
    "* In our case, we would certainly select ExtraTreesClassifier and would give a second chance to RandomForestClassifier, since its performance was not so far from ExtraTress.\n",
    "* We wouldn't give a second chance to GradientBoosting since 0.92 (for this context) is quite far from 0.98.\n",
    "\n",
    "\n",
    "* However, there could be a case where for example, the top four had similar performance on the default hyperparameter, then you would do an extensive hyperparameter optimisation on these four.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " Let's define the new hyperparameters for the extensive search.\n",
    "* You don't need to pass in the same quantity of hyperparameters for each algorithm, and the assigned values in the list will depend on the hyperparameter.\n",
    "* There is no fixed number of values to be parsed in this list; just remember the more values and hyperparameters you parse, the more time it will take to fit all possible combinations.\n",
    "\n",
    "# you don't have to necessarily list in any specific order here\n",
    "models_search = {\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "    \"ExtraTreesClassifier\":{\"model__n_estimators\": [20,50],\n",
    "                            },\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    \"RandomForestClassifier\":{\"model__n_estimators\": [40,20],\n",
    "                            },\n",
    "}\n",
    "\n",
    "Let's fit again using our HyperparameterOptimizationSearch class and our updated information on models_search and params_search.\n",
    "* The other arguments remain the same.\n",
    "* The goal here is to do an extensive search on the algorithms that performed better in a default hyperparameter optimisation.\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1,\n",
    "           cv=2)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\">\n",
    " Let's check the results summary with `.score_summary`\n",
    "* We could do a further round of extensive search with more hyperparameters and consider values around those that demonstrated good performance in this round. But for this teaching example, we are happy with the current search.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "Programatically, we grab the best model name, by using `.iloc[ ]` on the first row and column from the previous DataFrame.\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "Let's get the best model parameters.\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Finally, we want to grab the best pipeline.\n",
    "* The object grid_search_pipelines contains all trained pipelines. We first subset the pipelines from the algorithm having the best performance (with `best_model`), then used `.best_estimator_` to retrieve the pipeline that has the algorithm and hyperparameter configuration that best suits our data.\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The best pipeline is a tree-based algorithm, so we can check the most important features with `.feature_importances_`\n",
    "\n",
    "* The information on the `“best features”` is on the pipeline’s “feature selection” step as a boolean list. We get this list to subset the train set columns.\n",
    "* Go through the pseudo code and code comments to understand the logic.\n",
    "* Make sure you understand the variable data_cleaning_feat_eng_steps. If you use this code in your milestone project, you will likely need to update this value to the approprate one for your pipeline.\n",
    "\n",
    "# after data cleaning and feature engineering, the feature space may change\n",
    "# for example, you may drop variables, or you may add variables; such as a \"date\" variable\n",
    "# if you extract the day, month and year, for example.\n",
    "# then you ask yourself: how many data cleaning and feature engineering steps does your pipeline have?\n",
    "# in our case three: median, categorical_imputer and ordinal\n",
    "\n",
    "data_cleaning_feat_eng_steps = 3\n",
    "# we get these steps with .steps[] starting from 0 until the value we assigned above\n",
    "# then we .transform() to the train set and extract the columns\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "# we get the boolean list indicating the best features with best_pipeline['feat_selection'].get_support()\n",
    "# and use this list to sbuset columns_after_data_cleaning_feat_eng\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_pipeline['feat_selection'].get_support()].to_list()\n",
    "\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': best_features,\n",
    "          'Importance': best_pipeline['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()\n",
    "\n",
    "Finally, we evaluate the pipeline as usual with our custom function for classification tasks.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "We pass in the arguments we are familiar with.\n",
    "* Note the performance on the test set is the same as in the train set.\n",
    "* for label_map, we get the classes name with .unique()\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                # in this case the target variable is encoded as categories and we\n",
    "                # get the values with .unique() \n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Refit only with the most important features\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Now you know which algorithm and which hyperparameters best fit your data. \n",
    "* That is awesome! Look at your improvement and what you achieved so far.\n",
    "* However, your pipeline needs six columns and your model needs only three to predict. That means if you deploy this pipeline, your system will manage six inputs, when in fact you only need three.\n",
    "* That happens beacuse we consider a feature selection step, which is useful to determine the most appropriate features for the algorithm.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "  In practical terms, you don't need the features that got dropped by the feature selection step. Once you know which features you can ignore, **you can fit a new pipeline with only the most important features**.\n",
    "* This new pipeline will be deployed and contains an algorithm and hyperparameters that best suit your data and has the correct number of features.\n",
    "\n",
    "These are the most important features according to the previous analysis.\n",
    "\n",
    "best_features\n",
    "\n",
    "We will use the same workflow, but now using the `best_features` only,  for the train and test sets\n",
    "\n",
    "We split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> And subset the `best_features` !!!\n",
    "\n",
    "X_train = X_train.filter(best_features)\n",
    "X_test = X_test.filter(best_features)\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "X_train.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You will need to update your pipeline since you have fewer variables to consider and you don't need feature selection.\n",
    "* Before you had three steps for data cleaning and feature engineering.\n",
    "* Now, you have two steps: one for median imputation and another for categorical encoding.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Data Cleaning and Feature Engineering\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline_base = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'flipper_length_mm']) ),\n",
    "\n",
    "      ( \"ordinal\",OrdinalEncoder(encoding_method='arbitrary', variables = ['island']) ), \n",
    "\n",
    "      (\"feat_scaling\", StandardScaler() ),\n",
    "\n",
    "      # no feature selection!!!\n",
    "\n",
    "      (\"model\", model ),\n",
    "\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "\n",
    "\n",
    "We now list the model that performed best, in this case, ExtraTreesClassifier.\n",
    "\n",
    "models_search = {\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "}\n",
    "models_search\n",
    "\n",
    "We will need to hardcode the best parameters, so let's remind ourselves of the best params.\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "We need to parse the value between brackets `[ ]`\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesClassifier\":{'model__n_estimators': [20]\n",
    "                            },\n",
    "\n",
    "}\n",
    "params_search\n",
    "\n",
    "We fit the model using `HyperparameterOptimizationSearch` considering the model \"ExtraTreeClassifier\" and the parameters we set previously.\n",
    "* The goal here is not to do a hyperparameter optimisation search, but instead to fit a pipeline using the algorithm and best hyperparameter configuration we discovered.\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1,\n",
    "           cv=2)\n",
    "\n",
    "As usual, we check the search summary with the method .score_summary()\n",
    "* Note the performance is the same as the previous pipeline.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "We get the best model programmatically.\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "So we can grab the pipeline.\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The best pipeline is a tree-based algorithm, so we can check the most important features with `.feature_importances_`\n",
    "* The code is similar to the previous section, the difference is that now we don't have three steps in the pipeline related to data cleaning and feature engineering. Instead, we have two steps now.\n",
    "\n",
    "data_cleaning_feat_eng_steps = 2\n",
    "\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "best_features = columns_after_data_cleaning_feat_eng\n",
    "\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': best_features,\n",
    "          'Importance': best_pipeline['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()\n",
    "\n",
    "We parse the arguments we are familiar with to evaluate the classifier's performance.\n",
    "* Note the performance from this pipeline is the same as from the previous pipeline - as we should expect!\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                # in this case the target variable is encoded as categories and we\n",
    "                # get the values with .unique() \n",
    "                )\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Well done!\n",
    "* In this notebook, you learned how to conduct a hyperparameter optimisation search fitting multiple algorithms with the best features that predict a penguin's species.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Bonus: Most common Hyperparameters for the algorithms we cover in the course\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " It will take **time and experience** to learn which hyperparameters to consider when optimising your pipeline and which values would make sense to tune.\n",
    "* The key is to understand how the algorithm works, and that will take time and experience. We offer the most common hyperparameters for the algorithms we cover in the course. You can use them as a starting point and as a reference if you require them for the Portfolio Project or in the workplace.\n",
    "* Once again: the **library documentation** is your best friend to instruct you on the available hyperparameters the library offers for that given algorithm.\n",
    "\n",
    "\n",
    "* The hyperparameters we list here are a suggestion so that you can use them as a reference when you start fine-tuning your ML pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We will write the hyperparameters for all algorithms using the same dictionary structure we saw over the notebook, assuming you are arranging everything into a pipeline and the last step is called `'model'`\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Linear Regression\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Linear Regression doesn't have hyperparameters. You should parse an empty dictionary\n",
    "params_search = {\n",
    "    \"LinearRegression\":{},\n",
    "}\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Logistic Regression\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "params_search = {\n",
    "    \"LogisticRegression\":{'model__penalty': [\"l2\",\"l1\", \"elasticnet\"],\n",
    "                          'model__C': [1, 0.5, 2],\n",
    "                          'model__tol': [1e-4,1e-3,1e-5],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Decision Tree\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{'model__max_depth': [None,4, 15],\n",
    "                              'model__min_samples_split': [2,50],\n",
    "                              'model__min_samples_leaf': [1,50],\n",
    "                              'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeRegressor\":{'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Random Forest\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestRegressor\":{'model__n_estimators': [100,50, 140],\n",
    "                             'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestClassifier\":{'model__n_estimators': [100,50,140],\n",
    "                             'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Gradient Boosting\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingClassifier\":{'model__n_estimators': [100,50,140],\n",
    "                                  'model__learning_rate':[0.1, 0.01, 0.001],\n",
    "                                  'model__max_depth': [3,15, None],\n",
    "                                  'model__min_samples_split': [2,50],\n",
    "                                  'model__min_samples_leaf': [1,50],\n",
    "                                  'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\":{'model__n_estimators': [100,50,140],\n",
    "                                  'model__learning_rate':[0.1, 0.01, 0.001],\n",
    "                                  'model__max_depth': [3,15, None],\n",
    "                                  'model__min_samples_split': [2,50],\n",
    "                                  'model__min_samples_leaf': [1,50],\n",
    "                                  'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "####  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Ada Boost\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"AdaBoostClassifier\":{'model__n_estimators': [50,25,80,150],\n",
    "                          'model__learning_rate':[1,0.1, 2],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"AdaBoostRegressor\":{'model__n_estimators': [50,25,80,150],\n",
    "                          'model__learning_rate':[1,0.1, 2],\n",
    "                          'model__loss':['linear', 'square', 'exponential'],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "####  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> XG Boost\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"XGBRegressor\":{'model__n_estimators': [30,80,200],\n",
    "                    'model__max_depth': [None, 3, 15],\n",
    "                    'model__learning_rate': [0.01,0.1,0.001],\n",
    "                    'model__gamma': [0, 0.1],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"XGBClassifier\":{'model__n_estimators': [30,80,200],\n",
    "                      'model__max_depth': [None, 3, 15],\n",
    "                      'model__learning_rate': [0.01,0.1,0.001],\n",
    "                      'model__gamma': [0, 0.1],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> ExtraTree\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesClassifier\":{'model__n_estimators': [100,50,150],\n",
    "                          'model__max_depth': [None, 3, 15],\n",
    "                          'model__min_samples_split': [2, 50],\n",
    "                          'model__min_samples_leaf': [1,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesRegressor\":{'model__n_estimators': [100,50,150],\n",
    "                          'model__max_depth': [None, 3, 15],\n",
    "                          'model__min_samples_split': [2, 50],\n",
    "                          'model__min_samples_leaf': [1,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 07 - PCA (Principal Component Analysis)\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand what PCA (Principal Component Analysis) is and how it can be used in your project\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 06 - PCA (Principal Component Analysis)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Principal Component Analysis, or PCA, is a transformation to your data and attempts to find out what features explain the most variance in your data.\n",
    "\n",
    "* It reduces the number of variables, while it preserves as much information as possible. Therefore it is also referred to as \"dimensionality reduction\".\n",
    "* After the transformation, it creates a set of components, where each component contains the relevant information from the original variables.\n",
    "  * Each component explains a certain part of the variance of the whole dataset and is independent (uncorrelated) from each other.\n",
    "  * The drawback of PCA is that it is not easy to understand what each of these components represents since they don't relate one to one a specific variable, instead, each component corresponds to a combination of the original variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **We will not focus** on the mathematical study of PCA but instead will discuss the idea behind it and how to use PCA in practical terms in your data science project\n",
    "* It will take time and experience to understand how the PCA algorithm works. For now, the central aspect is to understand what PCA is and why it will help you in predictive modelling.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **Why and when should I consider using PCA?**\n",
    "\n",
    "\n",
    "* Imagine if your data has a lot of variables (or dimensions). \n",
    "\n",
    "  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " You want to be able to **visualise** your data to discover patterns, however, it is unfeasible to visualise all of your data in a single plot. You can use PCA to reduce your dataset to 2 or 3 components and visualise it. We will explore that in this notebook.\n",
    "\n",
    "  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "  In **predictive modelling**, we are concerned about which variables are more relevant for modelling. PCA is a tool capable of transforming your data, retaining only the most appropriate information or the most variance while keeping all the original variables that help the model learn the patterns in the data.\n",
    "  * In supervised learning, you can use PCA as a step when extracting features for your ML model. Instead of using, for example, `SelectFromModel()`. You may also use PCA to transform your features into relevant components that can help to predict your target variable. We will explore this technique in the Walkthrough Project 02.\n",
    "  * In addition, in unsupervised learning, you can use PCA as a step to reduce dimensionality. So your cluster algorithm will be able to understand better how to group similar data. We will explore this technique in the next lesson and Walkthrough Project 02.\n",
    "\n",
    "\n",
    "You can import PCA using the command below\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "In the next cells we are going to:\n",
    "* Load a dataset and define the pipeline steps to prepare the data for PCA\n",
    "* Transform the data using PCA and understand how many components to consider\n",
    "* Visualise the data after the PCA transformation\n",
    "\n",
    "---\n",
    "\n",
    "### Load Data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's load the breast cancer data from sklearn and apply PCA\n",
    "* It shows records for a breast mass sample and a diagnosis informing whether it is as malignant or benign cancer, where 0 is malignant, 1 is benign. \n",
    "* The target variable is 'diagnostic' and features are the remaining variables.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">  We know in advance this dataset has only numerical features and no missing data. \n",
    "* We are adding on purpose missing data (`np.NaN`) in the first 10 rows of 'mean smoothness' using `.iloc[:10,4]`, just to better simulate the datasets you will likely face in the workplace.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df_clf = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_clf['diagnostic'] = pd.Series(data.target)\n",
    "df_clf = df_clf.sample(frac=0.6, random_state=101)\n",
    "df_clf.iloc[:10,4] = np.NaN\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested in applying PCA to the features only (not the diagnostic)\n",
    "* We create 2 distinct DataFrames, `X` which is the features, and `df_target` which contains the diagnostic (benign or malignant). \n",
    "  * Note, there are 30 features in `X`.\n",
    "  * We will use `X` to apply PCA, and `df_target` at a later stage when we visualise the data.\n",
    "\n",
    "\n",
    "df_target = df_clf[['diagnostic']]\n",
    "X = df_clf.drop(['diagnostic'], axis=1)\n",
    "print(X.shape)\n",
    "X.head(3)\n",
    "\n",
    "---\n",
    "\n",
    "### Create pipeline steps\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> To apply PCA, we should scale the data. Therefore we create our pipeline that is responsible for data cleaning, feature engineering and feature scaling.\n",
    "* In our case, it will perform data cleaning (median imputation) and feature scaling.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "### Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def PipelineDataCleaningFeatEngFeatScaling():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median') ),\n",
    "\n",
    "      ( 'feature_scaling', StandardScaler() ),\n",
    "  ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineDataCleaningFeatEngFeatScaling()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We fit and transform the data to the pipeline.\n",
    "* The result is a NumPy array. Note there are still the same quantity of rows and columns (341, 30). The point to note is that the data type is now an array due to the feature scaling transformation.\n",
    "\n",
    "pipeline_pca = PipelineDataCleaningFeatEngFeatScaling()\n",
    "df_pca = pipeline_pca.fit_transform(X)\n",
    "print(df_pca.shape,'\\n', type(df_pca))\n",
    "\n",
    "Just to reinforce our learning, let's check `df_pca`. \n",
    "\n",
    "df_pca\n",
    "\n",
    "* As we expect, it is the familiar NumPy array we covered in previous sections. Note also it is a 2D array.\n",
    "\n",
    "---\n",
    "\n",
    "### PCA transformation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Now that the data is scaled, we can apply the PCA component\n",
    "* We are not assembling PCA to a pipeline in this lesson, we will do that at a later stage. The idea here is to understand how the process works\n",
    "* **A quick recap**: PCA reduces the number of variables, while it preserves as much information as possible. After the transformation, it creates a set of components, where each component contains the relevant information from the original variables.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png\n",
    "\"> The first question is: \n",
    "* **How many components should I consider?** That depends; let's test, setting the number of components as the number of columns the scaled data has, in this case, 30. That is useful in understanding the explained variance of each component. \n",
    "* Read the pseudo-code and comments in the below code cell to understand its logic. Once you run the cell, you will notice that:\n",
    "  * The first three components are more significant than the others. And, together, they sum 72.47% of the data variance. That is okay. It is a good sign when in a few components, like 3 or 4, you can get more than 80% of your data variance. So you could select three as the number of components, which is good progress since you had thirty features and now have three components.\n",
    "  * But in this exercise, for learning purposes, we will aim for more than 90% of data variance and use seven components since we could get more data variance with a relatively low increase of components. Before, we had thirty features with all data variance. Then switched to three components with 72% of data variance, and now seven components with 90% of data variance.\n",
    "  \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA # import PCA from sklearn\n",
    "\n",
    "n_components = 30 # set the number of components as all columns in the data\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)  # set PCA object and fit to the data\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "\n",
    "# the PCA object has .explained_variance_ratio_ attribute, which tells \n",
    "# how much information (variance) each component has \n",
    "# We store that to a DataFrame relating each component to its variance explanation\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "# prints how much of the dataset these components explain (naturally in this case will be 100%)\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "In the next cell we just copied the code from the cell above and changed n_components to 7. \n",
    "* With 7 components we achieved a bit more than 91% of data variance\n",
    "\n",
    "n_components = 7\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "Note that the data is transformed and stored at `x_PCA`. Let's check its content.\n",
    "* You will notice it is a NumPy array, and its dimension is 341 x 7, where the rows indicate the number of rows and seven relates to the number of components we defined earlier.\n",
    "* Imagine now that this data would be fed to a model. For this particular dataset, the ML task would be a classification.\n",
    "* Also, note that the PCA helped reduce from thirty features to seven components where these seven components contain 90% of the information.\n",
    "\n",
    "print(x_PCA.shape)\n",
    "x_PCA\n",
    "\n",
    "---\n",
    "\n",
    "### Visualise data after PCA transformation\n",
    "\n",
    "Imagine you want to visualise your data, before and after applying PCA.\n",
    "* If you had to visualize the thirty features, you could do a correlation analysis and look for features that are correlated among themselves or, in this particular dataset, features that are correlated to the target.\n",
    "* So let's suppose you want to visualise the relationship between \"mean concavity\" and \"mean concave points\" and the target. Since the features are numerical, you can do a scatter plot with them and colour by the target.\n",
    " * You will imagine/visualise the frontier between the blue and orange dots. Although that is good, the malignant and benign may look to be separable. At the same time, few data points look mingled in this frontier.\n",
    "  * However, what about the remaining variables? When you consider this dataset as a whole, is that informative enough to separate these classes?\n",
    "\n",
    "var1, var2 = 'mean concavity' , 'mean concave points'\n",
    "sns.scatterplot(x=X[var1], y=X[var2], hue=df_target['diagnostic'])\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2)\n",
    "plt.show()\n",
    "\n",
    "We can plot the PCA components to evaluate, from another perspective, how the data behaves.\n",
    "* We know x_PCA holds the data after transformation and has seven components. We will plot in a scatterplot the most representative components: components 0 and 1.\n",
    "\n",
    "sns.scatterplot(x=x_PCA[:,0], y=x_PCA[:,1])\n",
    "plt.xlabel('Component 0')\n",
    "plt.ylabel('Component 1')\n",
    "plt.show()\n",
    "\n",
    "We know that these two components hold by themselves 62% of the information (data variance).\n",
    "* This is powerful because with two variables (two components) we have a clearer vision of how the dataset looks to have enough information to separate malignant and benign.\n",
    "* We now colour the plot by diagnostic using df_target as the hue argument.\n",
    "  * Note we see a clearer border between 0 and 1.\n",
    "  * In a nutshell, we have the same data, showing the same information. The difference now is that the data was reduced to its major components.\n",
    "  * The drawback is that we lose the interpretation, since component 0 is made of a combination of the original variables.\n",
    "\n",
    "sns.scatterplot(x=x_PCA[:,0], y=x_PCA[:,1], hue=df_target['diagnostic'], alpha=0.8)\n",
    "plt.xlabel('Component 0')\n",
    "plt.ylabel('Component 1')\n",
    "plt.show()\n",
    "\n",
    "Naturally We can plot more components. In this exercise, we can plot three components in a 3D scatter plot using Plotly Express\n",
    "  * Move around the 3D plot and try to visualise if you could draw a surface that would separate the dots. The surface you imagined, is an ML model.\n",
    "  * Note again these three components alone hold 72% of all information from the dataset to diagnose malignant or benign.\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(x=x_PCA[:,0], y=x_PCA[:,1], z= x_PCA[:,2] , color=df_target['diagnostic'],\n",
    "                    labels=dict(x=\"Component 0\", y=\"Component 1\", z='Component 2'),\n",
    "                    color_continuous_scale='spectral',\n",
    "                    width=750, height=500)\n",
    "fig.update_traces(marker_size=5)\n",
    "fig.show()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 08 - Cluster\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand how to group similar data using KMeans clustering algorithm\n",
    "* Explain Clusters profiles\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 08 - Cluster\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Welcome to the world of unsupervised learning! It is slightly different from supervised learning, due to one aspect: there is no target variable. **The algorithm is left on its own to look for patterns in the data**\n",
    "* The ML task we will study is called a **cluster**, a type of unsupervised algorithm where it looks to group the data by similarity\n",
    "* The workflow used for a cluster will be also be slightly different from regression and classification tasks However, you will still do tasks like creating pipeline steps, fitting the pipeline using your data, and evaluating the pipeline. But now they will be done in a slightly different way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> There are multiple clustering algorithms in Scikit-learn; you may go to this [link](https://scikit-learn.org/stable/modules/clustering.html) and look for the potential algorithms to learn and use over your career. \n",
    "* We will study **KMeans** in this course since it is a starting point for your career and will not add much complexity to what we have been studying so far. In case you want to revise the concepts of KMeans, you may refer to Introduction to Predictive Analytics And Machine Learning - ML Essentials.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Workflow\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Introduction\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In practical terms, we don't know for sure how good your cluster model performance will be\n",
    "* Unless you gather a separate data and find a way to discover the actual value so you can compare it to the cluster prediction. That is not so trivial in practical terms, and in this course, we will not consider this alternative.\n",
    "* That being said, you will not know, for sure, for example, if a pipeline with four clusters is, in reality, better than a pipeline with seven clusters. However, there are approaches you can use to frame the project and reach more conclusive results that will help you to understand the patterns in your data.\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Expectation and Pipeline Objective\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> This notebook is dense and we cover many concepts. But always remember the core concept of this notebook is simple:\n",
    "* **Fit a Cluster Pipeline that groups similar data and explains each Cluster profile**\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Major ideas\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> The major ideas  we consider in this notebook are:\n",
    "* 1: **Create a Cluster pipeline**. Before fitting the pipeline, we need to define the number of PCA components and the number of clusters.\n",
    "* 2: **Fit the Cluster Pipeline**\n",
    "* 3: We need to **understand the Cluster profile**. We will use a classifier where the target is the cluster prediction to identify the most important variables that define a cluster.\n",
    "* 4: **Cluster analysis**: explain each cluster profile in terms of the most important variables. In addition, in case your dataset has a separate variable you want to study and you didn't include it in the cluster pipeline, you can study how this variable correlates to the clusters. In our case, we will analyze the clusters and the diagnostic (malignant or benign) \n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Practical Workflow\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> The practical workflow may be longer based on the ideas we outlined above. In particular, in this notebook, we will:\n",
    "* 1 - **Create a Cluster Pipeline** that contains the following steps: data cleaning, feature engineering, feature scaling, PCA and Cluster Model (KMeans). Note: this pipeline has parameters for PCA and Cluster that we will need to update over the notebook.\n",
    "* 2 - Analyse to determine the  number of components in a PCA. We will update that value in the Cluster Pipeline\n",
    "* 3 - Apply **Elbow Method and evaluate the Silhouette score**, to define the number of clusters in Cluster Pipeline\n",
    "* 4 - **Fit** the cluster pipeline\n",
    "* 5 - Add the cluster predictions to the data\n",
    "* 6 - Create a separate **Classifier Pipeline**, where the target variable is cluster predictions and features are the remaining variables\n",
    "* 7 - Fit this classifier, evaluate its performance and assess the most important features. These features are the most **important features needed to define the cluster predictions**\n",
    "* 8 - **Cluster analysis**: explain each cluster profile in terms of the most important features from the previous step. In addition, in case your dataset has a separate variable you want to study and you didn't include in the cluster pipeline, you can study how this variable correlates to the clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Load Data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's load the breast cancer data from sklearn. It shows records for a breast mass sample and a diagnosis confirming whether it is malignant or benign cancer, where 0 is malignant, and 1 is benign.\n",
    "* **Our objective is to cluster similar data points and then analyse the clusters against the diagnostic (malignant or benign)**\n",
    "  * As a result, **we will use only the thirty features** (all variables but Diagnostic) to fit the cluster pipeline.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We know in advance this dataset has only numerical features and no missing data.\n",
    "* We are intentionally adding missing data (`np.NaN`) in the first ten rows for 'mean smoothness' using `.iloc[:10,4]`, that better simulates the datasets you will likely face in the workplace.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df.iloc[:10,4] = np.NaN\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> ML Pipeline for Cluster\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The Cluster Pipeline is made of Data Cleaning (median imputation on `mean smoothness`) feature scaling, PCA and model (KMeans) steps\n",
    "* Note: `n_components` of PCA and `n_clusters` of KMeans values will be updated afterwards, for now, we leave arbitrary value of 50 (it could be any number).\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### ML algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def PipelineCluster():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['mean smoothness']) ),\n",
    "\n",
    "      (\"scaler\", StandardScaler()  ),    \n",
    "\n",
    "      (\"PCA\",  PCA(n_components=50, random_state=0)), \n",
    "\n",
    "      (\"model\", KMeans(n_clusters=50, random_state=0)  ), \n",
    "  ])\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineCluster()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Principal Component Analysis (PCA)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Principal Component Analysis, or PCA, is a transformation to your data and attempts to find out what features explain the most variance in your data.\n",
    "* PCA reduces the number of variables, while it preserves as much information as possible. After the transformation, it creates a set of components, where each component contains the relevant information from the original variables.\n",
    "* **This is useful in a Cluster pipeline since it is a method to reduce the feature space and provide data to the model that is in a better format for the algorithm to group similar data**.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested to find the most suitable `n_components`, then we update the value in the ML Pipeline for Cluster\n",
    "* To reach that, we will create an object based on PipelineCluster(), then remove the last two steps (PCA and model): `.steps[:-2]`\n",
    "* Finally, the `pipeline_pca` scales the data, so we can apply PCA afterwards\n",
    "\n",
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
    "df_pca = pipeline_pca.fit_transform(df)\n",
    "\n",
    "print(df_pca.shape,'\\n', type(df_pca))\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we apply PCA separately to the scaled data similar to what we did in previous unit notebooks.\n",
    "* Next, we are interested in defining the number of components from the PCA step. We will set the number of components as the number of columns the scaled data has, in this case, thirty. That is useful in understanding the explained variance of each component.\n",
    "* The interpretation is similar to the previous PCA notebook.\n",
    "  * The first three components are more significant than the others. And, together, they sum 72.47% of the data variance. That is okay. It is a good sign when in a few components, like three or four, you can get more than 80% of your data variance. So you could select three as the number of components, which is good progress since you had thirty features and now have three components.\n",
    "  * But in this exercise, for learning purposes, we will aim for more than 90% of data variance and use seven components since we could get more data variance with a relatively low increase of components.\n",
    "\n",
    "n_components = 30 # set the number of components as all columns in the data\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)  # set PCA object and fit to the data\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "\n",
    "# the PCA object has .explained_variance_ratio_ attribute, which tells \n",
    "# how much information (variance) each component has \n",
    "# We store that to a DataFrame relating each component to its variance explanation\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "# prints how much of the dataset these components explain (naturally in this case will be 100%)\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In the next cell, we just copied the code from the cell above and changed `n_components` to 7.\n",
    "  * With seven components we achieved a bit more than 90% of data variance\n",
    "\n",
    "n_components = 7\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)\n",
    "x_PCA = pca.transform(df_pca)\n",
    "\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "Next we rewrite the `PipelineCluster()`, updating `n_components` to 7\n",
    "* Note, in an actual project, you don't have to rewrite in the cell below the pipeline necessarily. You could have scrolled up to the cell where we defined the pipeline previously and updated there. But for learning purposes, we'll rewrite the pipeline in the cell below.\n",
    "\n",
    "def PipelineCluster():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['mean smoothness']) ),\n",
    "\n",
    "      (\"scaler\", StandardScaler()  ),    \n",
    "\n",
    "      (\"PCA\",  PCA(n_components=7, random_state=0)),  ##### we update the n_components to 7\n",
    "\n",
    "      (\"model\", KMeans(n_clusters=30, random_state=0)  ), \n",
    "  ])\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineCluster()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Elbow Method and Silhouette Score\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are now interested to find the most suitable value for `n_clusters`, then we update the value on the ML Pipeline for Cluster.\n",
    "* But how do you know the optimal amount of clusters for your data?\n",
    "* **We will combine 2 techniques (Elbow Method and Silhouette Score) to find the optimal value for the number of clusters**. Both will suggest values and we will use them in conjunction to decide on the optimal amount of clusters\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " We will first explain and apply Elbow. Then we will explain and apply the Silhouette score.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">  There is a technique called Elbow Method. According to [Yellowbrick documentation](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) - (an ML visualization library), the elbow method runs k-means clustering on the dataset for a range of values for k and then for each value of k computes an average score for all clusters. By default, the distortion score is computed as the sum of square distances from each point to its assigned centre.\n",
    "   \n",
    "* That is plotted as a line chart, where on the x-axis you find the values for the quantity of clusters and on the y-axis the distortion score. The line chart will remind you of an arm, then you will pick as a candidate the point of inflection (or the elbow) as the optimal value for the number of clusters.\n",
    "  * According to [Wikipedia](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), using the \"elbow\" or \"knee of a curve\" as a cutoff point is a common heuristic in mathematical optimization to choose a point where diminishing returns are no longer worth the additional cost. In clustering, this means one should choose several clusters so that adding another cluster doesn't give much better modelling of the data.\n",
    "  * You will also observe the plot and look at the values where there is a sharp steep fall in the distances. These ranges will be used in the Silhouette Score analysis. \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Prepare data for analysis\n",
    "  * You need to transform your data up to the point that it will hit the model, for Elbow Method and Silhouette score. \n",
    "    * Therefore we remove the last step (`.steps[:-1]`) and fit_transform `pipeline_analysis` to the data.\n",
    "    * Note the data has seven columns since it has passed through the PCA step.\n",
    "\n",
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))\n",
    "\n",
    "Next, we use [`KElbowVisualizer()`](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) from YellowbrickElbow Analysis to implement the Elbow Method\n",
    "* We pass in as arguments the algorithm we want (KMeans) and the range for the number of clusters we want to try, in this case from 1 to 10, so we pass in a tuple of (1,11), where the last value is not inclusive. \n",
    "* Here, there is no fixed recipe; you have to try a few ranges for the number of clusters. Initially, you may try a range of 1 to 10 or 1 to 15 and refine it accordingly.\n",
    "* Then we fit this object to the `df_elbow` (the data that passed through data cleaning, feature scaling and PCA)\n",
    "  * **Note the plot suggests three clusters!**\n",
    "  * **Note also that between 2 and 5 the values have a sharp and steep falloff. Outside this range, it does not fall off in a similar manner.**\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> There is also a **Silhouette score** that helps us to define the number of clusters. You can refer to Introduction to Predictive Analytics And Machine Learning > ML Essentials where we presented the concept in the performance metric video.\n",
    "\n",
    "* The silhouette score **interprets and validates the consistency within clusters**, which is based on the mean intra-cluster distance and mean nearest-cluster distance for each data point.\n",
    "  * The mean intra-cluster distance is the average distance between the data point and all other data points in the same cluster. Essentially, how far each data point is from the centre of its own cluster. \n",
    "  * The mean nearest-cluster distance on the other hand is the average distance between the data point and all other data points of the next nearest cluster. In other words, how far each data point in 1 cluster is to the centre of its nearest neighbouring cluster.\n",
    " \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The silhouette score range is from -1 to +1, where:\n",
    "  *   “+1” means that a clustered data point is dense and properly separated from other clusters. \n",
    "  * A score close to 0 means the clustered data point is overlapping with another cluster.  \n",
    "  * A negative score means that the clustered data point may be wrong; it may even belong to another cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " The silhouette score for each data point allows you to build a Silhouette plot, showing each silhouette score for each data point across all clusters.\n",
    "* You can then calculate an **average silhouette score** for the plot. This average helps to (1) compare different models with a different number of clusters and (2) define a performance metric for a given cluster model. A rule of thumb in the industry is that an average silhouette score greater than 0.5 means the clusters are nicely separated, but there may be a case where for your dataset, the optimal amount of cluster leads to an average lower than 0.5. That is fine also. It just means we computed the optimal way for that dataset to cluster even though it doesn't have a tremendous silhouette score.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> To evaluate a cluster's silhouette, we need the data formatted before it hits the model. We have done this already, and the result is stored at `df_analysis`\n",
    "* We will use [SilhouetteVisualizer](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html) and [KElbowVisualizer](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) from Yellowbrick\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The code has the following logic:\n",
    "* **First, you will calculate the average silhouette score for different numbers of clusters** using  KElbowVisualizer() by setting KMeans() as the algorithm,  the range 2 to 5 for the number of clusters (it doesn't accept 1 cluster) and the  metric='silhouette'. Then you will fit the scaled data (df_analysis) and show the results\n",
    "  * You will evaluate which number of clusters produce the higher average silhouette score.\n",
    "* Then you will iterate on the **silhouette plot for models with a different number of clusters**, in this case from 2 to 11. You will use SilhouetteVisualizer() and set the estimator as KMeans(). Then you will fit the scaled data (df_analysis) and show the results\n",
    "  *  You will evaluate if there are clusters with a maximum score below average score, if the silhouette values vary too much in the cluster, if there are too many silhouette values lower than the average silhouette score and if there are too many negative silhouette values.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note the following:\n",
    "* Average Silhouette Score: the best result is with two clusters, but three is not that far away. We will give more attention to evaluting the Silhouette Plot from these options. \n",
    "* Silhouette Plot:\n",
    "  * Two clusters: One cluster is dominant (the blue) since it has more observations, and the majority of its values are greater than the average score (the red dotted line). The other cluster (green) has a few data points with a negative score (these may belong to other clusters) and almost no data point is above the average score.\n",
    "\n",
    "  * Three clusters: One cluster is dominant (the blue) since it has more observations, and the majority of its values are greater than the average score (the red dotted line). The other two clusters look to have a similar frequency. The last blue cluster has a few data points greater than the average and a few with negative silhouette values. However, the green middle cluster has a few data points with a negative score (these may belong to other clusters) and almost no data points are above the average score. This is not as bad as the two clusters since more observations are above the average score in the non-dominant clusters.\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(2,7), metric='silhouette')\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "for n_clusters in np.arange(start=2,stop=11):\n",
    "  \n",
    "  print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "  visualizer = SilhouetteVisualizer(estimator = KMeans(n_clusters=n_clusters, random_state=0),\n",
    "                                    colors = 'yellowbrick')\n",
    "  visualizer.fit(df_analysis)\n",
    "  visualizer.show()\n",
    "  plt.show()\n",
    "  print(\"\\n\")\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%208-%20Challenge.png\"> What is the number of clusters then?\n",
    "* Elbow Method says three.\n",
    "* The average Silhouette Score says two, but the Silhouette Plot from three clusters is better than for two clusters.\n",
    "* As a result, we will pick three, since the Elbow Method and Silhouette Plot both support that decision.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next we rewrite the `PipelineCluster()`, updating `n_cluster` to 3\n",
    "* Note, you don't have to necessarily rewrite in the cell below the pipeline, in an actual project. You could have scrolled up to the cell where we defined the pipeline previously and updated there. But for learning purposes, we'll rewrite the pipeline in the cell below.\n",
    "\n",
    "def PipelineCluster():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['mean smoothness']) ),\n",
    "\n",
    "      (\"scaler\", StandardScaler()  ),    \n",
    "\n",
    "      (\"PCA\",  PCA(n_components=7, random_state=0)), \n",
    "\n",
    "      (\"model\", KMeans(n_clusters=3, random_state=0)  ),  ##### update n_clusters to 3 \n",
    "  ])\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineCluster()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> Notice the additional effort and steps we take in clustering compared to the workflow we have for Classification and Regression. Only now are we ready to train the pipeline.\n",
    "  * Note we have only one pipeline, and we are not doing hyperparameter optimisation when training the model.\n",
    "  * We \"kind\" of made a hyperparameter optimisation in the previous sections since we tried different options for PCA components and the number of clusters for KMeans().\n",
    "  * Let's fit the pipeline then!\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit Cluster Pipeline\n",
    "\n",
    "We don't need to split our data. All available data is used for training. \n",
    "* For training purposes, we create a DataFrame `X` that is a copy of your data.\n",
    "\n",
    "X = df.copy()\n",
    "print(X.shape)\n",
    "X.head(3)\n",
    "\n",
    "Then we fit the Cluster pipeline to the training data (`X`)\n",
    "\n",
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Add cluster predictions to dataset\n",
    "\n",
    "We add a column \"`Clusters`\" (with the Cluster Pipeline predictions) to X\n",
    "* Scroll to the right and check the last variable. That is the cluster predictions for each data point of your dataset.\n",
    "* The model predictions are stored in an attribute `.labels_`\n",
    "* Since the model is in a pipeline, you will grab the `model` using the notation `pipeline_cluster['model'].labels_`\n",
    "\n",
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next we are interested to know the cluster frequency\n",
    "* Note there are three clusters, and the counting starts from 0.\n",
    "* We note that the algorithm found that the majority of the data (63%) belongs to cluster number 2, where the remaining datapoints are shared equally between the other 2 clusters.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">  **But what is the profile of each cluster?**\n",
    "\n",
    "\n",
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit a classifier, where the target is cluster predictions and the features are the remaining variables\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are at a point where we have cluster predictions made from the cluster pipeline, but we can't interpret the clusters yet. \n",
    "\n",
    "\n",
    "We are **interested in learning each cluster's profile**, based on the most relevant dataset variables.\n",
    "* Our new dataset has `Clusters` as a variable. We use a technique where  `Clusters` will be the **target for a classifier**, and the remaining variables will be features for that target.\n",
    "  * We will assume that the most relevant features for this classifier, will be the most relevant variables that define a cluster.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " To do that, we will use the traditional workflow we covered in the previous notebooks: \n",
    " * 1 - split the data into train and test set\n",
    " * 2 - create the classifier pipeline\n",
    " * 3 - fit the classifier to training data\n",
    " * 4 - evaluate pipeline performance\n",
    " * 5 - and (most important for our analysis) **assess feature importance**.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png \"> Note: If you need, pause for a second and reflect on which step from the \"Major ideas\" section we are in. That may help you to better understand our goal, which point we are at, and the next step we move on to.\n",
    "\n",
    "\n",
    "We start by copying `X` to a DataFrame `df_clf`\n",
    "\n",
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)\n",
    "\n",
    "Next, we split train and test sets, where the target variable is `'Clusters'`\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['Clusters'],axis=1),\n",
    "                                    df_clf['Clusters'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=0\n",
    "                                    )\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Create a classifier pipeline \n",
    "* We should use the **data cleaning and feature engineering** steps from the Cluster Pipeline.\n",
    "* Then we add the conventional steps for supervised learning: **feature scaling, feature selection and modelling**\n",
    "* We are considering a model that typically offers good results, and feature's importance can be assessed with `.features_importance_` using a tree-based algorithm. We are using GradientBoostingClassifier since it typically has good performance while it is fast to train.\n",
    "  * We could conduct a detailed hyperparameter optimisation to find the best tree-based model, but we are most interested in finding a pipeline that can explain the relationship between the target (Clusters) and the features to assess the feature's importance afterwards.\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "def PipelineClf2ExplainClusters():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['mean smoothness']) ),\n",
    "\n",
    "      (\"scaler\", StandardScaler()  ),    \n",
    "\n",
    "      (\"feat_selection\", SelectFromModel(GradientBoostingClassifier(random_state=0)) ), \n",
    "\n",
    "      (\"model\",  GradientBoostingClassifier(random_state=0) ), \n",
    "  ])\n",
    "  return pipeline_base\n",
    "\n",
    "  \n",
    "PipelineClf2ExplainClusters()\n",
    "\n",
    "We fit the classifier to the training data\n",
    "* Note again, here we are not doing a detailed hyperparameter optimisation. This classification pipeline is useful only for the the features that look to be more important to predict the Clusters. We are not deploying this model, so fitting with the default hyperparameters is fine for this task.\n",
    "\n",
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Evaluate the classifier performance on the Train and Test Sets\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " In theory, we expect to have a good performance, since the Clusters were generated by the KMeans() and that algorithm has a logic. As a result, the classifier algorithm (GradientBoosting) would be able to map these relationships, in theory. So let's check that.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Then evaluate the performance on the Train set using `classification_report()`\n",
    "* It looks to have learned the relationships to ace all predictions in the train set.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> And finally, we evaluate in the test set. \n",
    "* It looks to have learned the relationship between the target and the features to generalise on the test set, since the performance is not much different from the train set.\n",
    "\n",
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Assess the Most Important Features that define a cluster\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Now we assess the feature importance from the pipeline. First, we need to know how many data cleaning and feature engineering steps your pipeline has.\n",
    "* It has one step only: median imputation.\n",
    "\n",
    "pipeline_clf_cluster\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We use the same code we saw in the previous unit notebook where we grab the feature importance from the feature selection step and store it in a DataFrame.\n",
    "* The plot shows that these are the 4 most important features in descending order: `['mean concavity', 'worst perimeter', 'worst fractal dimension', 'mean perimeter'] `\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> **We are considering these as the most important variables that define a Cluster. They will be used to understand the Cluster Profile**\n",
    "\n",
    "# after data cleaning and feat engineering, the feature space changes\n",
    "\n",
    "data_cleaning_feat_eng_steps = 1 # how many data cleaning and feature engineering steps does your pipeline have?\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
    "          'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "best_features = df_feature_importance['Feature'].to_list() # reassign best features in importance order\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Cluster Analysis\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Bravo! You know which variables to consider now to explain each cluster!\n",
    "* Let's create a custom function where we will explain the cluster profile, in terms of  `['mean concavity', 'worst perimeter', 'worst fractal dimension', 'mean perimeter']`. For each cluster, we want to know the most common values for each variable.\n",
    "\n",
    "* Go through the code and check the pseudo-code and comments to understand its logic. It may take a while to understand it all, but the focus is to understand and apply the function to our business problem.\n",
    "\n",
    "# df contains the most important features and the clusters\n",
    "# Note: your DataFrame needs to have a variable called 'Clusters' which will\n",
    "# contain the cluster prediction from the pipeline\n",
    "\n",
    "# It outputs a table showing for each cluster what is the most common values for a given variable\n",
    "\n",
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "  DescriptionAllClusters = pd.DataFrame(columns=df.drop(['Clusters'],axis=1).columns)\n",
    "  # iterate on each cluster , calls Clusters_IndividualDescription()\n",
    "  for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "    \n",
    "      EDA_ClusterSubset = df.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
    "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster,decimal_points)\n",
    "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
    "\n",
    "  \n",
    "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
    "  return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster,cluster, decimal_points):\n",
    "\n",
    "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "  # for a given cluster, iterate in all columns\n",
    "  # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
    "    # That will show the range for the most common values for the numerical variable\n",
    "  # if the variable is categorical, count the frequencies and display the top 3 most frequent\n",
    "    # That will show the most common levels for the category\n",
    "\n",
    "  for col in EDA_Cluster.columns:\n",
    "    \n",
    "    try:  # eventually a given cluster will have only missing data for a given variable\n",
    "      \n",
    "      if EDA_Cluster[col].dtypes == 'object':\n",
    "        \n",
    "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
    "        Description = ''\n",
    "        \n",
    "        for x in range(len(top_frequencies)):\n",
    "          freq = top_frequencies.iloc[x]\n",
    "          category = top_frequencies.index[x][0]\n",
    "          CategoryPercentage = int(round(freq*100,0))\n",
    "          statement =  f\"'{category}': {CategoryPercentage}% , \"  \n",
    "          Description = Description + statement\n",
    "        \n",
    "        ClustersDescription.at[0,col] = Description[:-2]\n",
    "\n",
    "\n",
    "      \n",
    "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
    "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "        Q1 = round(DescStats.iloc[4,0], decimal_points)\n",
    "        Q3 = round(DescStats.iloc[6,0], decimal_points)\n",
    "        Description = f\"{Q1} -- {Q3}\"\n",
    "        ClustersDescription.at[0,col] = Description\n",
    "    \n",
    "    \n",
    "    except Exception as e:\n",
    "      ClustersDescription.at[0,col] = 'Not available'\n",
    "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "  \n",
    "  ClustersDescription['Cluster'] = str(cluster)\n",
    "  \n",
    "  return ClustersDescription\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The next custom function is called `cluster_distribution_per_variable() ` and is used to analyse the Clusters and given Variable - in our case, it will evaluate **Clusters x Diagnostic**.\n",
    "* It will show the absolute and relative levels of Diagnostic (Malignant and Benign) per cluster\n",
    "* Go through the code and check the pseudo-code and comments to understand its logic. It may take a while to understand it, but the focus is to understand and apply the function to our business problem.\n",
    "\n",
    "import plotly.express as px\n",
    "def cluster_distribution_per_variable(df,target):\n",
    "\n",
    "  # the data should have 2 variables, the cluster predictions and\n",
    "  # the variable you want to analyze with, in this case we call \"target\"\n",
    "  \n",
    "  # we use plotly express to create 2 plots\n",
    "  # cluster distribution across the target\n",
    "  # relative presence of the target level in each cluster\n",
    "  \n",
    "   \n",
    "  df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index() \n",
    "  df_bar_plot.columns = ['Clusters',target,'Count']\n",
    "  df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
    "\n",
    "  print(f\"Clusters distribution across {target} levels\")\n",
    "  fig = px.bar(df_bar_plot, x='Clusters',y='Count',color=target,width=800, height=500)\n",
    "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "  df_relative = (df\n",
    "                 .groupby([\"Clusters\", target])\n",
    "                 .size()\n",
    "                 .groupby(level=0)\n",
    "                 .apply(lambda x:  100*x / x.sum())\n",
    "                 .reset_index()\n",
    "                 .sort_values(by=['Clusters'])\n",
    "                 )\n",
    "  df_relative.columns = ['Clusters',target,'Relative Percentage (%)']\n",
    " \n",
    "\n",
    "  print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "  fig = px.line(df_relative, x='Clusters',y='Relative Percentage (%)',color=target,width=800, height=500)\n",
    "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
    "  fig.update_traces(mode='markers+lines')\n",
    "  fig.show()\n",
    " \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> To start the analysis we want a DataFrame that contains the best features and Cluster Predictions since we want to analyse the patterns for each cluster.\n",
    "* We will copy `df_clf` DataFrame (since it has all the features and Cluster predictions) and filter `best_features` plus `['Clusters']`.\n",
    "\n",
    "\n",
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We want also to analyze Diagnostic levels\n",
    "* In this exercise, we get it from `data.target` and create a DataFrame.\n",
    "* We know in advance Diagnostic represents a categorical variable and came as an integer. Therefore we change its data type to `'object'`.\n",
    "\n",
    "df_diagnostic = pd.DataFrame(data.target, columns=['diagnostic'])\n",
    "df_diagnostic['diagnostic'] = df_diagnostic['diagnostic'].astype('object')\n",
    "df_diagnostic.head(3)\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Cluster profile on most important features\n",
    "\n",
    "We call `DescriptionAllClusters()` and parse a concatenated DataFrame made with `df_cluster_profile` and `df_diagnostic`. Before parsing let's just show this concatenated data so you can visualise it better.\n",
    "* It has the best features `['mean concavity', 'worst perimeter', 'worst fractal dimension', 'mean perimeter']`, Cluster Predictions and Diagnostic (where 0 is malignant, 1 is benign)\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([df_cluster_profile,df_diagnostic], axis=1).head(4)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Finally, we use `DescriptionAllClusters()` parsing the concatenated DataFrame. It outputs a table showing for each cluster what are the most common values for a given variable, including the diagnostic level (where 0 is malignant, 1 is benign). You will also parse the decimal points you want to display when the evaluated variable is numerical; depending on the range of the numerical variable, you may need more decimal points. In our case, 2 decimal points are fine, but you can re-run the function after and check with different values, like 0 and 4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> Recall that we found the most important variables that help to define a cluster are: `['mean concavity', 'worst perimeter', 'worst fractal dimension', 'mean perimeter']`\n",
    "* Note that the algorithm found that for Cluster 0, the most common values for mean concavity are between 0.13 -- 0.22, for worst perimeter, is between 145.7 -- 174.18, worst fractal is between\t0.08 -- 0.09 and mean perimeter is between\t120.88 -- 136.88. Also, all diagnoses in cluster 0 are 0 - malignant. **This is the profile from cluster 0!**\n",
    "  * Repeat this analysis for the remaining clusters. Note we start giving meaning to each cluster.\n",
    "* Note also our analysed variable (diagnostic). It shows that cluster 0 has  only malignant cases, cluster 1 is a mix between malignant and benign, but malignant is more dominant, and cluster only has two benign cases. \n",
    "  * Think for a moment about how cool that is. The algorithm found patterns to split into three groups, one with malignant, another a mix and the last benign. Now think how this analysis could be applied to solve other business problems.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note the major differences/patterns between clusters across variables, like:\n",
    "  * The ranges of mean concavity look to be smaller when the diagnostic is benign (1) and look to increase when diganostic tends to 0 (malignant).\n",
    "  * The values of the worst perimeter in clusters where malignant is predominant tend to be higher than in benign clusters.\n",
    "    * Note we keep adding meaning to how the clusters interact based on the analysis between a given variable (mean concavity, for example) and diagnostic\n",
    "  * Repeat the same analysis for other variables (worst fractal and mean perimeter)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " Typically you will notice differences in ranges across the clusters and across the levels of your analyzed variable (diagnostic). This difference is typically the pattern we are interested to discover.\t\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_diagnostic], axis=1),\n",
    "                                          decimal_points=2)\n",
    "clusters_profile\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Clusters distribution across Diagnostic levels & Relative Percentage of Diagnostic in each cluster\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> This analysis shows the Cluster's distribution across Diagnostic. This information is revealed in the previous table, but now we can make it more visual to stakeholders. It has 2 plots:\n",
    "* The first is a bar plot, in the x-axis the clusters, the bar length is how many data points are in that cluster and is coloured by the level of diagnosis (where 0 is malignant, 1 is benign).\n",
    "* The second plot gives a complementary vision to the first. In the first, we saw the absolute values (the counts). Now we see the relative (the percentage).\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> Let's analyse the plots\n",
    "* The first plot shows that cluster 0 has malignant cases only. Cluster 1 a mix of both cases with malignant predominant. The last cluster is predominantly benign cases (however, there are few malignant cases. If required, you could do a data analysis later on these malignant cases.)\n",
    "* The second plot reveals the percentage presence of Diagnostic (malignant and benign) and displays the percentage in each cluster.\n",
    "\n",
    "\n",
    "df_cluster_vs_diagnostic=  df_diagnostic.copy()\n",
    "df_cluster_vs_diagnostic['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_diagnostic, target='diagnostic')\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> What should I do now?\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> You could deploy the Cluster Pipeline as is. However, it would **need all 30 variables** to predict a given cluster for a new breast sample even though you used 4 variables to describe the profile from each cluster.\n",
    "* In a real system, we should consider the number of input variables we want to manage.\n",
    "* Therefore, we would consider an additional step for trying to **refit the cluster pipeline using the most important variables**. We say \"trying\" since we will need to conduct a tradeoff analysis to validate if the pipeline with all variables and the pipeline with only the \"best feature\" produce \"equivalent\" results.\n",
    "  * In case they produce \"equivalent\" results, you can deploy a pipeline with fewer variables that will deliver a similar performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> However, we will study this approach in our second walkthrough project\n",
    "* For the moment, what really matters is to understand that we can **cluster the data on similar data points, explain the profile of clusters, and we can analyse the clusters vs another variable** (in our case, clusters vs diagnostic)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 09 - NLP (Natural Language Processing)\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand and create an ML pipeline for NLP (Natural Language Processing)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - NLP (Natural Language Processing)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Conversational language, unlike text neatly entered into form inputs, is unstructured data that cannot be neatly broken down into elements in a row-column database table; there is a vast quantity of information available within it and waiting to be accessed. \n",
    "* Therefore, natural language processing aims to gather, extract and make available all of this information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%208-%20Challenge.png\"> NLP is not a trivial task since its goal is to understand the language, not only process the text/strings/keywords. \n",
    "* As we know, language is ambiguous, subjective and subtle.  New words and terms are constantly added/updated and their meaning may change according to the context. \n",
    "* These aspects all together make NLP a very interesting and challenging task for ML.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will study NLP (Natural Language Processing) as a supervised learning approach where the features are text and the target variable is a meaning associated with that given text. Therefore the ML task is Classification.\n",
    "* Therefore the workflow will be similar to what we covered for Classification tasks, where we:\n",
    "    * Load the data\n",
    "    * Define the pipeline steps\n",
    "    * Split the data into train and test sets\n",
    "    * Train multiple pipelines using hyperparameter optimisation\n",
    "    * Evaluate pipeline performance\n",
    "* One difference will be defining the pipeline steps, where we will use steps for pre-processing the textual data before the modelling stage. Once you have a processed text, you can use ML algorithms to predict your target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Load data\n",
    "\n",
    "We will use a dataset that contains records telling if a given SMS message is spam or not (spam or ham). We load the data from GitHub.\n",
    "* In this project we are interested to **predict if a given message is spam or not**, therefore the ML task is Classification.\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ShresthaSudip/SMS_Spam_Detection_DNN_LSTM_BiLSTM/master/SMSSpamCollection'\n",
    "df = (pd.read_csv(url, sep ='\\t',names=[\"label\", \"message\"])\n",
    "    .sample(frac=0.6, random_state=0)\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "df = df.sample(frac=0.5, random_state=101)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Note: just as a reminder, in an actual project, once you load your textual data, you could explore it using the techniques covered in the Text Analysis lesson. \n",
    "* We will not do that here since our focus is on the ML process used in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Split data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  As usual, we are splitting the data into train and test sets.\n",
    "* In this case, there are two columns in the dataset, where the `message` contains the text, and the `label` tells if the SMS message was spam or not.\n",
    "* In the end, we have a Pandas Series for the features (`message`) and target (`label`) - note the brackets subsetting the data, for example, `df['message']`\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'],\n",
    "                                                    test_size=0.2, random_state=101)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Create the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> We will consider classic steps in an NLP pipeline, where we first clean the text and extract the features for the model.\n",
    "* The pipeline steps will be slightly different from what we have been studying within Classification (Data Cleaning, Feature Engineering, Feature Scaling, Feature Selection and Model), but the purpose is the same: prepare the data for the model.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " Overall, here we will consider steps for **(1) cleaning the textual data and (2) representing the text as numbers, or feature extraction.**\n",
    "* (1) In our case, we will make the text lowercase and remove punctuation for text cleaning.\n",
    "    * The practical tasks for cleaning the textual data will differ from dataset to dataset; for example, you may have a dataset where you need to clean HTML tags, so you need a function to do that for you; or eventually, you need to remove diacritics (marks located above or below a letter to reflect a particular pronunciation, like *resumé*)\n",
    "  \n",
    "* (2) There are also multiple techniques for feature extraction; we will consider the ones we covered in Module 2; in this case, we **will tokenize the text and then use TF-IDF (Term Frequency－Inverse Document Frequency)**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are using texthero to clean the textual data, **by changing the text to lowercase and removing punctuation** from the textual data\n",
    "* If you want to refresh these concepts, you may refer back to Machine Learning Essentials > Machine Learning Tasks > Natural Language Processing, Recommender Systems unit video.\n",
    "* We must create a custom Python class to parse it into the pipeline thereafter. We are using the same approach for creating custom transformers we saw in the feature-engine lesson, where we use BaseEstimator, TransformerMixin, and create fit and transform methods. So the custom transformer can be added correctly to the ML pipeline.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import texthero as hero\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class text_cleaning(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self ):\n",
    "    return None\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    X = hero.preprocessing.lowercase(X)\n",
    "    X = hero.remove_punctuation(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For feature extraction, we use **CountVectorizer** and **TfidfTransformer**. You can find their documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
    "* We need to convert the textual data to a format from which the algorithms can learn the relationships, also known as vectors. \n",
    "  * CountVectorizer: According to its documentation, it converts a collection of text documents to a matrix of token counts. It stores the number of times every word is used in our text data. We are also removing English \"stop words\".\n",
    "  * (TfidfTransformer) Term Frequency－Inverse Document Frequency Transformer: It transforms a count matrix to a normalised tf or tf-idf representation according to its documentation. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and are empirically less informative than features that occur in a small fraction of the data. In addition, this highlights the words that are most unique to a document, thus better for characterising it. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> Finally, our pipeline will have four steps:\n",
    "* Text cleaning: lowercase the text and remove punctuation\n",
    "* CountVectorizer: convert text to token\n",
    "* TF-IDF: transform a count matrix to a normalised tf or tf-idf representation\n",
    "* Model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline = Pipeline([\n",
    "                       \n",
    "        ( 'text_cleaning', text_cleaning() ),\n",
    "        ( 'vect', CountVectorizer(stop_words='english') ),\n",
    "        ( 'tfidf', TfidfTransformer() ),\n",
    "        ( 'model', model )\n",
    "    ])\n",
    "  \n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We load the Python class (HyperparameterOptimizationSearch) that we studied in previous units, which aims to fit a set of algorithms with multiple hyperparameters. A quick reminder of what this class does: \n",
    "* The developer defines a set of algorithms and their respective hyperparameters values.\n",
    "* The code iterates on each algorithm and fits pipelines using GridSearchCV considering its respective hyperparameter values. The result is stored.\n",
    "That is repeated for all algorithms that the user listed.\n",
    "* Once all pipelines are trained, the developer can retrieve a list with a performance result summary and an object that contains all trained pipelines. The developer can then subset the best pipeline.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model=  PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> List algorithms\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Now we list the algorithms we want to use for this task. First, we are considering new estimators from Scikit-learn that typically tend to offer reasonable performance for NLP tasks.\n",
    "  * It doesn't mean we couldn't have considered the algorithms we have seen already in the course, like tree-based algorithms. However, the central aspect is that we should use algorithms that tend to be more effective for NLP tasks.\n",
    "  * For teaching purposes, we will consider only two algorithms (SGDClassifier and LinearSVC) from this set of algorithms used for NLP tasks to speed up the learning process. However, we suggest you try out the other algorithms at your own pace and time\n",
    "  * We will not give full details of how these other algorithms work to avoid overloading you with a lot of new information. It will be a matter of time, experience and curiosity for you to keep learning new topics as a data practitioner, including learning about additional families of algorithms. There is a BONUS section at the end of the next notebook where we will briefly explain the algorithms and present the typical hyperparameters used for the NLP classification task.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "models_search = {\n",
    "    #\"MultinomialNB\":MultinomialNB(),\n",
    "    \"SGDClassifier\":SGDClassifier(random_state=101),\n",
    "   # \"SVC\": SVC(random_state=101),\n",
    "    \"LinearSVC\": LinearSVC(random_state=101),\n",
    "}\n",
    "\n",
    "\n",
    "params_search = {\n",
    "   # \"MultinomialNB\":{},\n",
    "    \"SGDClassifier\": {},\n",
    "   # \"SVC\": {},\n",
    "    \"LinearSVC\": {},\n",
    "}\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\">\n",
    " We are using the technique we covered in previous units for hyperparameter optimisation, where we:\n",
    "* 1-  Fit multiple pipelines with multiple algorithms using their default hyperparameters. So we can find the algorithms that look to best fit the data\n",
    "* 2 - Then we fit multiple pipelines for the best algorithms using multiple hyperparameter combinations.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit multiple pipelines with multiple algorithms using their default hyperparameters\n",
    "\n",
    "We start by fitting multiple pipelines using the default hyperparameters.\n",
    "* We pass in the training data, set the scoring metric to accuracy (we assume our stakeholders are interested in how accurate their system is) and set cv=2 (typically you may set it to 5, but for simplification and to have a faster training, we set it to 2).\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-2,\n",
    "           cv=2)\n",
    "\n",
    "Let's check  the training results summary.\n",
    "* Note that SGDClassifier performed best, and the difference to LinearSVC is slight; both are close.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 09 - NLP (Natural Language Processing)\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand and create an ML pipeline for NLP (Natural Language Processing)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - NLP (Natural Language Processing)\n",
    "\n",
    "* We will continue from the previous notebook, where we found the algorithm that most suited the data (SGDClassifier) and now we are doing an extensive hyperparameter optimisation to find the pipeline with the best hyperparameter combination.\n",
    "* Once we find the best pipeline, we will evaluate the pipeline and make predictions using real-time data.\n",
    "* We will need to reload the data, and create a custom function for hyperparameter optimisation and pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Load data\n",
    "\n",
    "We will use a dataset that contains records telling if a given SMS message is spam or not (spam or ham). We load the data from GitHub.\n",
    "* In this project we are interested in **predicting if a given message is spam or not**, therefore the ML task is Classification\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ShresthaSudip/SMS_Spam_Detection_DNN_LSTM_BiLSTM/master/SMSSpamCollection'\n",
    "df = (pd.read_csv(url, sep ='\\t',names=[\"label\", \"message\"])\n",
    "    .sample(frac=0.6, random_state=0)\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "df = df.sample(frac=0.5, random_state=101)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Split data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  As usual, we are splitting the data into train and test sets.\n",
    "* In this case, there are two columns in the dataset, where the `message` contains the text, and the `label` tells if the SMS message is spam or not.\n",
    "* In the end, we have a Pandas Series for the features (`message`) and target (`label`) - note the brackets subsetting the data, for example, `df['message']`\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'],\n",
    "                                                    test_size=0.2, random_state=101)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Create the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> We will consider classic steps in an NLP pipeline, where we first clean the text and then extract the features for the model.\n",
    "* The pipeline steps will be slightly different from what we have been studying within Classification (Data Cleaning, Feature Engineering, Feature Scaling, Feature Selection and Model), but the purpose is the same: prepare the data for the model.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " Overall, here we will consider steps for **(1) cleaning the textual data and (2) representing the text as numbers or feature extraction.**\n",
    "* (1) In our case, we will make the text lowercase and remove punctuation for text cleaning.\n",
    "    * The practical tasks for cleaning the textual data will differ from dataset to dataset; for example, you may have a dataset where you need to clean HTML tags, so you need a function to do that for you; or eventually, you need to remove diacritics (marks located above or below a letter to reflect a particular pronunciation, like *resumé*)\n",
    "  \n",
    "* (2) There are also multiple techniques for feature extraction; we will consider the ones we covered in ML essentials; in this case, we **will tokenize the text then use TF-IDF (Term Frequency－Inverse Document Frequency)**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are using texthero to clean the textual data, **by changing the text to lowercase and removing punctuation** from the textual data\n",
    "* If you want to refresh these concepts, you may refer back to the NLP video.\n",
    "* We need to create a custom Python class to pass it into the pipeline afterwards. We are using the same approach for creating custom transformers we saw in the feature-engine lesson, where we use BaseEstimator, TransformerMixin, and create fit and transform methods. So the custom transformer can be added correctly to the ML pipeline.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import texthero as hero\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class text_cleaning(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self ):\n",
    "    return None\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    X = hero.preprocessing.lowercase(X)\n",
    "    X = hero.remove_punctuation(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For feature extraction we use **CountVectorizer** and **TfidfTransformer**, you can find their documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
    "* We need to convert the textual data to a format that the algorithms can learn the relationships from, also known as vectors. \n",
    "  * CountVectorizer: According to its documentation, it converts a collection of text documents to a matrix of token counts. It stores the number of times every word is used in our text data. We are also removing English \"stop words\".\n",
    "  * (TfidfTransformer) Term Frequency－Inverse Document Frequency Transformer: It transforms a count matrix to a normalized tf or tf-idf representation according to its documentation. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and are empirically less informative than features that occur in a small fraction of the data. In addition, this highlights the words that are most unique to a document, thus better for characterising it. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> Finally, our pipeline will have four steps:\n",
    "* Text cleaning: lowercase the text and remove punctuation.\n",
    "* CountVectorizer: convert text to token.\n",
    "* TF-IDF: transform a count matrix to a normalised tf or tf-idf representation.\n",
    "* Model.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline = Pipeline([\n",
    "                       \n",
    "        ( 'text_cleaning', text_cleaning() ),\n",
    "        ( 'vect', CountVectorizer(stop_words='english') ),\n",
    "        ( 'tfidf', TfidfTransformer() ),\n",
    "        ( 'model', model )\n",
    "    ])\n",
    "  \n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We load the Python class (HyperparameterOptimizationSearch) that we studied in previous units, which aims to fit a set of algorithms with multiple hyperparameters. A quick reminder on what this class does: \n",
    "* The developer defines a set of algorithms and their respective hyperparameters values\n",
    "* The code iterates on each algorithm and fits pipelines using GridSearchCV considering its respective hyperparameter values. The result is stored.\n",
    "That is repeated for all algorithms that the user listed.\n",
    "* Once all pipelines are trained, the developer can retrieve a list with a performance result summary and an object that contains all trained pipelines. The developer can then subset the best pipeline.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model=  PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Fit multiple pipelines for best algorithms using multiple hyperparameter combination\n",
    "\n",
    "We update our dictionaries using the algorithms and hyperparameters combinations we want to optimise.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "models_search = {\n",
    "    \"SGDClassifier\":SGDClassifier(random_state=101),}\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"SGDClassifier\": {'model__tol':[1e-2, 1e-1], },\n",
    "  }\n",
    "\n",
    "Next, we fit multiple pipelines using the algorithms we selected considering multiple combinations of hyperparameters.\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-2,\n",
    "           cv=2)\n",
    "\n",
    "Let's check the training results summary \n",
    "* Note that SGDClassifier performed best. Not only has the performance improved from the default hyperparameters but now SGDClassifier is performing better than LinearSVC.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "We check for the best model programmatically.\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "So we can grab the best model parameters.\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "And grab the best pipeline.\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pipeline Performance\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Finally, we evaluate the pipeline as usual with our custom function for classification tasks.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We pass in the arguments with which we are familiar.\n",
    "* Train and Test set\n",
    "* Best pipeline\n",
    "* for `label_map`, we get the classes name with `.unique()`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> Note: The model learned the relationships in the data in the train set and predicted everything correctly. In the test set, we had a few misclassifications, but still, the performance looks good, and the **model could generalise on the unseen data** (test set)\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df['label'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Predict using real-time data\n",
    "\n",
    "Pass in a real-time message to validate whether or not you shall click on the link   :)\n",
    "* Try new sentences, by changing the content on the `real_time_msg` variable.\n",
    "\n",
    "########################################################################\n",
    "real_time_msg = 'Congratulations, you won the auction. Please click on link below to get your prize'\n",
    "########################################################################\n",
    "\n",
    "X_live = pd.Series(data=real_time_msg, name='message')\n",
    "best_pipeline.predict(X_live)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Bonus: Typical hyperparameters for algorithms listed in this notebook\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We again reinforce that It will take time and experience to learn which hyperparameters to consider when optimising your pipeline and which values would make sense to tune.\n",
    "* the library documentation is your best friend instructing you on the library's available hyperparameters for that given algorithm.\n",
    "The hyperparameters we list here are a suggestion so that you can use them as a reference when you start fine-tuning your ML pipelines.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We will write the hyperparameters for the algorithms using the same dictionary structure we saw over the notebook, assuming you are arranging everything into a pipeline and the last step is called '`model`'\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Support Vector Machine\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Support Vector Machine (or SVM) is an algorithm that can be used for Classification or Regression\n",
    "* The idea is to find a hyperplane that separates the data.\n",
    "  * A hyperplane is a boundary that distinguishes the data points and will be N-1 dimensional, for example, if you have two variables (2 dimensions), you can plot these variables in an XY plot, like a 2D scatter plot. Your hyperplane in this case is a line. If you have 3 variables  (3 dimensions), you can plot these variables in an XYZ plot, like a 3D scatter plot. Your hyperplane in this case is a [plane](https://en.wikipedia.org/wiki/Plane_(geometry)) (note: it is a geometry plane, not an aeroplane)\n",
    "  * The hyperplane should have the maximum distance (here called the margin) between data points. Support vectors (therefore the algorithm name) are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_search = {\n",
    "    \"SVC\": {#'model__C':[1,0.5,1.5],\n",
    "          'model__tol':[1e-3,1e-2,1e-4],\n",
    "          #  'model__kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "            }\n",
    "}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Linear Support Vector Machine\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to its documentation, Linear Support Vector Machine is similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "params_search = {\n",
    "\n",
    "    \"LinearSVC\": {#'model__C':[1,0.5,1.5],\n",
    "                  'model__tol':[1e-3,1e-2,1e-4],\n",
    "                  },\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Linear classifier with SGD (Stochastic Gradient Descent)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit-learn documentation, this estimator implements regularized linear models (SVM, logistic regression, etc.) with stochastic gradient descent (SGD) learning.\n",
    "* SGD is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. SGD is merely an optimisation technique and does not correspond to a specific family of machine learning models. It is only a way to train a model.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"SGDClassifier\": {'model__tol':[1e-3, 1e-2, 1e-4],\n",
    "                    #  'model__penalty':['l2', 'l1', 'elasticnet'],\n",
    "                     # 'model__alpha':[0.0001,0.001],\n",
    "                      },\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Naive Bayes\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit-learn [documentation](https://scikit-learn.org/stable/modules/naive_bayes.html), Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "* Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "params_search = {\n",
    "    \"MultinomialNB\":{'model__alpha': [1.0, 0.6, 0.4, 1.3, 0.0]\n",
    "                     },\n",
    "}\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
