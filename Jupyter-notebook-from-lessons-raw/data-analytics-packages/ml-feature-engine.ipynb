{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 01 - Feature Engine Transformers and Pipeline\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%201%20-%20Lesson%20Learning%20Outcome.png\"> Lesson Learning Outcome\n",
    "\n",
    "* **Feature Engine Lesson consists of nine units.**\n",
    "* By the end of this lesson, you should be able to:\n",
    "  * Learn and use multiple estimators for handling missing data,  encode categorical variables, transform numerical variables, split continuous variables into independent, separate variables, handle outliers and detect correlated features.\n",
    "  * Create your own estimator and assemble it in your ML pipeline.\n",
    "  * Use a custom function for data cleaning and feature engineering workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use feature-engine Transformers and Pipelines.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* Feature-engine is a Python library with multiple built-in transformers which are used to engineer a dataset’s variables. The transformers learn parameters from the data and then transform the data.   \n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png\n",
    "\">\n",
    " **Why do we study feature-engine?**\n",
    "  * Because you can find in a centralised library a wide range of built-in transformers for engineering your variables. \n",
    "  * In addition, the transformers are compatible with the Scikit-learn pipeline. They take in a Pandas DataFrame and return a Pandas DataFrame, which is handy when your project is either in the research or production phase. \n",
    "\n",
    "\n",
    "---\n",
    "* Are there other Python libraries with built-in transformers?\n",
    "  * Yes, other Python libraries, like Scikit-learn, contain built-in transformers. We are using feature-engine due to the reasons stated above. \n",
    "  * We encourage you, over your data practitioner career, to explore and research at a later time additional libraries for these tasks.\n",
    "\n",
    "\n",
    "  ---  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%203%20-%20Additional%20Learning%20Context.png\"> Additional Learning Context\n",
    "\n",
    "* We encourage you to:\n",
    "  * Add **code cells to try out** other possibilities, ie.: play around with parameter values in a function/method, or consider additional function parameters etc.\n",
    "  * Also, **add your comments** in the cells. It can help you to consolidate your learning. \n",
    "\n",
    "* Parameters in a given function/method\n",
    "  * As you may expect, a given function in a package may contain multiple parameters. \n",
    "  * Some are mandatory to declare; some have pre-defined values, and some are optional. We will cover the most common parameters used/employed in Data Science for a particular function/method. \n",
    "  * However, you may seek additional information in the respective package documentation, where you will find instructions on how to use a given function/method. The studied packages are open source, so this documentation is public.\n",
    "  * **For feature-engine, the link is [here](https://feature-engine.readthedocs.io/en/1.1.x/)**.\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "We need to install `feature-engine`\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Engine Transformers and Pipeline\n",
    "\n",
    "\n",
    "Feature-engine has multiple built-in transformers which are used to engineer a dataset’s variables. \n",
    "* The transformers learn parameters from the data and then transform the data.\n",
    "* The transformers become particularly useful when assigned to an ML pipeline. But what is an ML pipeline?\n",
    "\n",
    "An ML pipeline is a sequence of tasks that are performed when training a machine learning model. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Distinct transformers are arranged in series into an ML pipeline, which helps to streamline the ML pipeline implementation\n",
    "\n",
    "We will use a Scikit-learn capability to create a pipeline. \n",
    "* Even though Scikit-learn is in an upcoming lesson, we will use its Pipeline class in this lesson to better understand feature-engine capabilities. \n",
    "\n",
    "\n",
    "We will import Pipeline from sklearn, the function documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "The idea is to define a set of steps that will be executed in a pipeline.\n",
    "* You will pass in a list of steps. Each step is defined in a tuple format, containing the step name and the functionality itself. Let's create a fictitious and non-runnable pipeline with 3 steps.\n",
    "* It is important to define a step name since you will be interested later in assessing a specific step to do a particular task (like checking what the transformer has learned, assessing feature importance from an ML model, etc.)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Warning: The cell below won't work! the idea is to show how steps are set and arranged in a pipeline.\n",
    "\n",
    "first_pipeline = Pipeline([\n",
    "     ('first_step', FunctionThatExecutesSomething(arguments)),\n",
    "     ('imagine_a_feature_engineering_step', function_feat_eng()),\n",
    "     ('this_is_a_ML_model', ml_model(argument_a, argument_b))\n",
    "     ])\n",
    "\n",
    "A set of estimators are arranged in series when training a machine learning model. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In an ML pipeline, typically, the last step will be the ML model, and the preceding steps will prepare the data for the model.\n",
    "\n",
    "You will use a set of methods in the pipeline, so you can:\n",
    "* Learn the data parameters and afterwards transform the data.\n",
    "* Train the ML model and run predictions.\n",
    "\n",
    "\n",
    "For the moment, we just need to understand `.fit()` and .`transform()`\n",
    "* The first learns from the data. The second transforms the data. We will see in later sections how to do this. \n",
    "* In addition, in a later Scikit lesson, we will cover this topic in more detail.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> \n",
    "In this lesson, you will see a combination of using feature-engine transformers \"alone\" and assembled in a pipeline, so you can get used to different coding situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-engine - Unit 02 - Handle Missing Data\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn how to handle Missing Data on numerical and categorical data.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Missing Data\n",
    "\n",
    "Feature-engine imputes missing data with values learned from the data or arbitrary values set by the user, either for numerical or categorical variables. We will study:\n",
    "* Mean Median Imputer\n",
    "* Arbitrary Number Imputer\n",
    "* Categorical Imputer\n",
    "* Drop Missing Data\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Mean Median Imputer\n",
    "\n",
    "It replaces missing data with the mean or median value of the variable. It works only with numerical variables. The documentation link is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/MeanMedianImputer.html)\n",
    "\n",
    "\n",
    "* Parse a list of variables to be imputed. Alternatively, this imputer can automatically select all variables of type numeric.\n",
    "* The imputer first calculates the mean/median values of the variables (with the fit method). Then replaces the missing data with the estimated value (with the transform method).\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "How do you know if you should impute the mean or the median?\n",
    "* You should assess the numerical distribution plot. If it is normally distributed (bell curve shape), you can replace missing values using ``mean``. Otherwise, replace using ``median``.\n",
    "\n",
    "Let's load the 'penguins' dataset.\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.head()\n",
    "\n",
    "Let's check missing data levels with `.isnull().sum()`\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "We assess the distribution. We will replace the values with the median.\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "for col in ['bill_length_mm' , 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "  sns.histplot(data=df, x=col, kde=True)\n",
    "  plt.show()\n",
    "  print('\\n')\n",
    "\n",
    "We load and set the transformer. The arguments are:\n",
    "* imputation_method: either mean or median\n",
    "* variables: list of numerical variables to apply the method to. If you don't pass in anything here, the transformer will consider all numerical variables.\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "imputer = MeanMedianImputer(imputation_method='median',\n",
    "                            variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                       'flipper_length_mm', 'body_mass_g'])\n",
    "\n",
    "We use the `.fit()` method, so the transformer can learn the median values from the selected variables. The argument is the dataset you are interested to learn from\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "As a confirmation step, let's check the learned values with the attribute `.imputer_dict_`\n",
    "\n",
    "imputer.imputer_dict_\n",
    "\n",
    "We now transform the data, which means we replace the missing data of each variable according to its respective learned median value. We use `.transform()` method. The argument is the dataset you want to transform.\n",
    "\n",
    "df = imputer.transform(df)\n",
    "\n",
    "Check the output, it is a DataFrame.\n",
    "* At first, we may think this is a minor detail; however other libraries for feature engineering, like scikit-learn, return, as an array, a ``.transform()`` command when doing a data transformation.\n",
    "\n",
    "print(type(df))\n",
    "df.head()\n",
    "\n",
    "Let's check missing levels on `['bill_length_mm' , 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']`. They were replaced with median values\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's use an example where we arrange a transformer in a pipeline. We will use this approach from now on. \n",
    "* First, we reload the dataset with missing data\n",
    "\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline in one step. We name it 'median'. Then we use the `MeanMedianImputer()` and the arguments we saw earlier.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                                'flipper_length_mm', 'body_mass_g']) )\n",
    "])\n",
    "pipeline\n",
    "\n",
    "We fit the pipeline. That means we will execute all the tasks in the pipeline.\n",
    "* In this example, the pipeline has one step that learns the median value from the selected variables.\n",
    "\n",
    "pipeline.fit(df)\n",
    "\n",
    "We then transform the dataset.\n",
    "\n",
    "df = pipeline.transform(df)\n",
    "\n",
    "And check for missing data.\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "If we want to check the learned values from the median imputed, we have to assess the step. Using bracket notation, we write the step name\n",
    "\n",
    "pipeline['median']\n",
    "\n",
    "We then use the respective attribute from the transformer, in this case, `.imputer_dict_`\n",
    "\n",
    "pipeline['median'].imputer_dict_\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Arbitrary Number\n",
    "\n",
    "It replaces missing data in numerical variables with an arbitrary number determined by the user. The function documentation is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/ArbitraryNumberImputer.html)\n",
    "* The arguments are the variables and the number to be imputed\n",
    "\n",
    "\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "Let's use the 'penguins' dataset and check missing data levels.\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline. Imagine you conducted the same data analysis as before and decided (with no basis) you want to impute`-100` where `bill_length_mm`, and `-500` for the remaining numerical variables with missing data.\n",
    "* The values we chose here are arbitrary. In a project, this imputation can relate to a particular business context. For example, imagine if the variable is Age and you have the long-term experience that if a row is missing for this variable, you should replace it with, say, 25.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'bill_length_mm',  ArbitraryNumberImputer(arbitrary_number=-100,\n",
    "                                                  variables=['bill_length_mm']) ),\n",
    "\n",
    "      ( 'other_variables',  ArbitraryNumberImputer(arbitrary_number=-500,\n",
    "                                                   variables=['bill_depth_mm',\n",
    "                                                              'flipper_length_mm',\n",
    "                                                              'body_mass_g']) )\n",
    "\n",
    "])\n",
    "pipeline\n",
    "\n",
    "We fit the pipeline with the df.\n",
    "\n",
    "pipeline.fit(df)\n",
    "\n",
    "We then transform the dataset.\n",
    "\n",
    "df = pipeline.transform(df)\n",
    "\n",
    "And check for missing data.\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "If we want to check the learned values from the arbitrary imputation, we have to assess the step. Using bracket notation, we write the step name. We first check `bill_length_mm`\n",
    "\n",
    "pipeline['bill_length_mm'].imputer_dict_\n",
    "\n",
    "Then for the remaining variables.\n",
    "\n",
    "pipeline['other_variables'].imputer_dict_\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Categorical Imputer\n",
    "\n",
    "It replaces missing data in categorical variables by an arbitrary value (typically with the label 'missing') or by the most frequent category. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/CategoricalImputer.html)\n",
    "* How do we select between the most frequent category or arbitrary value imputation?\n",
    "  * It will depend on your business context and the missing levels. If you believe there is a hidden pattern that your data is missing, in this categorical variable, you can replace it with 'missing' and may expect an algorithm will find and use that for predictions.\n",
    "  * Or maybe if the missing levels are so low, you can, in theory, replace them with the most frequent level without jeopardising the analysis.\n",
    "\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "##### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Replace with 'Missing'\n",
    "\n",
    "Let's use the 'penguins' dataset and check missing data levels.\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df = df.sample(n=50, random_state=1)\n",
    "df.isnull().sum()\n",
    "\n",
    "Let's assess `sex` frequency with .value_counts()\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "We will first replace it with a 'missing' label. We set the transformer in a pipeline by defining its name and CategoricalImputer as the function. The parameters are the imputation method, the value to be filled and the variables.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='missing',\n",
    "                                                  fill_value='Missing',\n",
    "                                                  variables=['sex']) )\n",
    "])\n",
    "pipeline\n",
    "\n",
    "For learning purposes, we can use `.fit_transform()`, so we can speed up the process of fitting and transforming the data. We assign the result to df\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again `sex` distribution with `.value_counts()`. Now ``missing`` is a label in this variable.\n",
    "\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "##### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Replace with the most frequent\n",
    "\n",
    "We will reload the 'penguins' dataset and use the other method: impute with the most frequent.\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "\n",
    "We set the pipeline and `.fit_transform()`\n",
    "* CategoricalImputer now has the imputation method as frequent.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='frequent',\n",
    "                                                  variables=['sex']) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again `sex` distribution with `.value_counts()`.\n",
    "* You may remember, at first, Male had 168 rows. Now it has increased after this transformation\n",
    "\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Drop Missing Data\n",
    "\n",
    "It deletes rows with missing values, similar to `pd.drop_na()`. It can handle numerical and categorical variables.\n",
    "* the arguments are the list of variables for which missing values should be removed. When you don't set the variables list explicitly, the transformer will drop all missing data rows. The documentation link is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/DropMissingData.html).\n",
    "* In theory, you should consider as a last resort the option to drop missing data since there was an effort and cost to collect the data. However, if you see the imputing methods will not serve you, and your missing data levels are low, you, in theory, can remove the missing data without jeopardising the analysis.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "As usual, let's consider the 'penguins' dataset and check missing data levels. We notice the dataset has 344 rows. The missing data level looks to be insignificant. The majority of missing levels is 2, and there is one with 11.\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "print(f\"{df.shape} \\n\")\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline with this transformer - we don't pass in any variables since we are interested in dropping all missing data. Then we `.fit_transform()` the data.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again how many rows the data has and the missing levels.\n",
    "* We notice now the data has 333 rows, before was 344 rows\n",
    "\n",
    "print(f\"{df.shape} \\n\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Engine - Unit 03 - Handle Categorical Variable Encoding\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn how to Handle Categorical Variable Encoding, using One Hot Encoder, Ordinal Encoder and Rare Label Encoder\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Categorical Variable Encoding\n",
    "\n",
    "A categorical encoder replaces variable labels with a calculated or arbitrary number. We will study:\n",
    "* One Hot Encoder\n",
    "* Ordinal Encoder\n",
    "* Rare Label Encoder\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  One Hot Encoder\n",
    "\n",
    "This technique replaces the categorical variable with a combination of binary variables (which takes value 0 or 1) where each new binary variable is related to a label from the categorical variable. The function is called `OneHotEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/OneHotEncoder.html)\n",
    "* For example, imagine if our variable is `Colour`, and has three labels: Yellow, Blue and Green\n",
    "* When you One Hot Encode (OHE) `Colour`, it is replaced by three binary variables `Colour_Yellow`, `Colour_Blue` and `Colour_Green`\n",
    "* Imagine if a given row of Colour is Yellow. Once One Hot Encoded, this row will be transformed to  Colour_Yellow = 1, Colour_Blue = 0 and Colour_Green = 0.\n",
    "* There is a concept called a redundant feature. Stop for a moment: do I need three binary variables to represent the variable `Colour`? \n",
    "  * The answer is no. If you have two binary variables for Colour, say Colour_Yellow and Colour_Blue, you can represent all possibilities as: \n",
    "    * Colour_Yellow = 1 and Colour_Blue = 0, meaning yellow\n",
    "    * Colour_Yellow = 0  and Colour_Blue = 1, meaning blue\n",
    "    * Colour_Yellow = 0  and Colour_Blue = 0, meaning green\n",
    "\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "Let's consider only categorical variables from the 'penguins' dataset.\n",
    "\n",
    "df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Let's create the pipeline with two steps (Handle Missing data and categorical encoding), and then use `.fit_transform()`\n",
    "* Note: we can't encode a categorical variable that has missing data. For the exercise, we dropped the missing data using the transformer from the previous unit (DropMissingData).\n",
    "* Using OneHotEncoder we pass a list of variables that we are interested to OHE.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ('drop_na', DropMissingData() ),\n",
    "      ('ohe', OneHotEncoder(variables=['species', 'island', 'sex']) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "But what about the redundant feature?\n",
    "* You just have to pass the argument `drop_last=True` to `OneHotEncoder()`\n",
    "* But first, we reload the dataset.\n",
    "\n",
    "df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Then set the same pipeline, but now add `drop_last=True`. Compare to the previous transformation and check which binary variables were removed.\n",
    "* Note there are only two binary variables related to species and island. There is only one binary variable pertaining to sex. This same set of variables carries the same amount of information as the previous OHE transformation.\n",
    "* You probably noticed that this transformation has the potential to generate a lot of new columns. That increases the feature space and may increase the chance of overfitting your model. To manage that, you may use, when possible, a FeatureSelection() step in your pipeline to select the most relevant features in your dataset. Don't worry. This topic will be covered in the next lesson.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('ohe', OneHotEncoder(variables=['species', 'island', 'sex'], drop_last=True) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Ordinal Encoder\n",
    "\n",
    "It replaces categories with ordinal numbers, like 0, 1, 2, 3 etc.  \n",
    "* The numbers can be on a first seen-first basis.\n",
    "* You can pass in a list of variables to encode, otherwise it will encode all categorical variables.\n",
    "\n",
    "The function is `OrdinalEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/OrdinalEncoder.html)\n",
    "\n",
    "The encoding method can be set to `ordered` or `arbitrary`.\n",
    "When set to `ordered`, the categories are numbered in ascending order, based on the target mean value per category. When set to `arbitrary`, the categories are numbered arbitrarily. \n",
    "Throughout the course, when we use this transformer, we will set it as `arbitrary`. In fact, \"arbitrary\" is the method argument used in a similar transformer from scikit-learn, which will be covered in an upcoming lesson. There are multiple packages that can engineer your variables, including both scikit-learn and feature-engine.\n",
    "When using `ordered`, remember your ML task must contain a target (like regression or classification). If it is a cluster, for example, the transformer will not work.\n",
    " For the teaching examples in this course, when we need to set the encoding method for this transformer, we will set it to `arbitrary`. However, you may try different options in your personal project or the workplace. After all, this transformation is part of your feature engineering strategy and as we studied, there is no fixed recipe when engineering your variables. It is a trial-and-error approach. \n",
    "\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "Let's consider categorical variables from the 'penguins' dataset.\n",
    "\n",
    "df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Let's create the pipeline with two steps (Handle Missing data and ordinal encoding) and then use `.fit_transform()`\n",
    "* We will not pass any variable list argument to `OrdinalEncoder()`, which means we will encode all variables. We set `encoding_method='arbitrary'`\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary') )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "Let's check the frequencies and labels names.\n",
    "* We use a for loop on DataFrame columns and print the variable name + the value counts for that variable.\n",
    "* Note the labels were replaced by numbers. For example, Male and Female were replaced by 0 and 1.\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts()} \\n\\n\")\n",
    "\n",
    "Let's check the encoder dictionary, to see how the transformer mapped the labels to numbers.\n",
    "\n",
    "pipeline['ordinal_encoder'].encoder_dict_\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Rare Label Encoder\n",
    "\n",
    "This encoder groups infrequent categories in a new category called 'Rare' (or other defined name)\n",
    "* For example, if your variable is Fruit, and the  percentage of rows for the labels banana, grape and apple is less than < 6 %, all these labels will be replaced by 'Rare'. That helps to decrease the chance of a model overfitting.\n",
    "* The function is `RareLabelEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/RareLabelEncoder.html). The arguments are:\n",
    "  * `tol`, which is the tolerance, or the minimum frequency a label should have to be considered frequent. Categories with frequencies lower than tol will be replaced as 'Rare'.\n",
    "  * `n_categories`: The minimum number of categories a variable should have for the encoder to find frequent labels. If the variable contains fewer categories, all of them will be considered frequent.\n",
    "  * `variables`: list of variables that you would like to apply this transformation on. If you don't parse anything, it will select all categorical variables.\n",
    "\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "Let's consider a few variables from the Titanic dataset. It holds passengers' records from the first and indeed last Titanic voyage.  \n",
    "* Note we are converting the variables to 'object' with `.astype()` since some of them were listed as numerical yet being represented as a ``category``.\n",
    "\n",
    "df = sns.load_dataset('titanic').filter(['parch', 'sibsp']).astype('object')\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "Let's assess missing levels.\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "Now let's check the label's frequencies for each variable\n",
    "* We loop on each variable and count its labels frequencies using .value_counts(normalize=True)\n",
    "* We note that there are some labels which are infrequent, like 6 for parch.\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "Let's create the pipeline with two steps (rare label encoding), and then use `.fit_transform()`. We show here the use case where we can perform multiple rare label encoding.\n",
    "* The first RareLabelEncoder deals with parch and sets the tolerance to 10% (this is a random number and is used to explain the concept). In the end, any parch label that is less frequent than 10%, will be replaced by 'Rare'.\n",
    "* The second RareLabelEncoder deals with sibsp and sets the tolerance to 8% (again, a random number to illustrate the concept). In the end, any sibsp label that is less frequent than 8%, will be replaced by 'Rare'.\n",
    "* Note: you can perform this technique with a set of variables. We created the example with single variables with different tolerance to illustrate the concept. In the workplace, the tol level will be selected based on the business context.\n",
    "* We set ``n_categories=2`` since we want to encode all possible labels.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('rle_parch', RareLabelEncoder(tol=0.1,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['parch']) ), \n",
    "      ('rle_sibsp', RareLabelEncoder(tol=0.08,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['sibsp']) )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df.head()\n",
    "\n",
    "Now let's check the label's frequencies for each variable again\n",
    "* Note the labels were grouped into a label called 'Rare' according to the rules defined in the pipeline.\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "But, you may think, my variable is still a category, what should I do?\n",
    "* The answer is, to arrange an Ordinal Encoder or OHE after the rare label encoder, so your categorical variables can be properly encoded.\n",
    "* Just as an example, let's reload the data and inspect labels frequencies.\n",
    "\n",
    "df = sns.load_dataset('titanic').filter(['parch', 'sibsp']).astype('object')\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "In one cell, we will do the following tasks:\n",
    "* Create a pipeline with four steps: drop missing data, two rare label encoders and an ordinal encoder.\n",
    "* Then we fit and transform the data.\n",
    "* Finally, we loop over the variables to check labels frequencies.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('rle_parch', RareLabelEncoder(tol=0.1,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['parch']) ), \n",
    "      ('rle_sibsp', RareLabelEncoder(tol=0.08,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['sibsp']) ),\n",
    "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary',\n",
    "                                         variables= ['parch', 'sibsp']) )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Engine - Unit 04 - Handle Numerical Variable Transformation\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Handle Numerical Variable Transformation, using Log Transformer, Reciprocal Transformer, Power Transformer, Box Cox and Yeo Johnson Transformer\n",
    "\n",
    "\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Numerical Variable Transformation\n",
    "\n",
    "The techniques presented here transform numerical variables considering multiple mathematical transformations. The idea is to transform the variable distribution, ideally to become close to a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). We will study the following transformers:\n",
    "* LogTransformer\n",
    "* ReciprocalTransformer\n",
    "* PowerTransformer\n",
    "* BoxCoxTransformer\n",
    "* YeoJohnsonTransformer\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We will do exercises with all of the transformers. You don't have to memorise the specific mathematical function for each transformer. Instead, you should be aware that we apply mathematical functions to numerical data. Later on, we will show a custom function that displays a report on numerical transformations, giving you criteria to select the most suitable transformer for your data.\n",
    "\n",
    "* We will use the pingouin package to run a Q-Q plot to visually check how close to the normal distribution a given variable is.\n",
    "\n",
    "\n",
    "import pingouin as pg\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Log Transformer\n",
    "\n",
    "It applies the [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm) (base e) or the base 10 logarithm to numerical variables. The function documentation is [here](https://feature-engine.readthedocs.io/en/1.1.x/transformation/LogTransformer.html)\n",
    "* The transformer, as we may expect, can't handle zero or negative values\n",
    "* The arguments are the `variables` you want to apply the method to. In cases where you don't pass in a list of variables, the transformer considers all numerical variables. The next argument is `base` (either 'e' or '10').\n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "We will consider the Boston dataset from [scikit learn datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html). It shows house prices in Boston.\n",
    "* We used four lines of code to unpack the dataset into a format to reach a DataFrame with all the features and the target variable. In the next lesson, we will investigate how to use sklearn functionalities. For now, we just need its dataset.\n",
    "* For this exercise, we are not interested in making sense of the variable's meaning and business impact. We're looking for numerical variables for handling transformation. we will consider only a subset of the variables.\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df['target'] = pd.Series(boston_data.target)\n",
    "\n",
    "df = df.filter(['DIS','LSTAT', 'target'])\n",
    "df.head()\n",
    "\n",
    "We assess the histogram and QQ plot by looping over the variables. We create custom functions for this task since we will repeat it across different transformers. The first will calculate skewness and kurtosis. The second plots a histogram and QQ plot for a given numerical variable.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A quick recap\n",
    "* Skewness is the asymmetry of the data. A distribution is symmetric when it looks the same to the left and right of the centre point. It is horizontally mirrored. Positive Skewness happens when the tail on the right side is longer. Negative skewness is the opposite.\n",
    "* Kurtosis relates to the tails of the distribution. It is a measure of outliers in the distribution. A negative kurtosis indicates the distribution has thin tails. Positive kurtosis indicates that the distribution is peaked and has thick tails.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_skew_kurtosis(df,col, moment):\n",
    "  print(f\"{moment}  | skewness: {df[col].skew().round(2)} | kurtosis: {df[col].kurtosis().round(2)}\")\n",
    "\n",
    "\n",
    "def distribution_before_applying_transformer(df):\n",
    "  for col in df.columns:\n",
    "    print(f\"*** {col} ***\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(\"Histogram\")\n",
    "    pg.qqplot(df[col], dist='norm',ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    calculate_skew_kurtosis(df,col,'before apply transformation')\n",
    "    print(\"\\n\")\n",
    "\n",
    "distribution_before_applying_transformer(df)\n",
    "\n",
    "We set the pipeline with this transformer: `vt.LogTransformer()`. Then we `.fit_transform()` the pipeline, assigning the result to `df_transformed`\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'log', vt.LogTransformer() )\n",
    "  ])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We now compare the distribution of the variables before and after applying the transformer. We create a custom function for that. It plots the histogram and QQplot for the same variable before and after applying the transformer.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note: When transforming variables, the summary statistics may change. We will consider here only skewness and kurtosis. What is important is to reach a gain where the transformed variable is closer to a normal distribution.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots\n",
    "* `DIS` decreased skewness, but its kurtosis increased and changed from positive to negative. The QQ plot is closer to the diagonal line after transformation, but it is still \"bent\".\n",
    "* `LSTAT` decreased skewness and changed from positive to negative. Its kurtosis decreased and changed from positive to negative. The QQ plot is closer to the diagonal line after transformation.\n",
    "* `target` decreased skewness and kurtosis. Skewness changed from positive to negative, but it is still \"bent\".\n",
    "\n",
    "We can say that in general this transformation, helped to transform these variables to become closer to a normal distribution when we compare the distribution shape and QQ plot before and after applying the transformer.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> However, we have more mathematical functions at our disposal to test. \n",
    "* That leads to another question: Which mathematical function should I apply to my variable? We prepared a custom function that will use all possible transformations in a given variable, so you can have a report to decide which transformer to apply.\n",
    "\n",
    "\n",
    "def compare_distributions_before_and_after_applying_transformer(df, df_transformed, method):\n",
    "\n",
    "  for col in df.columns:\n",
    "    print(f\"*** {col} ***\")\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\n",
    "\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Before {method}')\n",
    "    pg.qqplot(df[col], dist='norm',ax=axes[0,1])\n",
    "    \n",
    "    sns.histplot(data=df_transformed, x=col, kde=True, ax=axes[1,0])\n",
    "    axes[1,0].set_title(f'After {method}')\n",
    "    pg.qqplot(df_transformed[col], dist='norm',ax=axes[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    calculate_skew_kurtosis(df,col, moment='before transformation')\n",
    "    calculate_skew_kurtosis(df_transformed,col, moment='after transformation')\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "compare_distributions_before_and_after_applying_transformer(df, df_transformed, method='Log Transformer')\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Reciprocal Transformer\n",
    "\n",
    "This technique applies the reciprocal transformation 1 / x to numerical variables. As we may expect, it can't handle a variable that contains zero. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/transformation/ReciprocalTransformer.html)\n",
    "* The argument is `variables`. In cases where you don't pass in a list of variables, the transformer considers all numerical variables.\n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "We consider the Boston dataset with the same variables from the previous exercise.\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df['target'] = pd.Series(boston_data.target)\n",
    "\n",
    "df = df.filter(['DIS','LSTAT', 'target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution using the previous custom function.\n",
    "\n",
    "distribution_before_applying_transformer(df)\n",
    "\n",
    "We set the pipeline with this transformer: `vt.ReciprocalTransformer()`. Then we `.fit_transform()` the pipeline, assigning the result to `df_transformed`\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'reciprocal', vt.ReciprocalTransformer() )\n",
    "  ])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We compare the histograms and QQ plots before and after applying the transformers.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots.\n",
    "* ``DIS`` decreased skewness, and kurtosis changed from positive to negative. The QQ plots look similar. It doesn't look to have made any progress after transforming.\n",
    "* `LSTAT` increases both skewness and kurtosis. It doesn't look to have made any progress after transforming.\n",
    "* `target` increases both skewness and kurtosis. It doesn't look to have made any progress after transforming.\n",
    "\n",
    "\n",
    "compare_distributions_before_and_after_applying_transformer(df, df_transformed, method='ReciprocalTransformer')\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Power Transformer\n",
    "\n",
    "It applies power or exponential transformations to the numerical variable. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/transformation/PowerTransformer.html)\n",
    "* The arguments are the `variables` you want to apply the method to. In cases where you don't pass in a list of variables, the transformer considers all numerical variables. `exp` is the power of the exponent, and the default is 0.5\n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "We consider the Boston dataset with the same variables from the previous exercise.\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df['target'] = pd.Series(boston_data.target)\n",
    "\n",
    "df = df.filter(['DIS','LSTAT', 'target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution using the previous custom function.\n",
    "\n",
    "distribution_before_applying_transformer(df)\n",
    "\n",
    "We set the pipeline with this transformer: `vt.PowerTransformer()`. Then we `.fit_transform()` the pipeline, assigning the result to `df_transformed`\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('pt', vt.PowerTransformer() )\n",
    "  ])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We compare the histograms and QQ plots before and after applying the transformers.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots.\n",
    "* `DIS` decreased skewness, kurtosis changed from positive to negative. The QQ plots look similar. It doesn't look to have made any progress after transforming.\n",
    "* `LSTAT` decreased skewness and kurtosis changed from positive to negative. It looks to have improved when you look at the QQ plot.\n",
    "* `target` decreased skewness and kurtosis. It looks to have made minor progress when comparing the QQ plot before and after the transformation since the blue dots are close to the diagonal line.\n",
    "\n",
    "\n",
    "compare_distributions_before_and_after_applying_transformer(df, df_transformed, method='PowerTransformer')\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Box Cox Transformer\n",
    "\n",
    "It applies the BoxCox transformation to numerical variables. A mathematical formulation can be found [here](https://www.statisticshowto.com/box-cox-transformation/). The data must be positive for the transformer. The documentation for the function is found [here](https://feature-engine.readthedocs.io/en/1.1.x/transformation/BoxCoxTransformer.html)\n",
    "* The argument is `variables`. In cases where you don't pass in a list of variables, the transformer considers all numerical variables. \n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "We consider the Boston dataset with the same variables from the previous exercise.\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df['target'] = pd.Series(boston_data.target)\n",
    "\n",
    "df = df.filter(['DIS','LSTAT', 'target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution using the previous custom function.\n",
    "\n",
    "distribution_before_applying_transformer(df)\n",
    "\n",
    "We set the pipeline with this transformer: `vt.BoxCoxTransformer()`. Then we `.fit_transform()` the pipeline, assigning the result to `df_transformed`\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('bct', vt.BoxCoxTransformer() )\n",
    "  ])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We compare the histograms and QQ plots before and after applying the transformers.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots.\n",
    "* `DIS` decreased skewness, kurtosis changed from positive to negative. The QQ plots look similar. It doesn't look to have made any progress after transforming.\n",
    "* `LSTAT` decreased skewness and kurtosis changed from positive to negative. It looks to have made an improvement when you look at the QQ plot.\n",
    "* `target` decreased skewness and kurtosis. It looks to have made minor progress when comparing the QQ plot before and after the transformation since the blue dots are close to the diagonal line.\n",
    "\n",
    "\n",
    "compare_distributions_before_and_after_applying_transformer(df, df_transformed, method='BoxCoxTransformer')\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Yeo Johnson Transformer\n",
    "\n",
    "It applies the Yeo-Johnson transformation, more information on the mathematical formulation can be found [here](https://statisticaloddsandends.wordpress.com/2021/02/19/the-box-cox-and-yeo-johnson-transformations-for-continuous-variables/). The documentation for the function is found [here](https://feature-engine.readthedocs.io/en/1.1.x/transformation/YeoJohnsonTransformer.html).\n",
    "* The argument is a list of `variables`. In cases where you don't pass in a list of variables, the transformer considers all numerical variables. \n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "We consider the Boston dataset with the same variables from the previous exercise.\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df['target'] = pd.Series(boston_data.target)\n",
    "\n",
    "df = df.filter(['DIS','LSTAT', 'target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution using the previous custom function.\n",
    "\n",
    "distribution_before_applying_transformer(df)\n",
    "\n",
    "We set the pipeline with this transformer: `vt.YeoJohnsonTransformer()`. Then we `.fit_transform()` the pipeline, assigning the result to `df_transformed`\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('yj', vt.YeoJohnsonTransformer() )\n",
    "  ])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We compare the histograms and QQ plots before and after applying the transformers.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots.\n",
    "* `DIS` decreased skewness, and kurtosis increased and changed from positive to negative. The QQ plots look to have made progress since the blue dots are closer to the diagonal line.\n",
    "* `LSTAT`: same as above.\n",
    "* `target` decreased skewness and kurtosis. The QQ plot after the transformation looks better than before the transformation.\n",
    "\n",
    "\n",
    "compare_distributions_before_and_after_applying_transformer(df, df_transformed, method='YeoJohnsonTransformer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 05 - Handle Variable Discretization\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Handle Variable Discretization using Equal Frequency discretizer, Equal Width discretizer or Arbitrary discretizer\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Variable Discretization\n",
    "\n",
    "This technique consists of transforming continuous numerical variables into discrete variables. The discrete variables will contain intervals related to the numerical distribution. The interval will be decided based on the frequency or width. We will study:\n",
    "* EqualFrequencyDiscretiser\n",
    "* EqualWidthDiscretiser\n",
    "* ArbitraryDiscretiser\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> When should I consider using them? We can consider these use cases:\n",
    "* Eventually, your feature has an abnormal or weird numerical distribution, and by discretizing this variable, the categorical distribution is better understood by the model\n",
    "* You have a continuous target variable, and you are not successful in fitting a model to the dataset. Then, you can discretize the target variable and convert the ML task to classification since your target variable is now categorical. The expectation is that we will create more conditions to find a model that fits the data.\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Equal Frequency\n",
    "\n",
    "It divides continuous numerical variables into contiguous equal frequency intervals, intervals containing approximately the same proportion of observations. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/EqualFrequencyDiscretiser.html)\n",
    "* The arguments are `variables` to apply the method; if you don't parse anything, it will select all numerical variables. And `q` (for quantiles), which is the desired number of equal frequency intervals (or quantiles).\n",
    "\n",
    "\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution with sns.histplot()\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `EqualFrequencyDiscretiser()` on the target variable and look for five bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number for ``q``. Eventually, it will make sense to have 3 or 6. At the same time, you can run multiple simulations and assess the results for numerous ``q``\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('efd', EqualFrequencyDiscretiser(q=5, variables=['target'] ))\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the efd step and check what were the bins the transformer calculated with `.binner_dict_`\n",
    "\n",
    "pipeline['efd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot the bar where the target is zero; it corresponds to the numerical interval of -inf to 15.3. You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The upside of using this technique, considering you are using the target variable, is that your target variable for the classification task will be already balanced, which means the labels have similar frequencies.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Equal Width\n",
    "\n",
    "This technique divides continuous numerical variables into intervals of the same width. Note that the count of observations per interval may vary. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/EqualWidthDiscretiser.html).\n",
    "* The arguments are `variables` to apply the method to; if you don't parse anything, it will select all numerical variables. And `bins` which is the number of equal-width intervals/bins you want.\n",
    "\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess its distribution\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `EqualWidthDiscretiser()` on the target variable and look for six bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number of `bins`. Eventually, it will make sense to have 3 or 6. At the same time, you can run multiple simulations and assess the results for numerous `bins`\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('ewd', EqualWidthDiscretiser(bins=6, variables=['target']) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the ewd step and check what were the bins the transformer calculated with `.binner_dict_`\n",
    "\n",
    "pipeline['ewd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot, the bar where the target is zero corresponds to the numerical interval of -inf to 12.5.  You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The downside of using this technique, considering you are using the target variable, is that your target variable for the classification task will likely not be balanced.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Arbitrary Discretiser\n",
    "\n",
    "It divides continuous range intervals, which limits are determined by the user. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/ArbitraryDiscretiser.html). The used argument is:\n",
    "* ``binning_dict`` is a dictionary that tells which variable you want to apply the method and the intervals.\n",
    "\n",
    "* You may use this technique when the company is comfortable with how to map the numerical values to ranges. For example, imagine if the variable is Revenue from a given purchase. The business is comfortable assuming that Revenue smaller than 100 is small, between 100 and 1000 is medium and greater than 1000 is big. You can also conduct separate analyses with other custom ranges to question current assumptions and/or look for other criteria to discretize the data\n",
    "\n",
    "from feature_engine.discretisation import ArbitraryDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess its distribution\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `ArbitraryDiscretiser()` on the target variable and look for six bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number of `bins`. Eventually, it will make sense to have 3 or 6. At the same time, you can run multiple simulations and assess the results for numerous `bins`\n",
    "\n",
    "\n",
    "import numpy as np # we import NumPy to set -inf and +inf\n",
    "pipeline = Pipeline([\n",
    "      ( 'arbd', ArbitraryDiscretiser(binning_dict={'target':[-np.inf,10,20,40,np.inf]}) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the arbd step and check the bins we created with `.binner_dict_`\n",
    "\n",
    "pipeline['arbd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot the bar where the target is zero corresponds to the numerical interval of -inf to 10.  You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The upside of this technique is that we set the intervals we are comfortable with. The downside is that the categorical distribution may be imbalanced.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 06 - Handle Outlier\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Handle Outlier using Winsorizer, Arbitrary capper or Outlier Trimmer\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Outlier\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> These techniques aim to cap outliers based on a calculation or an arbitrary value. In addition, you may drop the outliers from the dataset. It is important to use the business context to manage outliers. For example:\n",
    "* If your variable is Age and you see a value of 400, that may mean an error when collecting the data. You may cap the outlier with a `Q3 + 1.5 * IQR` value, replace it with an arbitrary number, or drop the row. The practical decision depends on your business context. Luckily we can code and check the effect of multiple possibilities before deciding the most suitable option to handle the outlier.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">  We should consider it as the last option to drop rows containing outliers since the data collection process requires energy, time and money from some team, either your team or another team. Also, outliers may indicate that your data is changing its behaviour and you have collected the first samples of this new behaviour.\n",
    "\n",
    "\n",
    "\n",
    "We will study the following transformers\n",
    "* Winsorizer\n",
    "* ArbitraryOutlierCapper\n",
    "* OutlierTrimmer\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Winsorizer\n",
    "\n",
    "It caps the outliers as a continuous variable's maximum and/or minimum values. It calculates the capping values using specific methods. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/outliers/Winsorizer.html#)\n",
    "* The arguments are the variables with outliers you are interested in (if you don't parse anything, it will consider all numerical variables), `tail`, where you decide to cap outliers on the right, left or both tails. Another argument is `fold`, the number that will multiply IQR to calculate the capping values. The documentation says recommended values are 1.5 or 3 for the IQR proximity rule. Another argument is `capping_method`; we will consider  `'iqr'`: 75th quantile + 1.5* IQR for the right tail and 25th quantile - 1.5* IQR for the left tail.\n",
    "\n",
    "\n",
    "\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "We will consider the titanic data for this exercise. It holds passenger records from the Titanic's final journey. We will consider the variables `age` and `fare`\n",
    "\n",
    "df = sns.load_dataset('titanic').filter(['age', 'fare'])\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "We will assess variable distribution with a custom function by plotting a combined histogram and a boxplot.  The function code was used in the Descriptive Statistics unit, so it should be familiar to you. On top of that, we added more code to inform of the limits where the boxplot considers a data point an outlier (we calculate the [IQR](https://en.wikipedia.org/wiki/Interquartile_range) and the lower (Q1 - 1.5 x IRQ) and upper limits (Q3 + 1.5 x IQR) of the boxplot\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's comment on the plots in terms of outliers:\n",
    "* `Age` has few outliers on the right side of the tail (or on the right side of the plot)\n",
    "* `Fare` has multiple outliers on the right side of the tail\n",
    "\n",
    "\n",
    "def plot_histogram_and_boxplot(df):\n",
    "  for col in df.columns:\n",
    "    fig, axes = plt.subplots(nrows=2 ,ncols=1 ,figsize=(7,7), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "    sns.boxplot(data=df, x=col, ax=axes[0])\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=axes[1])\n",
    "    fig.suptitle(f\"{col} Distribution - Boxplot and Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "    IQR = df[col].quantile(q=0.75) - df[col].quantile(q=0.25)\n",
    "    print(\n",
    "        f\"This is the range where a datapoint is not an outlier: from \"\n",
    "        f\"{(df[col].quantile(q=0.25) - 1.5*IQR).round(2)} to \"\n",
    "        f\"{(df[col].quantile(q=0.75) + 1.5*IQR).round(2)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "plot_histogram_and_boxplot(df)\n",
    "\n",
    "We create a pipeline with two steps: ``DropMissingData()`` (since there should be no missing data),  ``then Winsorizer()``, on both variables using iqr as the capping method and fold as 1.5 on both tails.\n",
    "\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ( 'winsorizer_iqr', Winsorizer(capping_method='iqr', fold=1.5, tail='both', variables=['age', 'fare']) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note the **capping points** change when you apply iqr. We assess the cap values with `.right_tail_caps` and `.left_tail_caps`. We first check the right tails cap\n",
    "\n",
    "pipeline['winsorizer_iqr'].right_tail_caps_\n",
    "\n",
    "Then left tail caps\n",
    "\n",
    "pipeline['winsorizer_iqr'].left_tail_caps_\n",
    "\n",
    "For each variable, we will check the histogram and boxplot before and after the transformation\n",
    "  * Note the ranges have changed\n",
    "  * The outliers on the right tail were trimmed on `Q3 + 1.5 * IQR`\n",
    "\n",
    "print(\"========= Before Transformation ========= \\n\")\n",
    "plot_histogram_and_boxplot(df)\n",
    "print(\"\\n\\n ========= After Transformation =========\")\n",
    "plot_histogram_and_boxplot(df=df_transformed)\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Arbitrary Outlier Capper\n",
    "\n",
    "It caps a variable's maximum or minimum values at an arbitrary value indicated by the user. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/outliers/ArbitraryOutlierCapper.html)\n",
    "* The arguments are `max_capping_dict` and `min_capping_dict`, where you parse in a dictionary of the variables and limits (min and max) you want to cap\n",
    "\n",
    "from feature_engine.outliers import ArbitraryOutlierCapper\n",
    "\n",
    "We will consider the titanic data for this exercise. It holds passengers' records from the Titanic's final journey. We will consider the variables `age` and `fare`\n",
    "\n",
    "df = sns.load_dataset('titanic').filter(['age', 'fare'])\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "We will assess the distribution of the variables with a custom function by plotting a combined histogram and a boxplot. \n",
    "\n",
    "plot_histogram_and_boxplot(df)\n",
    "\n",
    "We create a pipeline with two steps: ``DropMissingData()`` (since there should be no missing data), then ``ArbitraryOutlierCapper()``, and set 40 as the max cap for fare and 50 as the max cap for age. We use these numbers so you can clearly see the effect in the histograms. In the workplace, you should reflect on the selected number for the cap.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ( 'arb', ArbitraryOutlierCapper(max_capping_dict={'fare':40 , 'age':50}) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "After applying the gaussian method, we will still have outliers; with the iqr methods, we will not.\n",
    "* For each variable, we check the histogram and boxplot, before and after the transformation, so you can see the behaviour we described\n",
    "  * Note the ranges have changed\n",
    "  * Note after applying the transformation, all outliers values became 40 for fare and 50 for age. We note a \"peak\" in the fare histogram around 40 and a \"peak\" in the age histogram around 50 \n",
    "\n",
    "print(\"========= Before Transformation ========= \\n\")\n",
    "plot_histogram_and_boxplot(df)\n",
    "print(\"\\n\\n ========= After Transformation =========\")\n",
    "plot_histogram_and_boxplot(df=df_transformed)\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Outlier Trimmer\n",
    "\n",
    "It removes observations with outliers from the data. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/outliers/OutlierTrimmer.html). The arguments are the variables you want to apply the transformer too. If you don't parse variables, it will get all numerical data. There are also `capping_method`, `tail` and `fold`, which have the same meaning as the  Winsorizer() technique. We will consider capping_method='irq', tail='both' and fold=1.5\n",
    "\n",
    "from feature_engine.outliers import OutlierTrimmer\n",
    "\n",
    "We will consider the titanic data for this exercise. It holds passengers' records from the Titanic's final journey. We will consider the variables `age` and `fare`\n",
    "\n",
    "df = sns.load_dataset('titanic').filter(['age', 'fare'])\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "We will assess the distribution of the variables with a custom function by plotting a combined histogram and a boxplot. In addition, we will calculate how many rows the dataset has when it includes outliers\n",
    "\n",
    "print(f\"* The dataset has {len(df)} rows, considering outliers.\\n\\n\")\n",
    "plot_histogram_and_boxplot(df)\n",
    "\n",
    "We create a pipeline with two steps: `DropMissingData()` (since there should be no missing data), then `OutlierTrimmer()`, where capping_method='iqr', fold=1.5, tail='both', and variables=['age', 'fare']. We `.fit_transform()` the data\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ( 'out_trimmer', OutlierTrimmer(capping_method='iqr', fold=1.5, tail='both', variables=['age', 'fare']) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "We notice the dataset length has decreased since we removed the observations from both variables, which were considered outliers\n",
    "\n",
    "print(f\"* The dataset has {len(df)} rows, considering outliers.\")\n",
    "print(f\"* Once it is transformed with OutlierTrimmer, dataset has {len(df_transformed)} rows\")\n",
    "\n",
    "But that doesn't mean the new dataset will not have outliers. Since under the new configuration, or new distribution, the data might be distributed in a way that may contain a few outliers. The difference is that now you will have fewer outliers for your model. \n",
    "* Note the range has changed, as we may expect. The distribution shape is the same in the area where there are no outliers (as we may expect as well)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Please consider this route as a last resort after carefully reflecting on why your original data had outliers in the first place.\n",
    "\n",
    "print(\"========= Before Transformation ========= \\n\")\n",
    "plot_histogram_and_boxplot(df)\n",
    "print(\"\\n\\n ========= After Transformation =========\")\n",
    "plot_histogram_and_boxplot(df=df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 07 -  Drop Features & Smart Correlated Features\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn how to apply Drop Features transformer & Smart Correlated Features transformer\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">Drop Features\n",
    "\n",
    "It drops a list of variables indicated by the developer. The function documentation is [here](https://feature-engine.readthedocs.io/en/1.1.x/selection/DropFeatures.html). The argument is the features you want to drop.\n",
    "\n",
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "We will use the penguin dataset. It has records for three different species of penguins collected from 3 islands in the Palmer Archipelago, Antarctica\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df.head()\n",
    "\n",
    "We will set the pipeline with `DropFeatures(),` and we want to drop the variables 'sex' and 'island'. We chose these arbitrarily, just for the exercise.\n",
    "* In the workplace, you may consider the context. For example, your variable might be CustomerID, which is typically a combination of letters and numbers with high cardinality. You can often only get a little information out of it. Therefore, you may drop this variable.\n",
    "* Other use cases could be when you create variables combining others, for example, 'distance' and 'time'; you may create a variable 'speed' when dividing one by another. After that, you may discard 'distance' and 'time'\n",
    "* After setting the pipeline, we `.fit_transform()` the data\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_features', DropFeatures(features_to_drop = ['sex', 'island']) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Smart Correlated Features\n",
    "\n",
    "\n",
    "According to the documentation, this transformer finds groups of correlated features. It then selects, from each group, a feature following certain criteria: Features with the least missing values, features with the most unique values, and features with the highest variance. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/selection/SmartCorrelatedSelection.html)\n",
    "* The arguments we will use are variables, which are the list of variables to evaluate, if you don't parse anything it will consider all numerical variables in the dataset. The next is a method (like 'Pearson' or 'Spearman'), and threshold, which according to the documentation, is the correlation threshold above which a feature will be deemed correlated with another one and removed from the dataset.\n",
    "\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "We will use the tips dataset. It holds records for waiter tips based on the day of the week, time of day, total bill, gender, if it is a table of smokers or not, and how many people were at the table.\n",
    "\n",
    "df = sns.load_dataset('tips')\n",
    "df.head()\n",
    "\n",
    "When you load the dataset from Seaborn, the categorical variables data type is 'category', and for the ML tasks, and more specifically, for the exercise, it should be 'object'.\n",
    "\n",
    "df.info()\n",
    "\n",
    "We change the data type to `'object'` by looping over all the variables where its current data type is `'category'`\n",
    "\n",
    "for col in df.select_dtypes(include='category').columns:\n",
    "  df[col] = df[col].astype('object')\n",
    "\n",
    "df.info()\n",
    "\n",
    "We check for missing data. \n",
    "* There is no missing data\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "``SmartCorrelatedSelection()`` transformer works on numerical data; therefore, we must encode the existing categorical variables. We do that in this exercise with ``OrdinalEncoder()``. Then we add ``SmartCorrelatedSelection()``, where we don't pass the variables, meaning we want all numerical variables to be evaluated. We set the method as Pearson, the threshold as 0.6 and selection_method as the variance. A threshold of 0.6 means that any variable correlations that are at least moderate will be considered and subject to removal\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **A Big warning**: the tips dataset is intended to be used in a regression task where you are interested in predicting tips. When working on a project, the tips variable wouldn't be a feature but a target. Here we left it in on purpose as a feature just for the sake of the exercise.\n",
    "\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "pipeline = Pipeline([\n",
    "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary') ),\n",
    "      ( 'SmartCorrelatedSelection', SmartCorrelatedSelection(method=\"pearson\",\n",
    "                                                             threshold=0.6,\n",
    "                                                             selection_method=\"variance\",))\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We can check which sets of features were marked as correlated (using the rules we set in the previous pipeline). We do that by accessing the pipeline step and using the attribute `.correlated_feature_sets_`\n",
    "\n",
    "pipeline['SmartCorrelatedSelection'].correlated_feature_sets_\n",
    "\n",
    "We check which variables were removed with the attribute `.features_to_drop_`\n",
    "\n",
    "pipeline['SmartCorrelatedSelection'].features_to_drop_\n",
    "\n",
    "Alternatively, we inspected the df_transformed, and as we expected, the variables were removed\n",
    "\n",
    "df_transformed.head()\n",
    "\n",
    " <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **Additional warning**: This transformer is used in the features when setting your pipeline for your ML task. It is typically one of the last steps of feature engineering since it requires pre-processing the data.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 08 - Create your own transformer\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Create your own transformer that can be arranged into a pipeline\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Create your own transformer\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> What if, for your existing project, you couldn't find a built-in transformer that satisfies your project needs?\n",
    "* You can create a transformer. Your custom transformer will be a Python Class, which is a topic you are already familiar with!\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Before defining your custom transformer, all transformers in scikit-learn (and scikit-learn compatible libraries, like feature-engine) are implemented as Python classes, each with its own attributes and methods. \n",
    "* Our custom transformer (or Class) must be implemented as a class with the same methods, like fit(), transform(), fit_transform() etc. We will inherit these methods using two scikit-learn base classes: TransformerMixin and BaseEstimator. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For that, we will need two base transformers from Scikit-learn. \n",
    "* `BaseEstimator`: According to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), it is a \"base class for all estimators in scikit-learn\". We will not focus on the technical aspects, only the frame, as it contains the core of what a transformer should have.\n",
    "* `TransformerMixin`: According to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html), it is a Mixin class for all transformers in scikit-learn.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "In feature-engine (and scikit-learn), we have a transformer that replaces the missing value with the mean. But let's imagine it didn't, and we want to create `MyCustomTransformerForMeanImputation()`\n",
    "* Let's follow along with the code's comment to understand the steps\n",
    "\n",
    "import pandas as pd # to use .mean()\n",
    "\n",
    "# We will define three methods for the class: _init_, fit and transform\n",
    "# The fit_transform() will be inherited since we are using BaseEstimator and TransformerMixin\n",
    "\n",
    "# Define your transformer name, and as an argument inherit the base classes\n",
    "class MyCustomTransformerForMeanImputation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  #### Here, you define the variables you need to parse when you initialize the class\n",
    "  def __init__(self, variables):\n",
    "    # We make sure the variables will be a list, even if only one element\n",
    "    if not isinstance(variables, list): \n",
    "      self.variables = [variables]\n",
    "    else: self.variables = variables\n",
    "\n",
    "  #### Here is where the learning happens. We perform the operation we are interested in\n",
    "  #### In this case, calculate the mean\n",
    "  def fit(self, X, y=None):\n",
    "   \n",
    "    # We want to keep the mean value in a dictionary\n",
    "    self.imputer_dict_ = {}\n",
    "      \n",
    "    # loop over each variable, calculate the mean and save it in the dictionary.  \n",
    "    for feature in self.variables:\n",
    "        self.imputer_dict_[feature] = X[feature].mean()\n",
    "    \n",
    "    return self\n",
    "\n",
    "  #### Here, you transform the variables based on what you learned in the .fit()\n",
    "  #### You can transform into the train set, test set or real-time data\n",
    "  def transform(self, X):\n",
    "    # loop over the variables and .fillna() in a given feature based on the \n",
    "    # mean of a given feature\n",
    "    for feature in self.variables:\n",
    "      X[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
    "      \n",
    "    return X\n",
    "\n",
    "You may create a custom transformer where you don't need to code the ``.fit().`` For example, imagine you want to apply the upper case method to all the variables. You don't need to learn that; you just need to execute it.\n",
    "* Let's create this transformer and call `ConvertUpperCase()`\n",
    "\n",
    "# The comments relate to the new concepts for this exercise\n",
    "\n",
    "class ConvertUpperCase(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, variables):\n",
    "    if not isinstance(variables, list): \n",
    "      self.variables = [variables]\n",
    "    else: self.variables = variables\n",
    "\n",
    "  # We don't need to learn anything here; we just return self\n",
    "  # We need to do that anyway to be compatible with scikit-learn format\n",
    "  def fit(self, X, y=None):\n",
    "      return self\n",
    "\n",
    "  # Here, we convert the variables using a method called .upper()\n",
    "  # We loop over all the variables, check if it is an object, and then use a lambda function...\n",
    "  # ...to apply .upper() to all rows\n",
    "  def transform(self, X):\n",
    "    for feature in self.variables:\n",
    "      if X[feature].dtype == 'object':\n",
    "        X[feature] = X[feature].apply(lambda x: x.upper())\n",
    "      else:\n",
    "        print(f\"Warning: {feature} data type should be object to use ConvertUpperCase()\")\n",
    "\n",
    "    return X\n",
    "\n",
    "We will use the penguin dataset. It has records for three different species of penguins collected from 3 islands in the Palmer Archipelago, Antarctica. \n",
    "* We check for missing data\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "df.isnull().sum()\n",
    "\n",
    "And inspect the DataFrame\n",
    "\n",
    "df.head()\n",
    "\n",
    "We are interested in:\n",
    "* Cleaning the missing data with `MyCustomTransformerForMeanImputation()` on the numerical variables and `CategoricalImputer()` for categorical variables\n",
    "* Next, we want to make all words from the 'sex' column upper case. We will use our own transformer: ConvertUpperCase()\n",
    "\n",
    "\n",
    "We set the pipeline using these rules in three steps. Then we run `.fit_transform()`\n",
    "* Once we inspect the data with .head(), we notice the `'sex'` variable has all letters in upper case!\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'custom_transf', MyCustomTransformerForMeanImputation(variables=['bill_length_mm',\n",
    "                                                                         'bill_depth_mm',\n",
    "                                                                         'flipper_length_mm',\n",
    "                                                                         'body_mass_g'] )),\n",
    "                     \n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='missing',\n",
    "                                                  fill_value='Missing',\n",
    "                                                  variables=['sex']) ),\n",
    "      \n",
    "      ('upper_case' , ConvertUpperCase(variables=['sex'])),\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "df_transformed.head()\n",
    "\n",
    "Let's check if the numerical data is cleaned\n",
    "* It is cleaned!\n",
    "\n",
    "df_transformed.isnull().sum()\n",
    "\n",
    "We now check the mean values from the original data\n",
    "\n",
    "df[['bill_length_mm' , 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].mean()\n",
    "\n",
    "And the learned mean values from `MyCustomTransformerForMeanImputation()` dictionary. We assess the 'custom_transf' steps and check the attribute `.imputer_dict_`, which happens to be the dictionary we stored the mean values in the `.fit()` method \n",
    "\n",
    "pipeline['custom_transf'].imputer_dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 09 - Custom functions for Data Cleaning and Feature Engineering Workflow\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand and use custom functions for data cleaning and feature engineering workflow, using feature-engine transformers\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Packages for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Custom functions for Data Cleaning and Feature Engineering Workflow\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You probably noticed the exercises from previous the units took time and energy. There is no fixed recipe but instead guidelines.\n",
    "* This is the reason that data practitioners spend a lot of energy and time in data cleaning and feature engineering the variables\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> We created a custom function, made with specific feature-engine transformers, to help you be more effective during the Data Cleaning and Feature Engineering stage. We will instruct you on how we expect you to use and interpret it.\n",
    "\n",
    "* We will present two functions to you now, and we will use them in Walkthrough Project 02.\n",
    "  * `DataCleaningEffect()`\n",
    "  * `FeatureEngineeringAnalysis()`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> These custom functions were delivered specially for this specialisation. The functions' logic and usability were tested and reviewed extensively; however, bugs may appear.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* We will not focus on explaining the code itself but focus on the functionality and instruct how we could use it\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " `DataCleaningEffect()`\n",
    "* Function objective: assess the effect of cleaning the data, when:\n",
    "  * imput mean, median or arbitrary number is a numerical variable\n",
    "  * replace with 'Missing' or most frequent a categorical variable\n",
    "* Parameters: `df_original`: data not cleaned, `df_cleaned`: cleaned data,`variables_applied_with_method`: variables where you applied a given method\n",
    "\n",
    "  * It is understandable if, at first, you don't understand all the code used in the function below. The point is to make sense of the pseudo-code and understand the function parameters.\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
    "\n",
    "  flag_count=1 # Indicate plot number\n",
    "  \n",
    "  # distinguish between numerical and categorical variables\n",
    "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
    "\n",
    "  # scan over variables, \n",
    "    # first on variables that you applied the method\n",
    "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
    "  for set_of_variables in [variables_applied_with_method]:\n",
    "    print(\"\\n=====================================================================================\")\n",
    "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
    "    print(f\"{set_of_variables} \\n\\n\")\n",
    "  \n",
    "\n",
    "    for var in set_of_variables:\n",
    "      if var in categorical_variables:  # it is categorical variable: barplot\n",
    "        \n",
    "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
    "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
    "        dfAux = pd.concat([df1, df2], axis=0)\n",
    "        fig , axes = plt.subplots(figsize=(15, 5))\n",
    "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend() \n",
    "\n",
    "      else: # it is numerical variable: histogram\n",
    "\n",
    "        fig , axes = plt.subplots(figsize=(10, 5))\n",
    "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
    "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.legend() \n",
    "\n",
    "      plt.show()\n",
    "      flag_count+= 1\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " `FeatureEngineeringAnalysis()`\n",
    "* Function objective: apply a set of transformers, defined by the user, for a given set of variables\n",
    "* Parameters: `df`: data, `analysis_type`:` ['numerical', 'ordinal_encoder',  'outlier_winsorizer']`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> You should parse the proper variable data types according to your analysis, for example, you shall parse only numerical variables when selecting 'numerical' for analysis_type\n",
    "\n",
    "  * It is understandable if, at first, you don't understand all the code used in the function below. The point is to make sense of the pseudo-code and understand the function parameters.\n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def FeatureEngineeringAnalysis(df,analysis_type=None):\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  - used for quick feature engineering on numerical and categorical variables\n",
    "  to decide which transformation can better transform the distribution shape \n",
    "  - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
    "\n",
    "  \"\"\"\n",
    "  check_missing_values(df)\n",
    "  allowed_types= ['numerical', 'ordinal_encoder',  'outlier_winsorizer']\n",
    "  check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
    "  list_column_transformers = define_list_column_transformers(analysis_type)\n",
    "  \n",
    "  \n",
    "  # Loop over each variable and engineer the data according to the analysis type\n",
    "  df_feat_eng = pd.DataFrame([])\n",
    "  for column in df.columns:\n",
    "    # create additional columns (column_method) to apply the methods\n",
    "    df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
    "    for method in list_column_transformers:\n",
    "      df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
    "      \n",
    "    # Apply transformers in respectives column_transformers\n",
    "    df_feat_eng, list_applied_transformers = apply_transformers(analysis_type, df_feat_eng, column)\n",
    "\n",
    "    # For each variable, assess how the transformations perform\n",
    "    transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng)\n",
    "\n",
    "  return df_feat_eng\n",
    "\n",
    "\n",
    "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
    "  ### Check analyis type\n",
    "  if analysis_type == None:\n",
    "    raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
    "  if analysis_type not in allowed_types:\n",
    "      raise SystemExit(f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
    "\n",
    "def check_missing_values(df):\n",
    "  if df.isna().sum().sum() != 0:\n",
    "    raise SystemExit(\n",
    "        f\"There is missing values in your dataset. Please handle that before getting into feature engineering.\")\n",
    "\n",
    "\n",
    "\n",
    "def define_list_column_transformers(analysis_type):\n",
    "  ### Set suffix colummns acording to analysis_type\n",
    "  if analysis_type=='numerical':\n",
    "    list_column_transformers = [\"log_e\",\"log_10\",\"reciprocal\", \"power\",\"box_cox\",\"yeo_johnson\"]\n",
    "  \n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    list_column_transformers = [\"ordinal_encoder\"]\n",
    "\n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    list_column_transformers = ['iqr']\n",
    "\n",
    "  return list_column_transformers\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformers(analysis_type, df_feat_eng, column):\n",
    "\n",
    "\n",
    "  for col in df_feat_eng.select_dtypes(include='category').columns:\n",
    "    df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
    "\n",
    "\n",
    "  if analysis_type=='numerical':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
    "  \n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
    "\n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
    "\n",
    "  return df_feat_eng,list_applied_transformers\n",
    "\n",
    "\n",
    "\n",
    "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
    "  # For each variable, assess how the transformations perform\n",
    "  print(f\"* Variable Analyzed: {column}\")\n",
    "  print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
    "  for col in [column] + list_applied_transformers:\n",
    "    \n",
    "    if analysis_type!='ordinal_encoder':\n",
    "      DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "    \n",
    "    else:\n",
    "      if col == column: \n",
    "        DiagnosticPlots_Categories(df_feat_eng, col)\n",
    "      else:\n",
    "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
    "  plt.xticks(rotation=90) \n",
    "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
    "  plt.show();\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Numerical(df, variable):\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "  sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=axes[0]) \n",
    "  stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
    "  sns.boxplot(x=df[variable],ax=axes[2])\n",
    "  \n",
    "  axes[0].set_title('Histogram')\n",
    "  axes[1].set_title('QQ Plot')\n",
    "  axes[2].set_title('Boxplot')\n",
    "  fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
    "  plt.show();\n",
    "\n",
    "\n",
    "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "  try:  \n",
    "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
    "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
    "  \n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
    "    \n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### Winsorizer iqr\n",
    "  try: \n",
    "    disc=Winsorizer(\n",
    "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
    "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_iqr\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatEngineering_Numerical(df_feat_eng,column):\n",
    "\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### LogTransformer base e\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_e\"])\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_e\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_e\"],axis=1,inplace=True)\n",
    "\n",
    "    ### LogTransformer base 10\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_10\"],base='10')\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_10\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_10\"],axis=1,inplace=True)\n",
    "\n",
    "  ### ReciprocalTransformer\n",
    "  try:\n",
    "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_reciprocal\"])\n",
    "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_reciprocal\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_reciprocal\"],axis=1,inplace=True)\n",
    "\n",
    "  ### PowerTransformer\n",
    "  try:\n",
    "    pt = vt.PowerTransformer(variables = [f\"{column}_power\"])\n",
    "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_power\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_power\"],axis=1,inplace=True)\n",
    "\n",
    "  ### BoxCoxTransformer\n",
    "  try:\n",
    "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_box_cox\"])\n",
    "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_box_cox\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_box_cox\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  ### YeoJohnsonTransformer\n",
    "  try:\n",
    "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yeo_johnson\"])\n",
    "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
    "  except:\n",
    "        df_feat_eng.drop([f\"{column}_yeo_johnson\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "We will present the use cases and interpretations so that you can conduct your data cleaning and feature engineering steps more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle missing data\n",
    "\n",
    "We are assuming that at this moment of your project in the workplace, you have already conducted an initial EDA of your data, and you know which variables require you to handle missing data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> In this exercise for Data Cleaning, we  will follow these steps:\n",
    "\n",
    "* 1 - Select an imputation method\n",
    "* 2 - Select variables to apply the method to\n",
    "* 3 - Create a separate DataFrame to apply the method\n",
    "* 4 - Assess the effect on the variable distribution\n",
    "\n",
    "Let's consider the titanic dataset. It holds passengers' records from its unique ride. \n",
    "\n",
    "df = sns.load_dataset('titanic').drop(['alive'],axis=1)\n",
    "df.head()\n",
    "\n",
    "We inspect the dataset and notice there are variable data types which are `'category'`. \n",
    "* Typically, categorical variables are handled as `'object'`, but sometimes, for some reason, the data is stored as `'category'` instead. \n",
    "* Feature engine library handles the data properly when a categorical variable is an `'object'` data type. \n",
    "\n",
    "df.info()\n",
    "\n",
    "We will convert them to `'object'` data type by looping over the variables with data type as `'category'` and converting to `'object'`\n",
    "\n",
    "for col in df.select_dtypes(include='category'):\n",
    "  df[col] = df[col].astype('object')\n",
    "\n",
    "We check for missing data. \n",
    "* There are numerical and categorical data with missing data\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Numerical\n",
    "\n",
    "Using the methods we covered, you may impute with mean, median or arbitrary.\n",
    "* For our exercise, we will assume we made an EDA and selected median\n",
    "\n",
    "1 - Select an imputation method\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "2 - Select the variables to apply the method to\n",
    "* You have to make sure you are using numerical variables\n",
    "\n",
    "variables_method = ['age']\n",
    "variables_method\n",
    "\n",
    "3 - Create a separate DataFrame to apply the method\n",
    "\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "\n",
    "\n",
    "4 - Assess the effect on the variable distribution\n",
    "* The function plots in the same Axes, the distribution before and after applying the method; This helps to give you insights into how different your variable would look after cleaning.\n",
    "* We notice the \"peak\" in the variable distribution after median imputation.\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Categorical\n",
    "\n",
    "In this exercise, we will impute 'Missing' on categorical variables \n",
    "\n",
    "1 - Select an imputation method\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "2 - Select variables to apply the method to\n",
    "* You have to make sure you are using categorical variables\n",
    "\n",
    "variables_method = ['embarked', 'deck', 'embark_town']\n",
    "variables_method\n",
    "\n",
    "3 - Create a separate DataFrame to apply the method\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method='missing',fill_value='Missing',\n",
    "                             variables=variables_method)\n",
    "\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "4 - Assess the effect on the variable distribution\n",
    "* It was probably not a good idea to consider this method on these variables\n",
    "  * For the deck, we might consider dropping the variable, since its missing levels are high\n",
    "  * For embarked and embark_town, we may consider replacing with most frequent since the missing data levels are low.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> This exercise gives an idea of how this function works in practice.\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Engineering\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> In this exercise for Feature Engineering workflow, we will follow these steps:\n",
    "\n",
    "* 1 - Select variable(s)\n",
    "* 2 - Create a separate DataFrame, for that variable(s)\n",
    "* 3 - Assess engineered variables distribution \n",
    "\n",
    "\n",
    "In your career, you will develop your preferences and unique methods for dealing with data cleaning and feature engineering. As a starting point, we suggest starting the feature engineering workflow by:\n",
    "* Looking for categorical encoding\n",
    "* Looking for handling outliers\n",
    "* Looking for numerical transformation\n",
    "\n",
    "---\n",
    "\n",
    "Let's recap our dataset\n",
    "\n",
    "df.head()\n",
    "\n",
    "We can check missing data levels\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "In the last section, we didn't impute any missing data to the original DataFrame (df); we just checked how it would look if we applied a given imputer.\n",
    "* For the next exercise, we create a quick pipeline to manage missing data, but dropping the feature with a lot of missing data, add median as imputer for age, and drop the remaining missing data\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "      ( 'DropFeatures', DropFeatures(features_to_drop=['deck']) ),\n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median', variables=['age']) ),\n",
    "      ( 'DropMissingData', DropMissingData()),\n",
    "])\n",
    "\n",
    "df = data_cleaning_pipeline.fit_transform(df)\n",
    "\n",
    "We check missing data levels again\n",
    "* We are good to go for feature engineering\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Ordinal Encoder\n",
    "\n",
    "Again We assume that at this moment, you are on a project in the workplace; you will have already done an EDA on the variables, so you will know which variables to encode.\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering= ['sex', 'embarked', 'who', 'embark_town']\n",
    "variables_engineering\n",
    "\n",
    "2 - Create a separate DataFrame for these variables\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "* We notice that the distribution will not be normally distributed when we encode a category to a number. The new data type is numerical discrete (not continuous), and that is fine\n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering,analysis_type='ordinal_encoder')\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Outlier \n",
    "\n",
    "Again We will assume that at this moment, you are on a project in the workplace, you will have already done an EDA on the variables, so you will know which variables to consider in this outlier analysis\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering = ['age', 'fare']\n",
    "variables_engineering\n",
    "\n",
    "2 - Create a separate DataFrame for the variable(s)\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "* We note that for both variables, replacing outliers with the IQR method didn't help to become normal distributed but helped to become less abnormal, and this tends to be positive for an ML model. Therefore, you will consider this step in your pipeline when age and fare are features. \n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering.dropna(),\n",
    "                                            analysis_type='outlier_winsorizer')\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Numerical\n",
    "\n",
    "Again We will assume that at this moment, you are on a project in the workplace, you will have already done an EDA on the variables, so you will know which variables to try numerical transformation\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering= ['fare']\n",
    "variables_engineering\n",
    "\n",
    "2 - Create a separate DataFrame for the variable(s)\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "\n",
    "* The function will try to transform a variable using the following transformer: Log base e and base 10 Transformer, Power Transformer, Reciprocal Transformer, Box Cox Transformer and Yeo Johnson Transformer. In case it is not possible to compute a given transformation (ex.: log transformation doesn't work for negative values), the function will dismiss that given transformation to that given variable.\n",
    "* For fare, it was possible only to apply Power Transformer and Yeo Johnson.\n",
    "* Yeo Johnson has a distribution with fewer outliers, and even not being normal distributed, it is better than before. We shall consider this transformer for rare features.\n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering,analysis_type='numerical')\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
