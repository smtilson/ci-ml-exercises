{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* We will tune the hyperparameters of for Logistic Regression and an Adaptive Boost model.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Training and Testing data sets from notebook 04.\n",
    "* Insights developed in the previous notebook.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* We will have saved models with tuned hyper parameters at the end of this notebook.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* We are making some philosophical assumptions about the nature of hyperparameters. The basic assumption is that the performance of a model trained with hyperparameters that are \"near enough\" to each other will perform \"similarly enough.\" This is the idea that the performance of the model depends _continuously_ on the hyperparameters. We in fact assume a certain amount of regularity of this dependence. In partial differential equations (pdes), the kind of behavior we are assuming is characteristic of elliptic pdes. We do not have a technical reason for believing this. We assume this is an active area of research for various models, but in general it is outside the scope of this project. It does influence our decision in how we go about searching for good hyperparameters.\n",
    "\n",
    "---\n",
    "# Change working directory\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()\n",
    "import os\n",
    "\n",
    "home_dir = '/workspace/pp5-ml-dashboard'\n",
    "os.chdir(home_dir)\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "We now load our training and test sets, as well as some of the packages that we will be using.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from src.utils import get_df, save_df\n",
    "\n",
    "train_dir = 'datasets/train/csv'\n",
    "X_TrainSet = get_df('X_TrainSet',train_dir)\n",
    "Y_TrainSet = get_df('Y_TrainSet',train_dir)\n",
    "\n",
    "test_dir = 'datasets/test/csv'\n",
    "X_TestSet = get_df('X_TestSet',test_dir)\n",
    "Y_TestSet = get_df('Y_TestSet',test_dir)\n",
    "## Section 1: Pipeline and Grid Search set up\n",
    "We recall the code for building our pipelines and the grid search that we performed in the last notebook. Note that some of the constants have changed. \n",
    "\n",
    "We have modified the pipeline to see how feateure selection impacts the performance. Note that setting `thresh=1` essentially removes the `'corr_selector'` step from the pipeline. We will eventually remove this step from the pipeline once we have selected a value for `thresh`.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine import transformation as vt\n",
    "from feature_engine.selection import DropFeatures, SmartCorrelatedSelection\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Constants needed for feature engineering\n",
    "TO_DROP = ['ftm_away', 'plus_minus_home', 'fg3m_away', 'pts_away', 'play_off',\n",
    "           'fgm_away', 'pts_home', 'fg3m_home', 'ftm_home', 'fgm_home',\n",
    "           'season']\n",
    "THRESH = 0.6\n",
    "TRANSFORMS = {'box_cox':(vt.BoxCoxTransformer,False),\n",
    "              'yeo_johnson':(vt.YeoJohnsonTransformer,False)}\n",
    "TRANSFORM_ASSIGNMENTS = {\n",
    "    'yeo_johnson': ['dreb_away', 'blk_home', 'oreb_away', 'fta_away',\n",
    "                    'dreb_home', 'ast_home', 'stl_away', 'stl_home',\n",
    "                    'reb_away', 'oreb_home', 'pf_away', 'pf_home'],\n",
    "    'box_cox': ['ast_away', 'fta_home']\n",
    "                            }\n",
    "\n",
    "\n",
    "def base_pipeline(thresh=THRESH):\n",
    "    pipeline = Pipeline([\n",
    "        ('dropper', DropFeatures(features_to_drop=TO_DROP)),\n",
    "        ('corr_selector', SmartCorrelatedSelection(method=\"pearson\",\n",
    "                                                   threshold=thresh,\n",
    "                                                   selection_method=\"variance\")\n",
    "                                                   )\n",
    "                        ])\n",
    "    return pipeline\n",
    "\n",
    "    \n",
    "def add_transformations(pipeline, transform_assignments):\n",
    "    # This needs to be called after the above is fit so that the correlation selector has that attr\n",
    "    dropping = pipeline['corr_selector'].features_to_drop_\n",
    "    \n",
    "    new_assignments = { key: [val for val in value if val not in dropping] \n",
    "                       for key,value in transform_assignments.items()}\n",
    "    for transform, targets in new_assignments.items():\n",
    "        if not targets:\n",
    "            continue\n",
    "        pipeline.steps.append(\n",
    "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
    "            )\n",
    "    pipeline.steps.append(('scaler', StandardScaler()))\n",
    "    return pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'GradientBoosting': GradientBoostingClassifier,\n",
    "    'AdaBoost': AdaBoostClassifier,\n",
    "}\n",
    "\n",
    "def create_pipe(model_name, random_state=42, params={}):\n",
    "    model = MODELS[model_name](random_state=random_state,**params)\n",
    "    base_pipe = base_pipeline()\n",
    "    base_pipe.fit(X_TrainSet)\n",
    "    pipe= add_transformations(base_pipe,TRANSFORM_ASSIGNMENTS)\n",
    "    pipe.steps.append((\"feat_selection\", SelectFromModel(model)))\n",
    "    pipe.steps.append(('model',model))\n",
    "    pipe.model_type = model_name\n",
    "    pipe.name = model_name\n",
    "    return pipe\n",
    "\n",
    "Next, we have the code for our grid search. As we will be treating `thresh` as a hyperparameter, it will be slightly different.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "\n",
    "\n",
    "def grid_search(X_train, y_train,pipe,param_grid={},verbosity=1):\n",
    "    print(f\"### Beginning grid search for {pipe.name} ###\") \n",
    "    grid=GridSearchCV(estimator=pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=5,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=verbosity,\n",
    "                    scoring=['accuracy','precision'],\n",
    "                    refit='precision')\n",
    "    grid.fit(X_train,y_train)\n",
    "    return grid\n",
    "\n",
    "## Section 2: Logistic Regression\n",
    "Logistic Regression models have many hyperparameters. We will focus on:\n",
    "* `penalty` is a regularization parameter.\n",
    "* `solver` specifies the type of algorithm used.\n",
    "* `C` controls the strength of the penalty.\n",
    "\n",
    "Not all penalties work for each solver.\n",
    "\n",
    "These will be our initial choice for hyperparameters. They will help us narrow down our search and find other ranges to test.\n",
    "thresholds = [round(0.1*i,2) for i in range(5,11)]\n",
    "C = [10**(2*i+1) for i in range(-2,2)]\n",
    "solver = ['newton-cg', 'newton-cholesky', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "param_grid = [\n",
    "            {'C':C,\n",
    "             'solver':['lbfgs','newton-cg','newton-cholesky','sag'],\n",
    "             'penalty':['l2',None]},\n",
    "            {'C':C,\n",
    "             'solver':['liblinear'],\n",
    "             'penalty':['l1','l2']},\n",
    "            {'C':C,\n",
    "             'solver':['saga'],\n",
    "             'penalty':['l1','l2',None,'elasticnet']}\n",
    "              ]\n",
    "logistic_param_grid = [{'model__'+key:value \n",
    "                                for key,value in param_dict.items()}\n",
    "                                for param_dict in param_grid]\n",
    "for params in logistic_param_grid:\n",
    "    params['corr_selector__threshold']=thresholds\n",
    "\n",
    "Now we are ready to do the grid search. We expect this to go well since Logistic Regression was our best performing model without any tuning. This initial training will help us establish a range to further tune the hyperparamters in.\n",
    "from src.utils import save_df, get_df\n",
    "\n",
    "\n",
    "def get_grid_results_df(pipe, name, dir, param_grid={}, verbosity=2):\n",
    "    try:\n",
    "        results_df = get_df(name, dir)\n",
    "    except FileNotFoundError:\n",
    "        pipe_grid_search = grid_search(\n",
    "            X_TrainSet, Y_TrainSet, pipe, param_grid=param_grid, verbosity=verbosity\n",
    "        )\n",
    "        results_df = pd.DataFrame(pipe_grid_search.cv_results_)\n",
    "        save_df(results_df, name, dir)\n",
    "        # this normalizes the types\n",
    "        results_df = get_df(name, dir)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "logistic_pipe = create_pipe(\"LogisticRegression\")\n",
    "results_name = \"logistic_grid_results_v1\"\n",
    "dir = \"experiment_results/tuning/grids\"\n",
    "\n",
    "logistic_results_df_v1 = get_grid_results_df(\n",
    "    logistic_pipe, results_name, dir, param_grid=logistic_param_grid\n",
    ")\n",
    "\n",
    "Let's look at what the best choices are at this stage.\n",
    "from src.model_eval import get_best_params_df\n",
    "\n",
    "get_best_params_df(logistic_results_df_v1)\n",
    "Clearly, the best correlation threshold is 0.8. The models have the same scores. Lets collect all of the parameters that have the same scores. We order the scores not by count but by score.\n",
    "from src.model_eval import present_score_counts, score_stats\n",
    "\n",
    "\n",
    "present_score_counts(logistic_results_df_v1)\n",
    "best_score = score_stats(logistic_results_df_v1)\n",
    "\n",
    "The most common scores are `nan` (we changed the score to be -1 if a `nan` value was showing up). This happens when a choice of parameters does not work well together. We won't worry about this as we are able to get quite good precision and accuracy with this first pass. Note that the best accuracy score is not far from the accuracy of the model with the best precision.\n",
    "\n",
    "34 different choices of parameters had the best performance. We would like to see what these estimators had in common and look at neighborhoods around these parameters to see if we can improve the performance before moving on to the next model.\n",
    "from src.model_eval import present_param_counts\n",
    "\n",
    "present_param_counts(logistic_results_df_v1, best_score)\n",
    "We will make the following modifications to `logistic_param_grid`:\n",
    "* remove `'liblinear'` and `'sag'` as they were used the least,\n",
    "* pick a neighborhood around 0.8 for correlation threshold,\n",
    "* focus on penalties `'l2'` and `'None'`\n",
    "* focus on the range 1 to 1000 for `C`\n",
    "\n",
    "We will see if focusing gives us any improvement in score.\n",
    "from src.utils import divide_range\n",
    "\n",
    "thresholds = divide_range(0.75,0.85,5)\n",
    "C = divide_range(1,1000,6)\n",
    "logistic_param_grid_v2=[{'model__C': C,\n",
    "  'model__solver': ['lbfgs', 'newton-cg', 'newton-cholesky','saga'],\n",
    "  'model__penalty': ['l2', None],\n",
    "  'corr_selector__threshold': thresholds}]\n",
    "logistic_results_df_v2 = get_grid_results_df(logistic_pipe,\n",
    "                                             'logistic_results_df_v2', dir, \n",
    "                                             param_grid=logistic_param_grid_v2,\n",
    "                                             verbosity=3)\n",
    "Let's proceed by doing the analysis we did above of the results of this grid search.\n",
    "get_best_params_df(logistic_results_df_v2)\n",
    "Again, very similar scores. So let's analyze the scores that showed up as we did before.\n",
    "present_score_counts(logistic_results_df_v2)\n",
    "best_score = score_stats(logistic_results_df_v2)\n",
    "Our most common score is our best score. It seems like we have chosen a good range of parameters since many of the combinations yield good results.\n",
    "present_param_counts(logistic_results_df_v2, best_score)\n",
    "All of the choices of parameters seem to be performing equally well, with insignificant exceptions. We will have to look at other metrics to determine distinguish between these choices of parameters. Things such as training time statistics, and standard deviation of the scores.\n",
    "best_results = logistic_results_df_v2.query(f'mean_test_precision == {best_score[0]} and mean_test_accuracy == {best_score[1]}')\n",
    "std_score_counts = {}\n",
    "\n",
    "for _, row in best_results.iterrows():\n",
    "    std_score = (row['std_test_precision'], row['std_test_accuracy'])\n",
    "    if std_score in std_score_counts:\n",
    "        std_score_counts[std_score] += 1\n",
    "    else:\n",
    "        std_score_counts[std_score] = 1\n",
    "for key, value in std_score_counts.items():\n",
    "    print(f\"std_test_precision: {key[0]}\"\n",
    "          f\"\\nstd_test_accuracy: {key[1]}\"\n",
    "          f\"\\ncount: {value}\")\n",
    "So standard deviation will not help us distinguish either. This is annoying, but good. We will see how the models perform on the test data set.\n",
    "\n",
    "Note: If you are tinkering and running cells multiple times, we recommend commenting out the code in the following three cells. They take a bit even after they have already been run the first time since we aren't saving the large number of pipelines and models.\n",
    "import ast\n",
    "\n",
    "def parameter_dicts(results_df, best_score):\n",
    "    relevant = results_df.query(f'mean_test_precision == {best_score[0]} and mean_test_accuracy == {best_score[1]}')\n",
    "    param_dicts = [ast.literal_eval(param_dict) for param_dict in relevant['params'].values]\n",
    "    return param_dicts\n",
    "\n",
    "best_params = parameter_dicts(logistic_results_df_v2, best_score)\n",
    "'''\n",
    "best_pipes = []\n",
    "for param_dict in best_params:\n",
    "    base_pipe = create_pipe('LogisticRegression')\n",
    "    pipe = base_pipe.set_params(**param_dict)\n",
    "    pipe.param_dict = param_dict\n",
    "    best_pipes.append(pipe)\n",
    "\n",
    "model_params = {key.split('__')[0]:key.split('__')[1]\n",
    "                for key in best_params[0].keys()}\n",
    "count = 0\n",
    "for pipe in best_pipes:\n",
    "    print(f\"Pipe {count}:\")\n",
    "    for step in pipe.get_params()['steps']:\n",
    "        if step[0] in model_params:\n",
    "            param = model_params[step[0]]\n",
    "            value = step[1].get_params()[param]\n",
    "            print(f\"{step[0]}\"\n",
    "                  f\"\\n{param}: {value}\")\n",
    "    print()\n",
    "    count += 1\n",
    "    if count>=2:\n",
    "        break\n",
    "'''\n",
    "We will now train all of the above pipelines and evaluate them on the test dataset.\n",
    "'''count = 0\n",
    "for pipe in best_pipes:\n",
    "    print(f\"Training pipe {count}:\")\n",
    "    print(pipe.param_dict)\n",
    "    pipe.fit(X_TrainSet, Y_TrainSet)\n",
    "    count+=1\n",
    "    '''\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "'''\n",
    "def evaluate_param_on_test_set(pipe,X_test, Y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    recall = recall_score(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred)\n",
    "    return ((precision, accuracy, recall, f1), pipe.param_dict)\n",
    "\n",
    "\n",
    "def evaluate_and_sort(fitted_pipes, X_test, Y_test):\n",
    "    evaluations = [evaluate_param_on_test_set(pipe, X_test, Y_test)\n",
    "               for pipe in fitted_pipes]\n",
    "    evaluation_dict = {}\n",
    "    for eval in evaluations:\n",
    "        if eval[0] in evaluation_dict:\n",
    "            evaluation_dict[eval[0]].append(eval[1])\n",
    "        else:\n",
    "            evaluation_dict[eval[0]] = [eval[1]]\n",
    "    sorted_eval_dict = {k:v for k,v in sorted(evaluation_dict.items(),\n",
    "                                         key=lambda item: item[0],\n",
    "                                         reverse=True)}\n",
    "    return sorted_eval_dict\n",
    "\n",
    "sorted_evals = evaluate_and_sort(best_pipes, X_TestSet, Y_TestSet)\n",
    "print(len(sorted_evals))\n",
    "'''\n",
    "It turns out that all of these best sets of parameters produce models that perform equally well with respect to the standard meterics. We will have to pick one to deploy. We will look at the time it took to score each model during the grid search.\n",
    "time_results = best_results.filter(['mean_score_time','std_score_time','params'])\n",
    "\n",
    "time_results = time_results.sort_values(by=['mean_score_time','std_score_time'])\n",
    "print(time_results.head())\n",
    "params_choice = time_results.iloc[0]['params']\n",
    "print(params_choice)\n",
    "\n",
    "We have the following choice of hyperparameters:\n",
    "* correlation threshold: 0.77\n",
    "* `C`: 500.5\n",
    "* solver method: newton-cg\n",
    "* penalty function: l2\n",
    "\n",
    "Let's train the model and then look at the classification report. We don't need to list the penalty function since l2 is the default penalty function.\n",
    "model_params = {'C':500.5, 'solver': 'newton-cg'}\n",
    "\n",
    "final_logistic_pipe = create_pipe(model_name='LogisticRegression', params=model_params)\n",
    "final_logistic_pipe.set_params(corr_selector__threshold=0.77)\n",
    "final_logistic_pipe\n",
    "Let's now see the importance of the different features according to our final model.\n",
    "### Section 3: AdaBoost\n",
    "We will now do a grid search with AdaBoost. After an initial search to determine a range, we will investigate more closely.\n",
    "\n",
    "With AdaBoost, there are two types of parameters. We have parameters for AdaBoost and parameters for the weak learner it is using as a base estimator. The default base estimator is a Decision tree.\n",
    "\n",
    "AdaBoost has the following hyperparameters.\n",
    "* `n_estimators` the max number of estimators\n",
    "* `learning_rate` which weights the estimators\n",
    "* `algorithm` of which there are two choices\n",
    "\n",
    "Decision trees classifiers have the following hyperparameters.\n",
    "* `max_depth`\n",
    "* `min_samples_split`\n",
    "* `min_leaf_split`\n",
    "\n",
    "In order to get some early results, we will break the hyperparameter grid into smaller pieces along certain axes. We will first investigate how the hyperparameters of AdaBoost impact the models and then we will tune the hyperparameters of the inner Decision tree.\n",
    "from src.utils import divide_range\n",
    "\n",
    "thresholds = divide_range(0.55,0.95,5)\n",
    "\n",
    "base_ada_params = [{'corr_selector__threshold': thresholds,\n",
    "'model__n_estimators':[int(i) for i in divide_range(20,50,5)],\n",
    "'model__learning_rate': divide_range(0.5,2,5),\n",
    "'model__algorithm':['SAMME', 'SAMME.R']\n",
    "}]\n",
    "\n",
    "ada_pipe = create_pipe('AdaBoost')\n",
    "base_ada_results_df_v1 = get_grid_results_df(ada_pipe,\n",
    "                                             'base_ada_results_df_v1', dir, \n",
    "                                             param_grid=base_ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "Let's see what the top scores are.\n",
    "present_score_counts(base_ada_results_df_v1)\n",
    "best_score = score_stats(base_ada_results_df_v1)\n",
    "Let's look at the parameters associated with the top 5 scores.\n",
    "from src.model_eval import collect_like_estimators\n",
    "\n",
    "def top_n(results_df,num=5,exclude=None):\n",
    "    estimators_by_score = collect_like_estimators(results_df)\n",
    "    scores = list(estimators_by_score.keys())\n",
    "    top = sorted(scores,reverse=True)[:num]\n",
    "    for score in top:\n",
    "        print(f\"Score: {score}\")\n",
    "        present_param_counts(results_df, score, exclude)\n",
    "\n",
    "top_n(base_ada_results_df_v1)\n",
    "This gives us an idea of how to narrow down our hyperparameters.\n",
    "* We will focus on a range estimators from 40-70\n",
    "* We will use the algorithm `'SAMME.R'`.\n",
    "* We will focus on a range of learning rate of 0.9 to 1.6\n",
    "* We will focus on a range of correlation threshold from 0.77 to 0.95\n",
    "thresholds = divide_range(0.77,0.95,5)\n",
    "\n",
    "base_ada_params_v2 = [{'corr_selector__threshold': thresholds,\n",
    "'model__n_estimators':[int(i) for i in divide_range(40,70,5)],\n",
    "'model__learning_rate': divide_range(0.9,1.6,5),\n",
    "'model__algorithm':['SAMME.R']\n",
    "}]\n",
    "base_ada_results_df_v2 = get_grid_results_df(ada_pipe,'base_ada_results_df_v2',\n",
    "                                             dir, param_grid=base_ada_params_v2,\n",
    "                                             verbosity=3)\n",
    "Let's see what the top scores are.\n",
    "present_score_counts(base_ada_results_df_v2)\n",
    "best_score = score_stats(base_ada_results_df_v2)\n",
    "Let's look at the parameters associated with the top 5 scores.\n",
    "top_n(base_ada_results_df_v2,6)\n",
    "It seems that an increase in the number of estimators improves performance, so we will slide the window we look at up a bit. Unfortunately, this will increase the fit time. We will take the average of the correlation thresholds. We feel we didn't cast a wide enough net with respect to learning rate, so we will look at a larger window for this parameter as well.\n",
    "base_ada_params_v3 = [{'corr_selector__threshold': [0.785],\n",
    "'model__n_estimators':[int(i) for i in divide_range(60,90,5)],\n",
    "'model__learning_rate': divide_range(1.1,5,5),\n",
    "'model__algorithm':['SAMME.R']}]\n",
    "\n",
    "base_ada_results_df_v3 = get_grid_results_df(ada_pipe,'base_ada_results_df_v3',\n",
    "                                             dir, param_grid=base_ada_params_v3,\n",
    "                                             verbosity=3)\n",
    "present_score_counts(base_ada_results_df_v3)\n",
    "best_score = score_stats(base_ada_results_df_v3)\n",
    "The scores did improve, but not significantly. If the improvement is due to values for learning rate and estimators being outside of our earlier ranges, then we will have to continue searching.\n",
    "top_n(base_ada_results_df_v3)\n",
    "So the number of estimators jumped a fair bit. In the case of the Logistic Regression model, we got many many high performing sets of parameters. This made it difficult to focus on a few small neighborhoods (in the parameter space) of high performing sets of parameters. Our learning rate has stabilized, so we can focus on tuning it more carefully.\n",
    "base_ada_params_v4 = [{'corr_selector__threshold': [0.785],\n",
    "'model__n_estimators':[int(i) for i in divide_range(75,100,5)],\n",
    "'model__learning_rate': divide_range(1.05,1.15),\n",
    "'model__algorithm':['SAMME.R']}]\n",
    "\n",
    "base_ada_results_df_v4 = get_grid_results_df(ada_pipe,'base_ada_results_df_v4',\n",
    "                                             dir, param_grid=base_ada_params_v4,\n",
    "                                             verbosity=3)\n",
    "present_score_counts(base_ada_results_df_v4)\n",
    "best_score = score_stats(base_ada_results_df_v4)\n",
    "top_n(base_ada_results_df_v4)\n",
    "We are closing in on a learning rate of 1.1. Perhaps we need to slide our number of estimators up a bit higher. While we haven't settled on a number of estimators, we have at least settled on a learning rate. This means that the parameter grid has shrunk.\n",
    "base_ada_params_v5 = [{'corr_selector__threshold': [0.785],\n",
    "'model__n_estimators':[int(i) for i in divide_range(95,125,10)],\n",
    "'model__learning_rate': [1.08,1.1,1.12],\n",
    "'model__algorithm':['SAMME.R']}]\n",
    "\n",
    "base_ada_results_df_v5 = get_grid_results_df(ada_pipe,'base_ada_results_df_v5',\n",
    "                                             dir, param_grid=base_ada_params_v5,\n",
    "                                             verbosity=3)\n",
    "present_score_counts(base_ada_results_df_v5)\n",
    "best_score = score_stats(base_ada_results_df_v5)\n",
    "present_score_counts(base_ada_results_df_v4,2)\n",
    "present_score_counts(base_ada_results_df_v5,2)\n",
    "The performance is about the same. Let's look at the parameters.\n",
    "standard_exclude = standard_exclude = ['model__base_estimator',\n",
    "           'corr_selector__threshold','model__algorithm']\n",
    "top_n(base_ada_results_df_v5,exclude=standard_exclude)\n",
    "\n",
    "We will use 125 estimators and a learning rate of 1.12. Now to tune the Decision tree specific hyperparameters. The hyperparameters we will concern ourself with and their default values are:\n",
    "* `min_samples_split`: 2\n",
    "* `min_samples_leaf`: 1\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "ada_params = {'corr_selector__threshold':[0.785],\n",
    "            'model__n_estimators': [125],\n",
    "            'model__learning_rate': [1.12], \n",
    "            'model__algorithm':['SAMME.R']}\n",
    "dt_params = {'min_samples_split': [int(i) for i in divide_range(2,10)],\n",
    "              'min_samples_leaf': [int(i) for i in divide_range(1,5)]}\n",
    "new_dt_params = {\"model__base_estimator__\"+key:value \n",
    "                 for key,value in dt_params.items()}\n",
    "\n",
    "ada_params.update(new_dt_params)\n",
    "ada_params['model__base_estimator'] = [DecisionTreeClassifier(random_state=42)]\n",
    "ada_dt_results_df_v1 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v1',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "This is performing worse than the default parameters. Which isn't so surprising since we put a cap on the \n",
    "ada_params[\"model__base_estimator__max_depth\"] = [10]\n",
    "ada_dt_results_df_v1_10 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v1_10_',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "present_score_counts(ada_dt_results_df_v1_5)\n",
    "best_score_5 = score_stats(ada_dt_results_df_v1_5)\n",
    "present_score_counts(ada_dt_results_df_v1_10)\n",
    "best_score_10 = score_stats(ada_dt_results_df_v1_10)\n",
    "There are many choices of parameters with the best score. Remember, we have fixed several hyperparameters already, so we will exclude them.\n",
    "standard_exclude = ['model__base_estimator',\n",
    "           'model__n_estimators','corr_selector__threshold',\n",
    "           'model__learning_rate','model__algorithm']\n",
    "exclude = standard_exclude + ['model__base_estimator__max_depth']\n",
    "present_param_counts(ada_dt_results_df_v1, best_score, exclude)\n",
    "It seems we have only learned that `min_samples_leaf` being 5 is a decent hyperparameter. Let's try some other values for `max_depth` and see how the results compare.\n",
    "ada_params[\"model__base_estimator__max_depth\"] = [10,20]\n",
    "ada_dt_results_df_v1_10_20 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v1_10_20',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "present_score_counts(ada_dt_results_df_v1_10_20)\n",
    "best_score = score_stats(ada_dt_results_df_v1_10_20)\n",
    "The top performers in this grid search outperformed the top performers when the `max_depth` was set to 5. So we will no longer use that value (which means it will take longer to train the models).\n",
    "top_n(ada_dt_results_df_v1_10_20, exclude=standard_exclude)\n",
    "For the next pass, we will fix the `max_depth` at 20 and see if we can't learn anything more about what the `min_samples_leaf` and `min_samples_split` parameters should be.\n",
    "ada_params['model__base_estimator__min_samples_leaf'] = [1,2,3,5]\n",
    "ada_params['model__base_estimator__min_samples_split'] = [2]#4,7,10]\n",
    "ada_params['model__base_estimator__max_depth'] = [20]\n",
    "\n",
    "ada_dt_results_df_v2_20_2 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v2_20_2',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "ada_params['model__base_estimator__min_samples_split'] = [4]#,7,10]\n",
    "\n",
    "ada_dt_results_df_v2_20_4 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v2_20_4',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "ada_params['model__base_estimator__min_samples_split'] = [7]\n",
    "\n",
    "ada_dt_results_df_v2_20_7 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v2_20_7',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "ada_params['model__base_estimator__min_samples_split'] = [10]\n",
    "\n",
    "ada_dt_results_df_v2_20_10 = get_grid_results_df(ada_pipe,'ada_dt_results_df_v2_20_10',\n",
    "                                             dir, param_grid=ada_params,\n",
    "                                             verbosity=3)\n",
    "\n",
    "# min_sample_splits = 2\n",
    "present_score_counts(ada_dt_results_df_v2_20_2)\n",
    "best_score = score_stats(ada_dt_results_df_v2_20_2)\n",
    "\n",
    "top_n(ada_dt_results_df_v2_20_2, exclude=exclude)\n",
    "# min_sample_splits = 4\n",
    "present_score_counts(ada_dt_results_df_v2_20_4)\n",
    "best_score = score_stats(ada_dt_results_df_v2_20_4)\n",
    "\n",
    "The training time is growing rapidly with the max_depth parameter of the Decision trees. We will stick to a few relatively small values for this parameter while we continue to tune the others.\n",
    "In order to get intermediate results, we will train the AdaBoost model with different base estimators separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* We will investigate the data using clustering algoritthms to investigate our hypothesis # something\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* The game data from before the train test split at the beginning of notebook 04. \n",
    "\n",
    "## Outputs\n",
    "\n",
    "* A clustering model and some analysis of the underlying data set.\n",
    "\n",
    "## Additional Comments\n",
    "* This notebook follow the analysis done in notebook 07 of the Churnometer walkthrough project.\n",
    "---\n",
    "# Change working directory\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()\n",
    "import os\n",
    "\n",
    "home_dir = '/workspace/pp5-ml-dashboard'\n",
    "os.chdir(home_dir)\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "We now load our prepared data.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from src.utils import get_df, save_df\n",
    "\n",
    "game_data = get_df('game_pre_split', 'datasets/clean/csv')\n",
    "game_data.isna().sum(axis=1).sum()\n",
    "## Section 1: Set up problem\n",
    "The game of basketball as evolved over the years. For example, we saw in our exploratory data analysis that there was a correlation between 3 pointers and year. We are going to test if clustering will detect the various eras of basketball, or perhaps it will define new ones.\n",
    "\n",
    "We will then look at the profiles of each cluster to see how it groups games of basketball and try to determine if these clusters have any correlation with time.\n",
    "\n",
    "\n",
    "game_data.drop(labels=['season'],inplace=True,axis=1)\n",
    "game_data.head()\n",
    "Now we construct our pipeline for clustering. The format for the pipeline was inspired by the clustering pipeline in the Churnometer walkthrough project.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_pipe(thresh=0.7,p_components=50,clusters=50):\n",
    "    pipe = Pipeline([\n",
    "        (\"corr_selector\", SmartCorrelatedSelection(method=\"pearson\",\n",
    "                                                   threshold=thresh, \n",
    "                                                   selection_method=\"variance\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"PCA\", PCA(n_components=p_components, random_state=42)),\n",
    "        (\"model\", KMeans(n_clusters=clusters, random_state=42)),\n",
    "    ])\n",
    "    return pipe\n",
    "We will end up tuning the number of clusters and components as hyperparameters. It will be interesting to luck at how the analysis changes as we move between the number of clusters. At each stage, we will see how the function that assigns each game to its season behaves on clusters.\n",
    "\n",
    "## Section 2: PCA\n",
    "We start by doing some Principal component analysis.\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "\n",
    "pipe = cluster_pipe()\n",
    "pca_pipe = Pipeline(pipe.steps[:-2])\n",
    "game_data_pca = pca_pipe.fit_transform(game_data)\n",
    "\n",
    "We are now going to analyze the principal components. Feel free to adjust the number of components.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "# This function is from the notebook 07 of the \n",
    "# churnometer walkthrough project.\n",
    "def pca_component_analysis(df_pca, n_components):\n",
    "    pca = PCA(n_components=n_components).fit(df_pca)\n",
    "\n",
    "    ComponentsList = [f\"Component {number}\"\n",
    "                      for number in range(n_components)]\n",
    "    dfExplVarRatio = pd.DataFrame(\n",
    "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
    "        index=ComponentsList,\n",
    "        columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
    "    )\n",
    "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
    "    )\n",
    "    print(\n",
    "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0, 110, 10))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pca_component_analysis(game_data_pca, 15)\n",
    "\n",
    "As we are hoping the data will be grouped into eras, each of which is multiple years long. We expect there to be a fair number of clusters, so we don't mind having many components.\n",
    "pca_component_analysis(game_data_pca,9)\n",
    "\n",
    "So we redefine our pipeline creation function to have 9 as the number of components.\n",
    "def cluster_pipe(thresh=0.7,clusters=50):\n",
    "    pipe = Pipeline([\n",
    "        (\"corr_selector\", SmartCorrelatedSelection(method=\"pearson\",\n",
    "                                                   threshold=thresh, \n",
    "                                                   selection_method=\"variance\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"PCA\", PCA(n_components=9, random_state=42)),\n",
    "        (\"model\", KMeans(n_clusters=clusters, random_state=42)),\n",
    "    ])\n",
    "    return pipe\n",
    "Now we need to use the elbow method and look at silhouette scores.\n",
    "\n",
    "## Section 3: Elbow Method and Silhouette scores\n",
    "These will help us determine the appropriate number of clusters to use for our algorithm. We are also taking into account some domain knowledge, which is that eras in basketball range from 6 to 10 years. It is not a well defined concept so we don't have a hard number. Our data set ranges from 1985 until 2022. So we (conservatively) expect there to be between 4 and 7 eras.\n",
    "new_pipe = cluster_pipe()\n",
    "pca_part_of_pipe = Pipeline(new_pipe.steps[:-1])\n",
    "game_data_pca = pca_part_of_pipe.fit_transform(game_data)\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=42), k=(1,11))\n",
    "visualizer.fit(game_data_pca) \n",
    "visualizer.show() \n",
    "plt.show()\n",
    "They are suggesting 4 clusters, but 6 also looks promising. Let's see how the Silhouette scores behave with respect to these different numbers of clusters.\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "n_cluster_start = 3\n",
    "n_cluster_stop = 8\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=42), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(game_data_pca)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters,\n",
    "                                                       random_state=42),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(game_data_pca)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "The analysis suggest using 3 clusters. After adding the clustering data to the data frame we will see if the clustering correlates with the `season` feature.\n",
    "pipe_w_clusters = cluster_pipe(clusters=3)\n",
    "game_w_clusters = game_data.copy()\n",
    "game_w_clusters.head()\n",
    "Now we will fit both of these pipelines and see what they tell us.\n",
    "pipe_w_clusters.fit(game_w_clusters)\n",
    "# this can be skipped and done after we have the profile of the clusters\n",
    "game_data_w_seasons = get_df('game_pre_split', 'datasets/clean/csv')\n",
    "def get_season(game_id):\n",
    "    return game_data_w_seasons.loc[game_id, 'season']\n",
    "    # ai suggested the above but I think it should be this one\n",
    "    #return game_data_w_seasons.loc[game_id]['season']\n",
    "game_test = game_w_clusters.copy()\n",
    "\n",
    "game_test.head()\n",
    "game_w_clusters['Clusters'] = pipe_w_clusters.predict(game_data)\n",
    "\n",
    "game_w_clusters.head()\n",
    "print(f\"* Cluster frequencies \\n{game_w_clusters['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "game_w_clusters['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "It is interesting to see how the clusters behave with respect to the components from the PCA step.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "def compare_components_w_clusters(a:int,b:int,pca_step,cluster_step,pipe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=pca_step[:, a], y=pca_step[:, b],\n",
    "                hue=cluster_step['Clusters'], palette='Set1', alpha=0.6)\n",
    "    plt.scatter(x=pipe['model'].cluster_centers_[:, 0],\n",
    "            y=pipe['model'].cluster_centers_[:, 1],\n",
    "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
    "    plt.xlabel(f\"PCA Component {a}\")\n",
    "    plt.ylabel(f\"PCA Component {b}\")\n",
    "    plt.title(\"PCA Components colored by Clusters\")\n",
    "    plt.show()\n",
    "\n",
    "pairs = [(a,b) for a in range(9) for b in range(9) if a<b]\n",
    "print(len(pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate how the clusters and the components from the PCA relate to one another.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "def compare_components_w_clusters(a:int,b:int,pca_step,cluster_step,pipe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=pca_step[:, a], y=pca_step[:, b],\n",
    "                hue=cluster_step['Clusters'], palette='Set1', alpha=0.6)\n",
    "    plt.scatter(x=pipe['model'].cluster_centers_[:, 0],\n",
    "            y=pipe['model'].cluster_centers_[:, 1],\n",
    "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
    "    plt.xlabel(f\"PCA Component {a}\")\n",
    "    plt.ylabel(f\"PCA Component {b}\")\n",
    "    plt.title(\"PCA Components colored by Clusters\")\n",
    "    plt.show()\n",
    "\n",
    "interesting_pairs = [(0,1), (0,3), (6,8)]\n",
    "for a,b in interesting_pairs:\n",
    "    compare_components_w_clusters(a, b, game_data_pca, game_w_clusters,\n",
    "                                  pipe_w_clusters)\n",
    "\n",
    "Feel free to look at how other components compare with respect to the clusters. We felt these three were the most interesting.\n",
    "\n",
    "## Cluster Profile\n",
    "We next wish to determine the profile of these clusters. We do this by training a classification model on the data with the clusters as the target. The important features of the models will help us to determine the profiles of the individual clusters.\n",
    "\n",
    "In our model selection notebook, we found that AdaBoost and Logistic Regression were good models for working with this data. We will use AdaBoost as it works one multi-class classification problems without any need for adjustment.\n",
    "# This will be our target variable.\n",
    "cluster_predictions = game_w_clusters['Clusters']\n",
    "\n",
    "# This will be our data.\n",
    "df = game_w_clusters.copy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    df.drop(['Clusters'], axis=1),\n",
    "                    df['Clusters'], test_size=0.2,\n",
    "                    random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "Now we create our classifier pipeline. We will try it with the default parameters as well as the parameters we found in the last notebook.\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def clf_pipe(thresh=0.7, params={}):\n",
    "    pipe = Pipeline([\n",
    "        ('corr_selector',\n",
    "         SmartCorrelatedSelection(method=\"pearson\",\n",
    "                                  threshold=thresh,\n",
    "                                  selection_method=\"variance\"))\n",
    "                        ])\n",
    "    pipe.fit(X_train)\n",
    "    dropping = pipe['corr_selector'].features_to_drop_\n",
    "    new_assignments = { key: [val for val in value if val not in dropping] \n",
    "                       for key,value in TRANSFORM_ASSIGNMENT.items()}\n",
    "    for transform, targets in new_assignments.items():\n",
    "        if not targets:\n",
    "            continue\n",
    "        pipe.steps.append(\n",
    "            (transform, TRANSFORMS[transform][0](variables=targets))\n",
    "            )\n",
    "    pipe.steps.append(('scaler', StandardScaler()))\n",
    "    model = AdaBoostClassifier(random_state=42,**params)\n",
    "    pipe.steps.append((\"feat_selection\", SelectFromModel(model)))\n",
    "    pipe.steps.append(('model',model))\n",
    "    return pipe\n",
    "\n",
    "ada_thresh = 0.8\n",
    "ada_params = {'n_estimators': 110, 'learning_rate': 1.133,\n",
    "              'algorithm': 'SAMME.R'}\n",
    "default_ada_pipe = clf_pipe()\n",
    "default_ada_pipe.fit(X_train, y_train)\n",
    "tuned_ada_pipe = clf_pipe(thresh=ada_thresh, params=ada_params)\n",
    "tuned_ada_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that attempted 3 points is more important to the clustering than attempted shots of the away team. We will use some functions to get the profile of the clusters from the model we trained. We will use the following functions to determine the profile of the clusters. They are all from the Chrunometer walkthrough project.\n",
    "# All functions are from notebook 07 of the Churnometer walkthrough\n",
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "    DescriptionAllClusters = pd.DataFrame(\n",
    "        columns=df.drop(['Clusters'], axis=1).columns)\n",
    "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
    "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "\n",
    "        EDA_ClusterSubset = df.query(\n",
    "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
    "        ClusterDescription = Clusters_IndividualDescription(\n",
    "            EDA_ClusterSubset, cluster, decimal_points)\n",
    "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
    "            ClusterDescription)\n",
    "\n",
    "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
    "    return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
    "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "    # for a given cluster, iterate over all columns\n",
    "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
    "    # That will show the range for the most common values for the numerical variable\n",
    "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
    "    # That will show the most common levels for the category\n",
    "\n",
    "    for col in EDA_Cluster.columns:\n",
    "        try:  # eventually a given cluster will have only missing data for a given variable\n",
    "            if EDA_Cluster[col].dtypes in ['float', 'int']:\n",
    "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
    "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
    "                Description = f\"{Q1} -- {Q3}\"\n",
    "                ClustersDescription.at[0, col] = Description\n",
    "            else:\n",
    "                raise ValueError(f\"Wrong data type for {col}: {EDA_Cluster[col].dtypes}.\")\n",
    "        except Exception as e:\n",
    "            ClustersDescription.at[0, col] = 'Not available'\n",
    "            print(\n",
    "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "    ClustersDescription['Cluster'] = str(cluster)\n",
    "    return ClustersDescription\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def cluster_distribution_per_variable(df, target):\n",
    "    \"\"\"\n",
    "    The data should have 2 variables, the cluster predictions and\n",
    "    the variable you want to analyze with, in this case we call \"target\".\n",
    "    We use plotly express to create 2 plots:\n",
    "    Cluster distribution across the target.\n",
    "    Relative presence of the target level in each cluster.\n",
    "    \"\"\"\n",
    "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
    "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
    "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
    "\n",
    "    print(f\"Clusters distribution across {target} levels\")\n",
    "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
    "                 color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.show(renderer='jupyterlab')\n",
    "\n",
    "    df_relative = (df\n",
    "                   .groupby([\"Clusters\", target])\n",
    "                   .size()\n",
    "                   .groupby(level=0)\n",
    "                   .apply(lambda x:  100*x / x.sum())\n",
    "                   .reset_index()\n",
    "                   .sort_values(by=['Clusters'])\n",
    "                   )\n",
    "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
    "\n",
    "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
    "                  color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.show(renderer='jupyterlab')\n",
    "\n",
    "df_cluster_profile = df.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df_cluster_profile, decimal_points=0)\n",
    "clusters_profile\n",
    "There are so many different relevant features. We think graphing the distributions may be more illuminating.\n",
    "feature_pairs = []\n",
    "feature_stems = list({term.split('_')[0] for term in best_features})\n",
    "for stem in feature_stems:\n",
    "    home_stem = stem + '_home'\n",
    "    away_stem = stem + '_away'\n",
    "    if home_stem in best_features and away_stem in best_features:\n",
    "        feature_pairs.append((home_stem, away_stem))\n",
    "    elif home_stem in best_features:\n",
    "        feature_pairs.append((home_stem, ''))\n",
    "    elif away_stem in best_features:\n",
    "        feature_pairs.append(('',away_stem))\n",
    "\n",
    "for home, away in feature_pairs:\n",
    "    if home and away:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "        sns.histplot(data=df, x=home, kde=True, element=\"step\", ax=axes[0],\n",
    "                     hue='Clusters',palette='deep')\n",
    "        sns.histplot(data=df, x=away, kde=True, element=\"step\", ax=axes[1],\n",
    "                     hue='Clusters', palette='deep')\n",
    "        axes[0].set_title(f'{home}')\n",
    "        axes[1].set_title(f'{away}')\n",
    "        plt.show()\n",
    "    elif home:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "        sns.histplot(data=df, x=home, kde=True, element=\"step\", ax=axes,\n",
    "                     hue='Clusters', palette='deep')\n",
    "        axes.set_title(f'{home}')\n",
    "        plt.show()\n",
    "    elif away:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "        sns.histplot(data=df, x=away, kde=True, element=\"step\", ax=axes,\n",
    "                     hue='Clusters', palette='deep')\n",
    "        axes.set_title(f'{away}')\n",
    "        plt.show()\n",
    "## Refit Clustering\n",
    "We now refit the clustering pipeline with the most relevant features determined by the profile in the last section. This will lead to a more focused model since it is only being trained on the features already determined to be most relevant to the clustering.\n",
    "to_drop = [col for col in game_data.columns if col not in best_features]\n",
    "new_assignments = { key: [val for val in value if val not in to_drop] \n",
    "                       for key,value in TRANSFORM_ASSIGNMENT.items()}\n",
    "    \n",
    "def last_cluster_pipe(clusters=3):\n",
    "    pipe = Pipeline([('yeo_johnson',vt.YeoJohnsonTransformer(\n",
    "                                    variables=new_assignments['yeo_johnson'])),\n",
    "                                    ('box_cox',vt.BoxCoxTransformer(\n",
    "                                    variables=new_assignments['box_cox'])),\n",
    "                                    ('scaler', StandardScaler()),\n",
    "                                    ('model', KMeans(n_clusters=3, \n",
    "                                                     random_state=42))])\n",
    "    return pipe\n",
    "\n",
    "game_data_reduced= game_data.copy().filter(best_features)\n",
    "game_data_reduced.head()\n",
    "\n",
    "We redo the Elbow method and Silhoutte score analysis with this modified pipeline. Again, we are following the Churnometer project quite closely.\n",
    "cluster_pipe = last_cluster_pipe()\n",
    "pipeline_analysis = Pipeline(cluster_pipe.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(game_data_reduced)\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "n_cluster_start, n_cluster_stop = 2, 7\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "Again, 3 clusters gives the highest Silhoutte coefficient.\n",
    "new_game_w_clusters = game_data_reduced.copy()\n",
    "last_pipe = last_cluster_pipe(clusters=3)\n",
    "last_pipe.fit(new_game_w_clusters)\n",
    "new_game_w_clusters['Clusters'] = last_pipe.predict(new_game_w_clusters)\n",
    "new_game_w_clusters.head()\n",
    "\n",
    "print(f\"* Cluster frequencies \\n{new_game_w_clusters['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "new_game_w_clusters['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "game_seasons = get_df('game_pre_split', 'datasets/clean/csv').filter(['game_id','season'])\n",
    "game_seasons.set_index('game_id',inplace=True)\n",
    "new_clusters_only = new_game_w_clusters.filter(['Clusters'])\n",
    "new_season_clusters = new_clusters_only.join(game_seasons)\n",
    "\n",
    "\n",
    "new_cluster_season_corr = new_season_clusters.corr()\n",
    "print(new_cluster_season_corr.head())\n",
    "\n",
    "Lets look at the distribution of seasons for each cluster like we did previously. We will compare it side by side with the old distribution. Note, the labels of the clusters may have changed so we have regruoped them by trying to match distributions.\n",
    "pairs = [(0,2),(1,0),(2,1)]\n",
    "for i,j in pairs:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    old_cluster = season_clusters.query(f'Clusters == {i}')\n",
    "    new_cluster = new_season_clusters.query(f'Clusters == {j}')\n",
    "    sns.countplot(data=old_cluster, x=\"season\", ax=ax[0]).set_title(f\"Old Cluster {i}\")\n",
    "    sns.countplot(data=new_cluster, x=\"season\", ax=ax[1]).set_title(f\"New Cluster {j}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "So perhaps the clusters are related to time. Let's look at the other correlations with respect to the clustering.\n",
    "clusters_correlation = game_w_clusters.corr()\n",
    "print(clusters_correlation['Clusters'].sort_values(ascending=False)[:8])\n",
    "We will see this again when we attempt to classify the clusters using an adaptive boost model.\n",
    "Let's investigate how the clusters and the components from the PCA relate to one another."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
